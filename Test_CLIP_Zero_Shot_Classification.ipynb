{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_CLIP-Zero-Shot-Classification",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delemarchand2020/IVADO_MILA_DL/blob/main/Test_CLIP_Zero_Shot_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C76CDbIJi2DY"
      },
      "source": [
        "# How to use CLIP Zero-Shot on your own classificaiton dataset\n",
        "\n",
        "This notebook provides an example of how to benchmark CLIP's zero shot classification performance on your own classification dataset.\n",
        "\n",
        "[CLIP](https://openai.com/blog/clip/) is a new zero shot image classifier relased by OpenAI that has been trained on 400 million text/image pairs across the web. CLIP uses these learnings to make predicts based on a flexible span of possible classification categories.\n",
        "\n",
        "CLIP is zero shot, that means **no training is required**. \n",
        "\n",
        "Try it out on your own task here!\n",
        "\n",
        "Be sure to experiment with various text prompts to unlock the richness of CLIP's pretraining procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOF3Feb7jrnu"
      },
      "source": [
        "# Download and Install CLIP Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyUIWjzOi23X"
      },
      "source": [
        "#installing some dependencies, CLIP was release in PyTorch\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqN0UVpssA7J",
        "outputId": "d39402ca-4b08-4ec3-b62d-59c340197ff9"
      },
      "source": [
        "#clone the CLIP repository\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "%cd CLIP"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 90 (delta 0), reused 3 (delta 0), pack-reused 86\u001b[K\n",
            "Unpacking objects: 100% (90/90), done.\n",
            "/content/CLIP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwCkgL73rHE0"
      },
      "source": [
        "# Download Classification Data or Object Detection Data\n",
        "\n",
        "We will download the [public flowers classificaiton dataset](https://public.roboflow.com/classification/flowers_classification) from Roboflow. The data will come out as folders broken into train/valid/test splits and seperate folders for each class label.\n",
        "\n",
        "You can easily download your own dataset from Roboflow in this format, too.\n",
        "\n",
        "We made a conversion from object detection to CLIP text prompts in Roboflow, too, if you want to try that out.\n",
        "\n",
        "\n",
        "To get your data into Roboflow, follow the [Getting Started Guide](https://blog.roboflow.ai/getting-started-with-roboflow/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHHnCdsKrCVF"
      },
      "source": [
        "#download classification data\n",
        "#replace with your link\n",
        "!curl -L \"https://public.roboflow.com/ds/voBptUkaY3?key=S51gBfZMC4\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m7FYTSjCN46",
        "outputId": "b367782a-e3ef-4198-db96-223e93bd6465"
      },
      "source": [
        "import os\n",
        "#our the classes and images we want to test are stored in folders in the test set\n",
        "class_names = os.listdir('./train/')\n",
        "class_names.remove('_tokenization.txt')\n",
        "class_names"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tomato Cherry Red',\n",
              " 'Tomato 4',\n",
              " 'Tomato 3',\n",
              " 'Walnut',\n",
              " 'Tomato Maroon',\n",
              " 'Tomato Yellow']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QELzUB7pnr-h",
        "outputId": "af03ee4a-2551-4597-e1c7-c6c55c0789c9"
      },
      "source": [
        "#we auto generate some example tokenizations in Roboflow but you should edit this file to try out your own prompts\n",
        "#CLIP gets a lot better with the right prompting!\n",
        "#be sure the tokenizations are in the same order as your class_names above!\n",
        "%cat ./train/_tokenization.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An example picture from the Fruits Dataset dataset depicting a Tomato 3\n",
            "An example picture from the Fruits Dataset dataset depicting a Tomato 4\n",
            "An example picture from the Fruits Dataset dataset depicting a Tomato Cherry Red\n",
            "An example picture from the Fruits Dataset dataset depicting a Tomato Maroon\n",
            "An example picture from the Fruits Dataset dataset depicting a Tomato Yellow\n",
            "An example picture from the Fruits Dataset dataset depicting a Walnut"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPQxIGxXn8bR",
        "outputId": "036591cc-560d-40b9-a101-058237530dea"
      },
      "source": [
        "#edit your prompts as you see fit here\n",
        "%%writefile ./train/_tokenization.txt\n",
        "Tomato Cherry Red\n",
        "Tomato 3\n",
        "Tomato 4\n",
        "Walnut\n",
        "Tomato Maroon\n",
        "Tomato Yellow"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting ./train/_tokenization.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXaqPH3oSyO"
      },
      "source": [
        "candidate_captions = []\n",
        "with open('./train/_tokenization.txt') as f:\n",
        "    candidate_captions = f.read().splitlines()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGtoICrcGnxw",
        "outputId": "8c0229a8-27f0-4b97-a67e-107cd32452e8"
      },
      "source": [
        "print(candidate_captions)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Tomato Cherry Red', 'Tomato 3', 'Tomato 4', 'Walnut', 'Tomato Maroon', 'Tomato Yellow']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMZJy1SduxiE"
      },
      "source": [
        "# Run CLIP inference on your classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAi7cvucnFPr",
        "outputId": "7eb48ad9-fba8-4c7d-dfb3-12c2ed8dab86"
      },
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "def argmax(iterable):\n",
        "    return max(enumerate(iterable), key=lambda x: x[1])[0]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "correct = []\n",
        "class_miss =[]\n",
        "\n",
        "#define our target classificaitons, \n",
        "#you can should experiment with these strings of text as you see fit, \n",
        "#though, make sure they are in the same order as your class names above\n",
        "text = clip.tokenize(candidate_captions).to(device)\n",
        "\n",
        "for cls in class_names:\n",
        "    class_correct = []\n",
        "    test_imgs = glob.glob('./train/' + cls + '/*.jpg')\n",
        "    for img in test_imgs:\n",
        "        #print(img)\n",
        "        image = transform(Image.open(img)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(image)\n",
        "            text_features = model.encode_text(text)\n",
        "            \n",
        "            logits_per_image, logits_per_text = model(image, text)\n",
        "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "            pred = class_names[argmax(list(probs)[0])]\n",
        "            #print(pred)\n",
        "            if pred == cls:\n",
        "                correct.append(1)\n",
        "                class_correct.append(1)\n",
        "            else:\n",
        "                correct.append(0)\n",
        "                class_correct.append(0)\n",
        "                class_miss.append([pred,cls])\n",
        "    \n",
        "    print('accuracy on class ' + cls + ' is :' + str(sum(class_correct)/len(class_correct)))\n",
        "print('accuracy on all is : ' + str(sum(correct)/len(correct)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on class Tomato Cherry Red is :0.8313008130081301\n",
            "accuracy on class Tomato 4 is :0.0\n",
            "accuracy on class Tomato 3 is :0.0\n",
            "accuracy on class Walnut is :1.0\n",
            "accuracy on class Tomato Maroon is :0.989100817438692\n",
            "accuracy on class Tomato Yellow is :0.7625272331154684\n",
            "accuracy on all is : 0.6383636988655895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "4TYVngYPwD0h",
        "outputId": "7b2535ae-253c-4e4c-815c-be0f3fc20eaa"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(len(class_miss))\n",
        "#print(class_miss)\n",
        "misses = np.array(class_miss)\n",
        "print(sum(misses[:,1] == 'Tomato 4'))\n",
        "#np.histogram(misses.flatten)\n",
        "plt.hist(misses[:,0])\n",
        "plt.hist(misses[:,1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1052\n",
            "479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  4.,   0.,  83.,   0.,   0., 479.,   0., 377.,   0., 109.]),\n",
              " array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "E2FJi6YMBOlf",
        "outputId": "4501f96d-4eaa-43b6-a595-04d9c8315c4d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_names = np.array(['Tomato Cherry Red',\n",
        " 'Tomato 4',\n",
        " 'Tomato 3',\n",
        " 'Walnut',\n",
        " 'Tomato Maroon',\n",
        " 'Tomato Yellow'])\n",
        "\n",
        "test = np.array([['Tomato Maroon', 'Tomato Cherry Red'],\n",
        " ['Tomato Maroon', 'Tomato Cherry Red'],\n",
        " ['Tomato Maroon', 'Tomato Cherry Red'],\n",
        " ['Tomato 4','Tomato Yellow']])\n",
        "\n",
        "test_mapping = np.zeros(len(test))\n",
        "for i in range(len(test)):\n",
        "  id_class,  = np.where(class_names==test[i,1])\n",
        "  test_mapping[i] = int(id_class)\n",
        "\n",
        "np.histogram(test_mapping)\n",
        "\n",
        "plt.hist(test[:,0])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
              " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOaUlEQVR4nO3dfYxldX3H8ffHnRVNFIjsRMk+OKbQJmIUdYoam4ZqTfAhkkYaIa2KsdnUatTExKAxiDZptTaSKEaykY2gBrVozCprCBWsWCs6u12WJ222lpYlKMNiefABXfrtH/dQb8eZuXfuPbO7+Hu/kps9D797zm+W5H3Pnrn3kqpCkvTb7XFHewKSpPVn7CWpAcZekhpg7CWpAcZekhowc7ROvGnTppqbmztap5ekx6Q9e/bcW1Wza33eUYv93NwcCwsLR+v0kvSYlOQ/J3met3EkqQHGXpIaYOwlqQHGXpIaYOwlqQHGXpIaMDL2SZ6Q5LtJbkpya5L3LzPmuCSfT3IgyY1J5tZjspKkyYxzZf8w8JKqeg5wOnBWkhcuGfMm4CdVdQpwMfChfqcpSZrGyNjXwEPd6sbusfRL8M8GLu+WrwJemiS9zVKSNJWxPkGbZAOwBzgF+HhV3bhkyGbgToCqOpzkfuAk4N4lx9kObAfYtm3bxJOeu+DqiZ87rTs++Mqjdm5JmtRYv6Ctqkeq6nRgC3BGkmdNcrKq2lFV81U1Pzu75q92kCRNaE3vxqmq/wauB85asusuYCtAkhngBOBQHxOUJE1vnHfjzCY5sVt+IvAy4PtLhu0C3tAtnwNcV/7PbSXpmDHOPfuTgcu7+/aPA75QVV9N8gFgoap2AZcBn05yALgPOHfdZixJWrORsa+q/cBzl9l+4dDyL4A/7XdqkqS++AlaSWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWrAyNgn2Zrk+iS3Jbk1yduXGXNmkvuT7OseF67PdCVJk5gZY8xh4J1VtTfJk4E9Sa6tqtuWjLuhql7V/xQlSdMaeWVfVXdX1d5u+UHgdmDzek9MktSfNd2zTzIHPBe4cZndL0pyU5KvJTlthedvT7KQZGFxcXHNk5UkTWbs2Cd5EvBF4B1V9cCS3XuBp1fVc4CPAV9e7hhVtaOq5qtqfnZ2dtI5S5LWaKzYJ9nIIPSfraovLd1fVQ9U1UPd8m5gY5JNvc5UkjSxcd6NE+Ay4Paq+sgKY57WjSPJGd1xD/U5UUnS5MZ5N86LgdcBNyfZ1217D7ANoKouBc4B3pzkMPBz4NyqqnWYryRpAiNjX1XfAjJizCXAJX1NSpLULz9BK0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ABjL0kNMPaS1ICRsU+yNcn1SW5LcmuSty8zJkk+muRAkv1Jnrc+05UkTWJmjDGHgXdW1d4kTwb2JLm2qm4bGvNy4NTu8QLgE92fkqRjwMgr+6q6u6r2dssPArcDm5cMOxu4oga+A5yY5OTeZytJmsia7tknmQOeC9y4ZNdm4M6h9YP85gsCSbYnWUiysLi4uLaZSpImNnbskzwJ+CLwjqp6YJKTVdWOqpqvqvnZ2dlJDiFJmsBYsU+ykUHoP1tVX1pmyF3A1qH1Ld02SdIxYJx34wS4DLi9qj6ywrBdwOu7d+W8ELi/qu7ucZ6SpCmM826cFwOvA25Osq/b9h5gG0BVXQrsBl4BHAB+Bryx/6lKkiY1MvZV9S0gI8YU8Ja+JiVJ6pefoJWkBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBhh7SWqAsZekBoyMfZKdSe5JcssK+89Mcn+Sfd3jwv6nKUmaxswYYz4FXAJcscqYG6rqVb3MSJLUu5FX9lX1TeC+IzAXSdI66eue/YuS3JTka0lOW2lQku1JFpIsLC4u9nRqSdIofcR+L/D0qnoO8DHgyysNrKodVTVfVfOzs7M9nFqSNI6pY19VD1TVQ93ybmBjkk1Tz0yS1JupY5/kaUnSLZ/RHfPQtMeVJPVn5LtxklwJnAlsSnIQeB+wEaCqLgXOAd6c5DDwc+Dcqqp1m7Ekac1Gxr6qzhux/xIGb82UJB2j/AStJDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA0bGPsnOJPckuWWF/Uny0SQHkuxP8rz+pylJmsY4V/afAs5aZf/LgVO7x3bgE9NPS5LUp5Gxr6pvAvetMuRs4Ioa+A5wYpKT+5qgJGl6Mz0cYzNw59D6wW7b3UsHJtnO4Oqfbdu29XBqSZrM3AVXH7Vz3/HBVx7xcx7RX9BW1Y6qmq+q+dnZ2SN5aklqWh+xvwvYOrS+pdsmSTpG9BH7XcDru3flvBC4v6p+4xaOJOnoGXnPPsmVwJnApiQHgfcBGwGq6lJgN/AK4ADwM+CN6zVZSdJkRsa+qs4bsb+At/Q2I0lS7/wErSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1YKzYJzkryQ+SHEhywTL7z0+ymGRf9/iL/qcqSZrUzKgBSTYAHwdeBhwEvpdkV1XdtmTo56vqreswR0nSlMa5sj8DOFBVP6yqXwKfA85e32lJkvo0Tuw3A3cOrR/sti31miT7k1yVZOtyB0qyPclCkoXFxcUJpitJmkRfv6D9CjBXVc8GrgUuX25QVe2oqvmqmp+dne3p1JKkUcaJ/V3A8JX6lm7b/6mqQ1X1cLf6SeD5/UxPktSHcWL/PeDUJM9I8njgXGDX8IAkJw+tvhq4vb8pSpKmNfLdOFV1OMlbgWuADcDOqro1yQeAharaBbwtyauBw8B9wPnrOGdJ0hqNjD1AVe0Gdi/ZduHQ8ruBd/c7NUlSX/wErSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgOMvSQ1wNhLUgPGin2Ss5L8IMmBJBcss/+4JJ/v9t+YZK7viUqSJjcy9kk2AB8HXg48EzgvyTOXDHsT8JOqOgW4GPhQ3xOVJE1unCv7M4ADVfXDqvol8Dng7CVjzgYu75avAl6aJP1NU5I0jZkxxmwG7hxaPwi8YKUxVXU4yf3AScC9w4OSbAe2d6sPJfnBJJMGNi099pES/80iaUr50FQNe/okTxon9r2pqh3AjmmPk2ShquZ7mJIkHXFHo2Hj3Ma5C9g6tL6l27bsmCQzwAnAoT4mKEma3jix/x5wapJnJHk8cC6wa8mYXcAbuuVzgOuqqvqbpiRpGiNv43T34N8KXANsAHZW1a1JPgAsVNUu4DLg00kOAPcxeEFYT1PfCpKko+iINyxegEvSbz8/QStJDTD2ktSAVWOf5KQk+7rHj5LcNbT++L4mkeTEJH81wfMqyWeG1meSLCb5al9zk9SmY71/3XM3JPnXcZq3auyr6lBVnV5VpwOXAhc/ut59mrYvJwKT/LA/BZ6V5Ind+sv4zbeFrqp7q6gk/T+Pgf4BvB24fZyBa76Nk+Sl3SvJzUl2Jjmu235Hkr/tXvUWkjwvyTVJ/j3JX3ZjnpTk60n2ds9/9GsXPgj8TvfcD2fgw0lu6ca9dpUp7QZe2S2fB1w5NNczkvxLN99vJ/m9bvv5SXYluQ74epKnJPlykv1JvpPk2d24lbZf1P3s30jywyRvW+vfo6THnmOpf0m2MGjfJ8eafFWN9QAuAt7L4GsRfrfbdgXwjm75DuDN3fLFwH7gycAs8ONu+wxwfLe8CTgABJgDbhk612uAaxm81fOpwH8BJy8zp4eAZzP4Pp4nAPuAM4GvdvuPB2a65T8Gvtgtn8/gax+e0q1/DHhft/wSYN+I7RcB3waO636OQ8DGcf8uffjw8dh6HKP9uwp4/nDzVnus9cp+A/AfVfVv3frlwB8O7X/0w1Y3AzdW1YNVtQg8nOTE7gf7myT7gX9k8J06T13mPH8AXFlVj1TVj4F/An5/uQlV1f7uL+s8Blf5w04A/iHJLQz+A5w2tO/aqrpv6Hyf7o53HXBSkuNX2Q5wdVU9XFX3Aves8HNI+u1xzPQvyauAe6pqz7iT7/t+9cPdn/8ztPzo+gzwZwxe6Z5fVb9KcgeDK/Jp7QL+nsEr3ElD2/8auL6q/iSD79j/xtC+n055zuGf7xGO8PcMSTrmHMn+vRh4dZJXdMc4PslnqurPV3rCWq/sHwHmkpzSrb+OwavOuE5g8Gr0qyR/xK+/ve1BBv/kedQNwGu73zTPMnj1/O4qx90JvL+qbl7mfI/+wvb8VZ5/A4P/ECQ5E7i3qh5YZbuk9hwz/auqd1fVlqqaY/CNBdetFnpY+9XoL4A3Mrg1MsPge3MuXcPzPwt8JcnNwALw/W7ih5L8c3e75WvAu4AXATcBBbyrqn600kGr6iDw0WV2/R1weZL3AlevMq+LgJ3dP69+xq+/52el7ZLac0z2b1x+XYIkNcBP0EpSA4y9JDXA2EtSA4y9JDXA2EtSA4y9JDXA2EtSA/4Xx1RzavK2aDsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}