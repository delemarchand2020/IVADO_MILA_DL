{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week2_tutorial_denis_lemarchand.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delemarchand2020/IVADO_MILA_DL/blob/main/week2_tutorial_denis_lemarchand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipSoOVlavkb"
      },
      "source": [
        "# IVADO/MILA DEEP LEARNING SCHOOL\n",
        "# Spring 2021\n",
        "# Tutorial : Categorical data with multilayer perceptron (MLP)\n",
        "\n",
        "## Authors: \n",
        "\n",
        "Arsène Fansi Tchango <arsene.fansi.tchango@mila.quebec>\n",
        "\n",
        "Gaétan Marceau Caron <gaetan.marceau.caron@mila.quebec>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLHwvggEZERd"
      },
      "source": [
        "# Preface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKNGtQkkohiM"
      },
      "source": [
        "This tutorial introduces the practical aspects of Deep Learning through the realization of a simple end-to-end project. We will use the deep learning library <a href=\"https://pytorch.org/\"> `PyTorch`</a>, which is well-known for its ease of use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu9DZNmYpePz"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWCNdeTIpkCa"
      },
      "source": [
        "Before we begin, we must install all the required libraries for this part of the tutorial. To do so, we will use the `pip` utility. Execute the cell below by selecting it and pressing `shift`+`Enter`. (This operation may take a few minutes.)\n",
        "\n",
        "We need to be using the latest version of `pillow` for this tutorial. If you are prompted with:\n",
        "\n",
        "> WARNING: The following packages were previously imported in this runtime:\n",
        "  [PIL]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "\n",
        "Then click on restart runtime and rerun the cells afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g_0k-_Eppi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed188828-2472-40ac-f3ea-8d4b9e8e75bc"
      },
      "source": [
        "!pip3 install torch torchvision matplotlib\n",
        "!pip3 install --upgrade pillow==8.1.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (8.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already up-to-date: pillow==8.1.0 in /usr/local/lib/python3.7/dist-packages (8.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKufekB4FnqN"
      },
      "source": [
        "To ensure that all required libraries are available, let's try to load all libraries and modules we will need during this tutorial by executing this cell: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loLQlRg3sV-r"
      },
      "source": [
        "import importlib\n",
        "required_libraries = ['torch', 'torchvision', 'PIL', 'matplotlib', \n",
        "                      'numpy', 'pandas']\n",
        "for lib in required_libraries:\n",
        "    if importlib.util.find_spec(lib) is None:\n",
        "        print(\"%s unavailable\" % lib)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeLUcnk5scNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45e0779-7e36-4b3f-ea1e-160969223f21"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "print(\"Torch version: \", torch.__version__)\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch version:  1.8.1+cu101\n",
            "GPU Available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc9wc4qq7qob"
      },
      "source": [
        "Fix the seed for the different libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05jTJ12r7msf"
      },
      "source": [
        "seed = 4321\n",
        "np.random.seed(seed) # Set the random seed of numpy for the data split.\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKzgFV9Favkt"
      },
      "source": [
        "# PyTorch in a nutshell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrus_-F0avkt"
      },
      "source": [
        "*PyTorch* is a Python library that supports a vibrant ecosystem of tools and libraries for machine learning (ML) in vision, NLP, and more. It provides two high-level features:\n",
        "<ul>\n",
        "<li> operations on <a href=\"https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\">tensors</a> (such as NumPy) with GPU support,</li>\n",
        "<li> operations for creating and optimizing computational graphs with an automatic differentiation system called <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\">Autograd</a>.</li>\n",
        "</ul>\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/torch.html\">PyTorch docs</a> contain the API documentation and <a href=\"https://pytorch.org/tutorials/\">many tutorials</a>.\n",
        "Also, PyTorch offers several data processing utilities. One of these utilities is the class <a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.Dataset`</a> which offers an easy to use interface to handle a data set. For more information, please refer to the following urls: \n",
        "<ul>\n",
        "<li>PyTorch data sets: <a href=\"http://pytorch.org/docs/master/data.html\"> PyTorch - datasets</a>.</li>\n",
        "<li>A tutorial for loading data: <a href=\"http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\"> PyTorch - data loading tutorial</a>.</li>\n",
        "</ul>\n",
        "\n",
        "<a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> is a package that provides the same functions as CPU tensors but for  CUDA tensors, which are used for GPU computing. <a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> returns a boolean indicating if CUDA is currently available. Finally, we recommend using a `device` variable that identifies the device that will perform computations. We can assign a tensor to a device with the method `.to(device)`. By default, the tensors are CPU tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm122vNmq92L"
      },
      "source": [
        "# Ingredients for a proof of concept (POC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqvhR0ebavmE"
      },
      "source": [
        "To realize a ML POC, you need:\n",
        "<ul>\n",
        "<li>a task description as well as data to support it,</li>\n",
        "<li>evaluation metrics to assess the performance of models,</li>\n",
        "<li>a model description,</li>\n",
        "<li>a loss function to minimize,</li>\n",
        "<li>an optimizer that adjusts the parameters of the model.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8_pfpu2f6AO"
      },
      "source": [
        "# How to prepare the dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5piZxYUhSzq"
      },
      "source": [
        "In this tutorial, we study the tragedy of the Titanic through a data-driven approach. Our task is to determine whether or not a passenger survived the Titanic sinking based on passenger data only. The results will show how the passengers' fate was pre-determined by the features selected in this study, without considering other factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4GuYNDFavlU"
      },
      "source": [
        "## Titanic dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiOJx2ytavlU"
      },
      "source": [
        "First, we download the Titanic dataset from the following address:\n",
        "<br/>\n",
        "https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true.<br/>\n",
        "This dataset provides information on the fate of 1309 passengers of the first and only journey of the liner RMS Titanic, summarized by economic status (class), gender, age, family information, and survival. We use this dataset because we can train models very quickly for the purpose of this tutorial. The Kaggle platform also uses this dataset as an introduction to classical machine learning. <br/>\n",
        "\n",
        "Let's take a look at the features and some examples from this dataset. To do so, we use the library <a href=\"https://pandas.pydata.org/\">Pandas</a> to load the dataset into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX_RSiffavlW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "ecfcacc7-97e2-418f-8337-a29ed8c83e5f"
      },
      "source": [
        "titanic_df = pd.read_csv(\n",
        "    'https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true', \n",
        "    sep='\\t', \n",
        "    index_col=None, \n",
        "    na_values=['NA']\n",
        ")\n",
        "\n",
        "# a snapshot of the first 5 data points\n",
        "titanic_df.head()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pclass</th>\n",
              "      <th>survived</th>\n",
              "      <th>name</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>ticket</th>\n",
              "      <th>fare</th>\n",
              "      <th>cabin</th>\n",
              "      <th>embarked</th>\n",
              "      <th>boat</th>\n",
              "      <th>body</th>\n",
              "      <th>home.dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allen, Miss. Elisabeth Walton</td>\n",
              "      <td>female</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24160</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>B5</td>\n",
              "      <td>S</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>St Louis, MO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allison, Master. Hudson Trevor</td>\n",
              "      <td>male</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Miss. Helen Loraine</td>\n",
              "      <td>female</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
              "      <td>male</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>135.0</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
              "      <td>female</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pclass  survived  ...   body                        home.dest\n",
              "0       1         1  ...    NaN                     St Louis, MO\n",
              "1       1         1  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "2       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "3       1         0  ...  135.0  Montreal, PQ / Chesterville, ON\n",
              "4       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj88WmCmavlf"
      },
      "source": [
        "Here's the list of the features with their description:\n",
        "\n",
        "<ol>\n",
        "\n",
        "  <li> <b>pclass</b>: Passenger class (1 = first; 2 = second; 3 = third) </li>\n",
        "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
        "  <li> <b>name</b>: Name </li>\n",
        "  <li> <b>sex</b>: Sex </li>\n",
        "  <li> <b>age</b>: Age </li>\n",
        "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
        "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
        "  <li> <b>ticket</b>: Ticket number </li>\n",
        "  <li> <b>fare</b>: Passenger fare </li>\n",
        "  <li> <b>cabin</b>: Cabin number </li>\n",
        "  <li> <b>embarked</b>: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) </li>\n",
        "  <li> <b>boat</b>: Lifeboat (if the passenger survived, otherwise the value is Not a Number (NaN) </li>\n",
        "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found, otherwise the value is NaN) </li>\n",
        "  <li> <b>home.dest</b>: the passenger's destination </li>\n",
        " </ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2ed5fozqjce"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__vcZhPnavlg"
      },
      "source": [
        "### Feature selection\n",
        "Some features are not relevant to the task, for example:\n",
        "<ol>\n",
        "  <li> <b>name</b>: Name </li>\n",
        "  <li> <b>ticket</b>: Ticket number </li>\n",
        "  <li> <b>cabin</b>: Cabin number </li>\n",
        "  <li> <b>home.dest</b>: Passenger's destination </li>\n",
        " </ol>\n",
        " \n",
        "Other features are directly related to the passenger survival and are not interesting for our study because they give away the label to be predicted:\n",
        "<ol>\n",
        "  <li> <b>boat</b>: Lifeboat (if the passenger survived) </li>\n",
        "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found) </li>\n",
        " </ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoD8x0dXofmj"
      },
      "source": [
        "titanic_df.drop(['name', 'ticket', 'cabin', 'home.dest', 'boat', 'body'], axis=1, inplace=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PW_IWwqwdHq"
      },
      "source": [
        "### Handling missing values\n",
        "\n",
        "Handling missing values in datasets is difficult since examples with missing values can be informative for subgroups. Suppose that features associated with third-class tickets have many missing values compared to first-class tickets. Removing all examples with missing values will introduce a bias in the dataset and could completely change the correlations between features and the probability of survival. Another approach is to impute missing values by replacing them with simples statistics such as the mean or the median of the feature. Other imputing techniques use the other features to predict the missing values, which is very close to a ML task. See Scikit-learn reference for more details ([Imputation of missing values](https://www.google.com/url?q=https://scikit-learn.org/stable/modules/impute.html&sa=D&source=editors&ust=1617044175668000&usg=AFQjCNHxRm4BcLBrZEPzW7i3C_PJ7oAuPQ)). Thus, handling missing values should be done carefully and often depend on the dataset and domain knowledge. In the following, we discard all examples with missing values, but since it is a substantial number of examples, we are probably introducing a bias in the dataset. To avoid such bias, one can re-annotate the data manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8YKDg69wjb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0f8222-9d61-4991-ffd5-1cc73cd35578"
      },
      "source": [
        "n_examples = len(titanic_df)\n",
        "titanic_df = titanic_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
        "print(f'We removed {n_examples-len(titanic_df)} examples over {n_examples} containing missing values.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We removed 266 examples over 1309 containing missing values.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MckYm0M_xhR"
      },
      "source": [
        "### Feature encoding\n",
        " \n",
        "Some features are **categorical variables**, which means that they can take a finite number of values.\n",
        " <ol>\n",
        "  <li> <b>pclass</b>: Passenger Class </li>\n",
        "  <li> <b>sex</b>: Sex </li>\n",
        "  <li> <b>embarked</b>: Port of embarkation </li>\n",
        " </ol>\n",
        "\n",
        "To process categorical variables, we need to encode them in a way that does not imply an arbitrary order such as using natural numbers (e.g., 1, 2, 3). <a href=\"https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics\">One-hot encoding</a> is a way to achieve it. To use this encoding, we can simply call the function `get_dummies` in Pandas. The meaning of the encoded variables is as follows:\n",
        "\n",
        "<ol>\n",
        "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
        "  <li> <b>pclass_1</b>: (1 if passenger in first class; 0 if not) </li>\n",
        "  <li> <b>pclass_2</b>: (1 if passenger in second class; 0 if not) </li>\n",
        "  <li> <b>pclass_3</b>: (1 if passenger in third class; 0 if not) </li>\n",
        "  <li> <b>sex_female</b>: (1 if passenger is female; 0 if not) </li>\n",
        "  <li> <b>sex_male</b>: (1 if passenger is male; 0 if not) </li>\n",
        "  <li> <b>age</b>: Age </li>\n",
        "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
        "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
        "  <li> <b>fare</b>: Passenger fare </li>\n",
        "  <li> <b>embarked_C</b>: (1 if Port of embarkation = Cherbourg (C); 0 otherwise) </li> \n",
        "  <li> <b>embarked_Q</b>: (1 if Port of embarkation = Queenstown (Q); 0 otherwise) </li> \n",
        "  <li> <b>embarked_S</b>: (1 if Port of embarkation = Southampton (S); 0 otherwise)</li> \n",
        " </ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7bFJc5X_qjx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "b0e3e936-2363-4911-e174-50d3f95faf1c"
      },
      "source": [
        "titanic_preprocess_df = pd.get_dummies(data=titanic_df, columns=['pclass', 'sex', 'embarked'])\n",
        "titanic_preprocess_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   survived      age  sibsp  ...  embarked_C  embarked_Q  embarked_S\n",
              "0         1  29.0000      0  ...           0           0           1\n",
              "1         1   0.9167      1  ...           0           0           1\n",
              "2         0   2.0000      1  ...           0           0           1\n",
              "3         0  30.0000      1  ...           0           0           1\n",
              "4         0  25.0000      1  ...           0           0           1\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeBNt5EW6tm9"
      },
      "source": [
        "Now, we can check if all examples have a one-hot encoding for their categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RjZZEokvJ3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d089db91-ea0f-4c56-d67a-0cb09cc317a3"
      },
      "source": [
        "titanic_preprocess_df.loc[titanic_preprocess_df[['sex_male','sex_female']].sum(axis=1) != 1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [survived, age, sibsp, parch, fare, pclass_1, pclass_2, pclass_3, sex_female, sex_male, embarked_C, embarked_Q, embarked_S]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3yueVPfvEhI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9cdb11fa-c5d1-4ff5-b697-22626152a56e"
      },
      "source": [
        "titanic_preprocess_df.loc[titanic_preprocess_df[['pclass_1','pclass_2', 'pclass_3']].sum(axis=1) != 1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [survived, age, sibsp, parch, fare, pclass_1, pclass_2, pclass_3, sex_female, sex_male, embarked_C, embarked_Q, embarked_S]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4No15nhu8Hi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "35d9f6bd-1e15-4a7c-8648-d43d9513fdb9"
      },
      "source": [
        "titanic_preprocess_df.loc[titanic_preprocess_df[['embarked_C','embarked_Q', 'embarked_S']].sum(axis=1) != 1]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [survived, age, sibsp, parch, fare, pclass_1, pclass_2, pclass_3, sex_female, sex_male, embarked_C, embarked_Q, embarked_S]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4a-35Xa66Tj"
      },
      "source": [
        "Since there are only two examples with no port of embarkation, we decide to discard them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wODe7zbglC1I",
        "outputId": "4bb27d4e-0340-47c0-8119-5e271c50f003"
      },
      "source": [
        "titanic_df.loc[148,:]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pclass            1\n",
              "survived          0\n",
              "sex          female\n",
              "age              50\n",
              "sibsp             0\n",
              "parch             0\n",
              "fare        28.7125\n",
              "embarked          C\n",
              "Name: 148, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkBNPNPDk9ZN"
      },
      "source": [
        "# Drop rows where there is no port of embarkation associated\n",
        "#titanic_preprocess_df = titanic_preprocess_df.drop(index=148).reset_index(drop=True)\n",
        "#titanic_preprocess_df = titanic_preprocess_df.drop(index=248).reset_index(drop=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2p2GAKHm-92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6330bc2-29b6-4460-9ccd-dd93e73e8799"
      },
      "source": [
        "print(f'There are {len(titanic_preprocess_df)} remaining examples in the dataset.')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1043 remaining examples in the dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJcs6PUTavlm"
      },
      "source": [
        "## Train / validation / test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjbgvffmavlo"
      },
      "source": [
        "At this point, we need to divide the dataset into three subsets:\n",
        "\n",
        "<ol>\n",
        "<li> <b> Train</b> (60% of the dataset): used to train the classification model. </li>   \n",
        "<li> <b> Validation</b> (20% of the dataset): used to evaluate hyper-parameters on held-out data. </li>   \n",
        "<li> <b> Test</b> (20% of the dataset): used to evaluate the generalization performance of the chosen model on held-out data. </li>\n",
        "</ol>\n",
        "\n",
        "We use the <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.split.html\">numpy.split function</a> to separate our dataset into subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs07L8F5488f"
      },
      "source": [
        "### Exercise 1\n",
        "Complete the missing code to create the validation and the test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBmL8VBOavlo"
      },
      "source": [
        "train, validate, test = np.split(\n",
        "    titanic_preprocess_df.sample(frac=1, random_state=seed), # cela revient a un shuffle du set de donnees\n",
        "    [int(.6*len(titanic_preprocess_df)), int(.8*len(titanic_preprocess_df))])\n",
        "\n",
        "# Remove the label column from X and create a label vectors.\n",
        "X_train = train.drop(['survived'], axis=1).to_numpy()\n",
        "y_train = train['survived'].to_numpy()\n",
        "\n",
        "X_val = validate.drop(['survived'], axis=1).to_numpy() \n",
        "y_val = validate['survived'].to_numpy()\n",
        "\n",
        "X_test = test.drop(['survived'], axis=1).to_numpy() \n",
        "y_test = test['survived'].to_numpy()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv74TbIWavlr"
      },
      "source": [
        "## Datasets in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_LJtG-Xavlt"
      },
      "source": [
        "We will use the subclass <b><a href=\"https://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset\"> `torch.utils.data.TensorDataset`</a> </b> to manipulate together the features and targets of a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZUfbAtG5S92"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Complete the missing code to load the validation and the test sets in TensorDatasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JtT4tV7avlt"
      },
      "source": [
        "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obEPHnlTavkc"
      },
      "source": [
        "# How to define the learning algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhN5GL6Gavks"
      },
      "source": [
        "To train a deep learning model, we need to define:\n",
        "* the network architecture by choosing the non-linear function and the number of hidden units per layer, \n",
        "* the loss function and optimizer.\n",
        "\n",
        "In this tutorial, we consider the multilayer perceptron (MLP). A MLP is a simple computational graph composed of \"hidden layers,\" which are defined by two modules: a **linear transformation** followed by a **non-linearity**. The result of a hidden layer is a vector called a **distributed representation** where each component is associated with a hidden unit.\n",
        "\n",
        "To solve our task, we will use a MLP with the following architecture:\n",
        "* the input dimension of the model is 12,\n",
        "* the output dimension of the model is 2,\n",
        "* the first dimension of the output is the probability of death and the second dimension is the probability of survival,\n",
        "* the number of hidden layers is 3, \n",
        "* the dimensions of the hidden layers are 20, 40, 20 respectively, \n",
        "* the activation function is a ReLu for all hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701t0e-ravkr"
      },
      "source": [
        "## How to define a model in PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4F5cyijavkv"
      },
      "source": [
        "The <a href=\"https://pytorch.org/docs/stable/nn.html\">PyTorch NN package</a> contains many useful classes for creating computation graphs.\n",
        "* The class <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module\">torch.nn.Module</a>: \n",
        "any new module must inherit from this class or its descendants (subclasses).\n",
        "* The `forward` method:  any class defining a module must implement the `forward(...)` method, which defines the transformation of inputs to outputs.\n",
        "* The class <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a>: this class implements a linear transformation. By default, it takes two parameters: \n",
        "    * `in_features`: the size of the data at the input of the module. \n",
        "    * `out_features`: the size of the data at the output of the module.\n",
        "\n",
        "* The module <a href=\"https://pytorch.org/docs/master/nn.functional.html#torch-nn-functional\">`torch.nn.functional`</a>: \n",
        "it defines a set of functions that can be applied directly to any tensor. As examples, we have:\n",
        "    * non-linear functions: `sigmoid(...)`, `tanh(...)`, `relu(...)`, ...\n",
        "    * loss functions: `mse_loss(...)`, `nll(...)`, `cross_entropy(...)`, ... \n",
        "    * regularization functions: `droupout(...)`, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tscha6S-KIBB"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Complete the following methods to define a neural network:\n",
        "* The `__init__` method that defines the layers.\n",
        "* The `forward(input)` method that returns the `output`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR5eBfIbavk0"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(NeuralNet, self).__init__()\n",
        "      self.layer1 = nn.Linear(12, 20)\n",
        "      self.layer2 = nn.Linear(20, 40)\n",
        "      self.layer3 = nn.Linear(40, 20)\n",
        "      self.output = nn.Linear(20, 2)\n",
        "\n",
        "    def forward(self, x):      \n",
        "      x = F.relu(self.layer1(x))\n",
        "      x = F.relu(self.layer2(x))\n",
        "      x = F.relu(self.layer3(x))\n",
        "      out = torch.sigmoid(self.output(x))\n",
        "      return out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLnHRZ5avk2"
      },
      "source": [
        "## Making predictions with a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEXgJMDDavk3"
      },
      "source": [
        "Now, we are ready to test our neural network on randomly selected data.\n",
        "\n",
        "In PyTorch, a model has two different modes:\n",
        "    <ul>\n",
        "    <li> <b>train</b>: used during training, </li>\n",
        "    <li> <b>eval</b>: used during inference for model evaluation. </li>\n",
        "    </ul>\n",
        "\n",
        "The distinction is important since some modules behave differently according to this mode.\n",
        "We will use the <b>eval</b> mode in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqtE_hr650pz"
      },
      "source": [
        "### Exercise 4\n",
        "Complete the missing code so that the model outputs a probability vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzcABMezavk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6e47e8-496d-4dfd-c9b8-9a1fa7cbec06"
      },
      "source": [
        "# Model definition\n",
        "neural_net = NeuralNet()\n",
        "neural_net = neural_net.to(device)\n",
        "\n",
        "# Evaluation mode activation\n",
        "neural_net = neural_net.eval()\n",
        "\n",
        "# Select the first 5 data points\n",
        "data, target = val_dataset[0:5]\n",
        "data = data.to(device)\n",
        "target = target.to(device)\n",
        "\n",
        "# Forward propagation of the data through the model\n",
        "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
        "\n",
        "# Convert the logits into probabilities with softmax function\n",
        "output_proba = F.softmax(output,dim=1)\n",
        "\n",
        "# Printing the probability\n",
        "print(output_proba)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4132, 0.5868],\n",
            "        [0.4284, 0.5716],\n",
            "        [0.4176, 0.5824],\n",
            "        [0.4445, 0.5555],\n",
            "        [0.3480, 0.6520]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVep0BElavlS"
      },
      "source": [
        "The rows define the output of the network, in terms of probabilities over two classes: <b>deceased</b> (first column) or <b>survived</b> (second column), for each of the five input data points. Let us take the label with maximum probability as the predicted label and compare it to the correct label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jV4No36qjdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8b1d1f-e0c7-4da0-bd2c-8f6b690d9523"
      },
      "source": [
        "# Printing predictions (class with the highest probability)\n",
        "_, prediction = torch.max(output_proba, dim=1)\n",
        "\n",
        "print('Model prediction')\n",
        "print(prediction)\n",
        "\n",
        "# Printing the real labels\n",
        "print(\"Actual data\")\n",
        "print(target)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model prediction\n",
            "tensor([1, 1, 1, 1, 1], device='cuda:0')\n",
            "Actual data\n",
            "tensor([0, 0, 0, 0, 0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEIIjqOuqjdc"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "1.   What would be a good way to measure the model performances?\n",
        "2.   How does our model perform?\n",
        "3.   Considering that the model is not trained on the dataset, do you see any problem with your selected measure?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa66z1qqH9em",
        "cellView": "form"
      },
      "source": [
        "#@title Métriques\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Compute the accuracy score.\n",
        "  \n",
        "  Args:\n",
        "     y_true: ground truth labels.\n",
        "     y_pred: predicted labels by a classifier.\n",
        "     \n",
        "  Return:\n",
        "     Accuracy score.\n",
        "     \n",
        "  \"\"\"\n",
        "  return metrics.accuracy_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Compute the F1 score.\n",
        "  \n",
        "  Args:\n",
        "     y_true: ground truth labels.\n",
        "     y_pred: predicted labels by a classifier.\n",
        "     \n",
        "  Return:\n",
        "     F1 score.\n",
        "     \n",
        "  \"\"\"\n",
        "  return metrics.f1_score(y_true, y_pred, average='macro')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reJFVgMHABD7",
        "outputId": "475af7cb-14e4-4ef2-8792-f45345f6b8ec"
      },
      "source": [
        "# calcul F1 score\n",
        "F1_score = f1_score(target.cpu().numpy(), prediction.cpu().numpy())\n",
        "print(f'F1 score is {F1_score:.2%}'.format(F1_score))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score is 0.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uySA2TCavmD"
      },
      "source": [
        "## Define the loss function and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkoobCLMavmE"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkX7uSXQavmF"
      },
      "source": [
        "We define the loss function according to the task we want to achieve.\n",
        "\n",
        "PyTorch offers <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">many ready-to-use loss functions</a>.\n",
        "\n",
        "For classification problems, the usual loss function is <b>cross-entropy</b>, and this is the one we will use in this tutorial. In PyTorch, it is defined by the function <a href=\"https://pytorch.org/docs/master/nn.functional.html#cross-entropy\">`torch.nn.functional.cross_entropy`</a>.  Cross entropy allows comparing a $p$ distribution with a reference distribution $t$. It attains its minimum when $t=p$. Its formula for calculating it between the prediction and the target is: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction, $t$ the target, $i$ the example and $j$ the classe of the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHnfYeS5avmF"
      },
      "source": [
        "def loss_function(prediction, target):\n",
        "    loss = F.cross_entropy(prediction, target)\n",
        "    return loss"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsx_cv9Wqjdj"
      },
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hcZaIKtavmH"
      },
      "source": [
        "In Pytorch, thanks to the automatic differentiation mechanism <a href=\"http://pytorch.org/docs/master/notes/autograd.html\">Autograd</a>, it is possible to automatically calculate the gradient of the loss function and backpropagate it through the computational graph.\n",
        "\n",
        "To do this, we only have to call the method `backward()` on the variable returned by the loss function, e.g., with\n",
        "\n",
        "```python\n",
        "loss = loss_function(....)\n",
        "loss.backward()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YNo_ymYavmH"
      },
      "source": [
        "### Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4AlX9TwavmH"
      },
      "source": [
        "PyTorch provides a <a href=\"http://pytorch.org/docs/master/optim.html#algorithms\">set of optimization methods (`torch.optim`)</a> commonly used by the deep learning community. These methods include the following: \n",
        "* <b>SGD</b> (Stochastic Gradient Descent) <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a>\n",
        "* <b>Adam</b> (Adaptive Moment Estimation): a variant of the gradient descent method in which the learning rate is adjusted for each parameter by estimating the first and second moments of the gradients. This optimizer has demonstrated excellent performance compared to SGD on many benchmarks.\n",
        "\n",
        "To be able to use an optimizer in PyTorch, we must instantiate it by passing the following elements:\n",
        "* <b>The parameters of the model</b>: these are obtained using the method `parameters()` on the instantiated model.\n",
        "* <b>The learning rate (lr)</b>: this is the learning rate to be used to update parameters during the optimization process. \n",
        "* There may be other parameters specific to the chosen optimizer.\n",
        "\n",
        "PyTorch offers a simplified interface to interact with any optimizer:\n",
        "* `zero_grad()`: Allows to reinitialize the gradients to zero at the beginning of an optimization step.\n",
        "* `step()`: Allows to perform an optimization step after a gradient backpropagation step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ-lKExqavmI"
      },
      "source": [
        "We will use Adam with a lr of 0.001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDMOziJTavmI"
      },
      "source": [
        "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnFOAfdGqjdr"
      },
      "source": [
        "# How to train and evaluate a model?\n",
        "First, we need some definitions:\n",
        "<ol>\n",
        "<li>\n",
        "<b>Epoch</b>: a complete pass over the entire training dataset.\n",
        "</li>\n",
        "<li>\n",
        "<b>Iteration</b>: an update of the model parameters. Many iterations can occur before the end of an epoch.\n",
        "</li>\n",
        "<li>\n",
        "<b>Mini-batch</b>: A subset of training data used to estimate the average of gradients. In other words, at each iteration, a mini-batch is used. \n",
        "</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLXjNiDTavmK"
      },
      "source": [
        "## Creating the mini-batches\n",
        "PyTorch offers a utility called <b><a href=\"http://pytorch.org/docs/master/data.html\"> `torch.utils.data.DataLoader`</a></b> to load any dataset and automatically split it into mini-batches. During training, the data presented to the network should appear in a different order from one epoch to another. We will prepare the `DataLoader` for our three datasets (training, validation, and test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGoQZSdqavmM"
      },
      "source": [
        "train_batch_size = 32  # number of data in a training batch.\n",
        "eval_batch_size = 32   # number of data in an batch.\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size, shuffle=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4ecBllSdsQ_",
        "outputId": "eebdfb7b-78f2-406d-fd24-fd78c8f0e907"
      },
      "source": [
        "train_loader_iter = iter(train_loader)\n",
        "\n",
        "for i in range(len(train_loader)):\n",
        "  t = next(train_loader_iter)\n",
        "  print(len(t[0])) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia3ai-GvavmP"
      },
      "source": [
        "## Simple training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9wNZrTnavmQ"
      },
      "source": [
        "Here we define our training procedure for an epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyK9xCsZavmR"
      },
      "source": [
        "def train(epoch, model, train_loader, optimizer, device):\n",
        "    \n",
        "    # activate the training mode\n",
        "    model.train()\n",
        "    \n",
        "    torch.set_grad_enabled(True)\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    # iteration over the mini-batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        # transfer the data on the chosen device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # reinitialize the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward propagation on the data\n",
        "        prediction = model(data)\n",
        "        \n",
        "        # compute the loss function w.r.t. the targets\n",
        "        loss = loss_function(prediction, target)\n",
        "        \n",
        "        # execute the backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # execute an optimization step\n",
        "        optimizer.step()\n",
        "        \n",
        "        # accumulate the loss\n",
        "        total_loss += loss.item()*len(data)\n",
        "        \n",
        "        # compute the number of correct predictions\n",
        "        _, pred_classes = torch.max(prediction, dim=1)        \n",
        "        correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()\n",
        "         \n",
        "        \n",
        "    # compute the average loss per epoch\n",
        "    mean_loss = total_loss/len(train_loader.dataset)\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "        \n",
        "    print('Train Epoch: {}   Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
        "        epoch, mean_loss, correct, len(train_loader.dataset),\n",
        "        100. * acc))   \n",
        "    \n",
        "    # return the average loss and the accuracy\n",
        "    return mean_loss, acc"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGexbWaHavmU"
      },
      "source": [
        "## Evaluation procedure\n",
        "Here we define our model evaluation procedure.\n",
        "<br/>\n",
        "In addition to switching the model to **eval** mode, it is essential to disable the gradient calculation. \n",
        "<br/>\n",
        "To do this, PyTorch offers a set of context managers to <a href=\"https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation\">locally disable/enable gradient calculation </a>:\n",
        "1. `torch.no_grad()`: disable gradient calculation.\n",
        "2. `torch.enable_grad()`: enable gradient calculation.\n",
        "3. `torch.set_grad_enabled(bool)`: enable/disable gradient calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gQj9W5LavmU"
      },
      "source": [
        "def evaluate(model, eval_loader, device):\n",
        "    \n",
        "    # activate the evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # iterate over the batches\n",
        "        for batch_idx, (data, target) in enumerate(eval_loader):\n",
        "\n",
        "            # transfer the data on the chosen device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # forward propagation on the data\n",
        "            prediction = model(data)\n",
        "\n",
        "            # compute the loss function w.r.t. the targets\n",
        "            loss = loss_function(prediction, target)           \n",
        "\n",
        "\n",
        "            # accumulate the loss\n",
        "            total_loss += loss.item()*len(data)\n",
        "\n",
        "            # compute the number of correct predictions en sortie)\n",
        "            _, pred_classes = torch.max(prediction, dim=1) \n",
        "            correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()         \n",
        "          \n",
        "    \n",
        "    # compute the average loss per epoch\n",
        "    mean_loss = total_loss/len(eval_loader.dataset)\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc = correct / len(eval_loader.dataset)\n",
        "        \n",
        "    print('Eval:  Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
        "        mean_loss, correct, len(eval_loader.dataset),\n",
        "        100. * acc)) \n",
        "    \n",
        "    # return the average loss and the accuracy\n",
        "    return mean_loss, acc"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQLklQXAavmW"
      },
      "source": [
        "## Checkpointing\n",
        "\n",
        "During training, we recommend saving the model parameters periodically. By doing so, we avoid having to retrain the model from scratch if the experiment goes wrong such as losing communication with GPU, out-of-memory error, numerical errors, incorrect learning rates making the experiment unstable, etc. This practice is commonly referred to as <b>checkpointing</b>.\n",
        "\n",
        "PyTorch offers <a href=\"http://pytorch.org/docs/master/notes/serialization.html\">a simple mechanism</a> to perform this operation.\n",
        "\n",
        "We implement two methods here:\n",
        "<ul>\n",
        "<li> the first one for <b> saving </b> a model,</li>\n",
        "<li> the second for <b> loading </b> a model checkpoint. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMmNpma2avmX"
      },
      "source": [
        "def save_model(epoch, model, path='./'):\n",
        "    \n",
        "    # creating the file name indexed by the epoch value\n",
        "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
        "    \n",
        "    # saving the model parameters\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZptgqQRavmZ"
      },
      "source": [
        "def load_model(epoch, model, path='./'):\n",
        "    \n",
        "    # creating the file name indexed by the epoch value\n",
        "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
        "    \n",
        "    # loading the parameters of the saved model\n",
        "    model.load_state_dict(torch.load(filename))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve8sOocWavma"
      },
      "source": [
        "It is also possible to save the optimizer's status in PyTorch, which is very important when we want to resume training the model from a given backup. For more information, please consult <a href='https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/3'>the following URL</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lcAP8-1avma"
      },
      "source": [
        "## Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keMpyePsavmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3482c7c-9f52-4a0d-8b04-ddd72324937e"
      },
      "source": [
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# maximum number of epoch\n",
        "numEpochs = 200\n",
        "\n",
        "# Saving frequency\n",
        "checkpoint_freq = 10\n",
        "\n",
        "# Directory for data backup\n",
        "path = './'\n",
        "\n",
        "# Accumulators of average losses obtained per epoch\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Performance accumulators per epoch\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Model definition\n",
        "neural_net = NeuralNet()\n",
        "\n",
        "# Load the model on the chosen device\n",
        "neural_net = neural_net.to(device)\n",
        "\n",
        "# Optimizer definition\n",
        "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) \n",
        "#optimizer = optim.SGD(neural_net.parameters(), lr=0.001) \n",
        "\n",
        "\n",
        "# Learning loop\n",
        "for epoch in range(1, numEpochs + 1):\n",
        "    \n",
        "    # train the model with the train dataset\n",
        "    train_loss, train_acc = train(epoch, neural_net, train_loader, optimizer, device)   \n",
        "\n",
        "    # voir la descente du gradient\n",
        "    last_param = list(neural_net.parameters(recurse=True))[-1]\n",
        "    print('Grad: [{:.5f},{:.5f}] Param: [{:.5f},{:.5f}]'.format(last_param.grad.tolist()[0],\n",
        "                                                                last_param.grad.tolist()[1],\n",
        "                                                                last_param.data.tolist()[0],\n",
        "                                                                last_param.data.tolist()[1]))\n",
        "\n",
        "\n",
        "    # evaluate the model with the validation dataset\n",
        "    val_loss, val_acc = evaluate(neural_net, val_loader, device)       \n",
        "    \n",
        "    # Save the losses obtained\n",
        "    train_losses.append(train_loss)    \n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    # Save the performances\n",
        "    train_accuracies.append(train_acc)    \n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    # Checkpoint\n",
        "    if epoch % checkpoint_freq ==0:\n",
        "        save_model(epoch, neural_net, path)\n",
        "\n",
        "# Save the model at the end of the training\n",
        "save_model(numEpochs, neural_net, path)\n",
        "    \n",
        "print(\"\\n\\n\\nOptimization ended.\\n\")    \n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1   Avg_Loss: 0.68869   Acc: 342/625 (54.720%)\n",
            "Grad: [-0.01831,0.02077] Param: [-0.18411,-0.11513]\n",
            "Eval:  Avg_Loss: 0.67038   Acc: 138/209 (66.029%)\n",
            "Train Epoch: 2   Avg_Loss: 0.65402   Acc: 415/625 (66.400%)\n",
            "Grad: [-0.02390,0.02761] Param: [-0.17239,-0.12826]\n",
            "Eval:  Avg_Loss: 0.64462   Acc: 137/209 (65.550%)\n",
            "Train Epoch: 3   Avg_Loss: 0.63161   Acc: 418/625 (66.880%)\n",
            "Grad: [0.02959,-0.03313] Param: [-0.16620,-0.13603]\n",
            "Eval:  Avg_Loss: 0.63230   Acc: 140/209 (66.986%)\n",
            "Train Epoch: 4   Avg_Loss: 0.62115   Acc: 416/625 (66.560%)\n",
            "Grad: [-0.00117,0.00300] Param: [-0.16458,-0.13900]\n",
            "Eval:  Avg_Loss: 0.62511   Acc: 143/209 (68.421%)\n",
            "Train Epoch: 5   Avg_Loss: 0.61499   Acc: 428/625 (68.480%)\n",
            "Grad: [-0.00901,0.00862] Param: [-0.16548,-0.13940]\n",
            "Eval:  Avg_Loss: 0.61940   Acc: 145/209 (69.378%)\n",
            "Train Epoch: 6   Avg_Loss: 0.61209   Acc: 427/625 (68.320%)\n",
            "Grad: [0.01287,-0.00081] Param: [-0.16448,-0.14118]\n",
            "Eval:  Avg_Loss: 0.61630   Acc: 145/209 (69.378%)\n",
            "Train Epoch: 7   Avg_Loss: 0.60847   Acc: 426/625 (68.160%)\n",
            "Grad: [-0.00221,0.00033] Param: [-0.16499,-0.14165]\n",
            "Eval:  Avg_Loss: 0.61552   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 8   Avg_Loss: 0.60756   Acc: 425/625 (68.000%)\n",
            "Grad: [0.00231,0.00468] Param: [-0.16585,-0.14097]\n",
            "Eval:  Avg_Loss: 0.61412   Acc: 143/209 (68.421%)\n",
            "Train Epoch: 9   Avg_Loss: 0.60589   Acc: 427/625 (68.320%)\n",
            "Grad: [-0.00355,0.00530] Param: [-0.16521,-0.14173]\n",
            "Eval:  Avg_Loss: 0.61355   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 10   Avg_Loss: 0.60352   Acc: 427/625 (68.320%)\n",
            "Grad: [-0.00869,0.01109] Param: [-0.16693,-0.14032]\n",
            "Eval:  Avg_Loss: 0.61153   Acc: 143/209 (68.421%)\n",
            "Train Epoch: 11   Avg_Loss: 0.60063   Acc: 427/625 (68.320%)\n",
            "Grad: [-0.02341,0.02793] Param: [-0.16733,-0.14009]\n",
            "Eval:  Avg_Loss: 0.60977   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 12   Avg_Loss: 0.59863   Acc: 428/625 (68.480%)\n",
            "Grad: [0.01196,-0.01801] Param: [-0.16775,-0.13901]\n",
            "Eval:  Avg_Loss: 0.60550   Acc: 147/209 (70.335%)\n",
            "Train Epoch: 13   Avg_Loss: 0.59414   Acc: 429/625 (68.640%)\n",
            "Grad: [0.01701,-0.00911] Param: [-0.16806,-0.13813]\n",
            "Eval:  Avg_Loss: 0.60235   Acc: 146/209 (69.856%)\n",
            "Train Epoch: 14   Avg_Loss: 0.58976   Acc: 437/625 (69.920%)\n",
            "Grad: [-0.01974,0.00729] Param: [-0.16876,-0.13683]\n",
            "Eval:  Avg_Loss: 0.59859   Acc: 148/209 (70.813%)\n",
            "Train Epoch: 15   Avg_Loss: 0.58556   Acc: 445/625 (71.200%)\n",
            "Grad: [-0.00190,0.00179] Param: [-0.16800,-0.13558]\n",
            "Eval:  Avg_Loss: 0.59109   Acc: 147/209 (70.335%)\n",
            "Train Epoch: 16   Avg_Loss: 0.57850   Acc: 461/625 (73.760%)\n",
            "Grad: [-0.04754,0.05380] Param: [-0.16824,-0.13307]\n",
            "Eval:  Avg_Loss: 0.58369   Acc: 151/209 (72.249%)\n",
            "Train Epoch: 17   Avg_Loss: 0.57684   Acc: 460/625 (73.600%)\n",
            "Grad: [-0.00063,0.01688] Param: [-0.16767,-0.13155]\n",
            "Eval:  Avg_Loss: 0.58478   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 18   Avg_Loss: 0.56290   Acc: 469/625 (75.040%)\n",
            "Grad: [-0.00818,0.00990] Param: [-0.16574,-0.13229]\n",
            "Eval:  Avg_Loss: 0.58345   Acc: 153/209 (73.206%)\n",
            "Train Epoch: 19   Avg_Loss: 0.55276   Acc: 473/625 (75.680%)\n",
            "Grad: [-0.00556,0.00354] Param: [-0.16592,-0.12809]\n",
            "Eval:  Avg_Loss: 0.56193   Acc: 155/209 (74.163%)\n",
            "Train Epoch: 20   Avg_Loss: 0.54498   Acc: 474/625 (75.840%)\n",
            "Grad: [0.01011,-0.00996] Param: [-0.16421,-0.12596]\n",
            "Eval:  Avg_Loss: 0.55422   Acc: 152/209 (72.727%)\n",
            "Train Epoch: 21   Avg_Loss: 0.53266   Acc: 495/625 (79.200%)\n",
            "Grad: [-0.02760,0.02914] Param: [-0.16237,-0.12479]\n",
            "Eval:  Avg_Loss: 0.56417   Acc: 158/209 (75.598%)\n",
            "Train Epoch: 22   Avg_Loss: 0.52523   Acc: 492/625 (78.720%)\n",
            "Grad: [-0.00701,0.00706] Param: [-0.16487,-0.12084]\n",
            "Eval:  Avg_Loss: 0.56631   Acc: 150/209 (71.770%)\n",
            "Train Epoch: 23   Avg_Loss: 0.54254   Acc: 479/625 (76.640%)\n",
            "Grad: [-0.01936,0.01855] Param: [-0.16321,-0.12185]\n",
            "Eval:  Avg_Loss: 0.55143   Acc: 154/209 (73.684%)\n",
            "Train Epoch: 24   Avg_Loss: 0.52622   Acc: 481/625 (76.960%)\n",
            "Grad: [-0.00131,-0.00032] Param: [-0.16368,-0.12071]\n",
            "Eval:  Avg_Loss: 0.54477   Acc: 155/209 (74.163%)\n",
            "Train Epoch: 25   Avg_Loss: 0.51382   Acc: 501/625 (80.160%)\n",
            "Grad: [0.00947,-0.01111] Param: [-0.16359,-0.12065]\n",
            "Eval:  Avg_Loss: 0.53727   Acc: 160/209 (76.555%)\n",
            "Train Epoch: 26   Avg_Loss: 0.51188   Acc: 502/625 (80.320%)\n",
            "Grad: [0.02889,-0.02562] Param: [-0.16340,-0.11959]\n",
            "Eval:  Avg_Loss: 0.53535   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 27   Avg_Loss: 0.51113   Acc: 495/625 (79.200%)\n",
            "Grad: [-0.00639,0.00675] Param: [-0.16353,-0.11962]\n",
            "Eval:  Avg_Loss: 0.53469   Acc: 159/209 (76.077%)\n",
            "Train Epoch: 28   Avg_Loss: 0.50754   Acc: 500/625 (80.000%)\n",
            "Grad: [0.00144,-0.00034] Param: [-0.16364,-0.11904]\n",
            "Eval:  Avg_Loss: 0.53097   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 29   Avg_Loss: 0.50250   Acc: 505/625 (80.800%)\n",
            "Grad: [-0.00076,0.00428] Param: [-0.16292,-0.11951]\n",
            "Eval:  Avg_Loss: 0.53116   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 30   Avg_Loss: 0.50363   Acc: 504/625 (80.640%)\n",
            "Grad: [0.00979,-0.01154] Param: [-0.16213,-0.12036]\n",
            "Eval:  Avg_Loss: 0.52973   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 31   Avg_Loss: 0.50394   Acc: 499/625 (79.840%)\n",
            "Grad: [0.02038,-0.01850] Param: [-0.16035,-0.12146]\n",
            "Eval:  Avg_Loss: 0.53791   Acc: 158/209 (75.598%)\n",
            "Train Epoch: 32   Avg_Loss: 0.50223   Acc: 505/625 (80.800%)\n",
            "Grad: [-0.01005,0.01137] Param: [-0.16252,-0.11890]\n",
            "Eval:  Avg_Loss: 0.53615   Acc: 159/209 (76.077%)\n",
            "Train Epoch: 33   Avg_Loss: 0.50525   Acc: 505/625 (80.800%)\n",
            "Grad: [0.00646,-0.00536] Param: [-0.16163,-0.11973]\n",
            "Eval:  Avg_Loss: 0.52897   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 34   Avg_Loss: 0.49894   Acc: 505/625 (80.800%)\n",
            "Grad: [-0.01033,0.01234] Param: [-0.16181,-0.11932]\n",
            "Eval:  Avg_Loss: 0.52909   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 35   Avg_Loss: 0.49699   Acc: 506/625 (80.960%)\n",
            "Grad: [0.02035,-0.02357] Param: [-0.16137,-0.11930]\n",
            "Eval:  Avg_Loss: 0.52601   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 36   Avg_Loss: 0.50312   Acc: 500/625 (80.000%)\n",
            "Grad: [-0.01686,0.01829] Param: [-0.16174,-0.11820]\n",
            "Eval:  Avg_Loss: 0.52922   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 37   Avg_Loss: 0.50092   Acc: 502/625 (80.320%)\n",
            "Grad: [0.00502,-0.00770] Param: [-0.16106,-0.11861]\n",
            "Eval:  Avg_Loss: 0.53065   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 38   Avg_Loss: 0.49644   Acc: 507/625 (81.120%)\n",
            "Grad: [0.00685,-0.00694] Param: [-0.16005,-0.11918]\n",
            "Eval:  Avg_Loss: 0.53236   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 39   Avg_Loss: 0.49709   Acc: 506/625 (80.960%)\n",
            "Grad: [0.01371,-0.01475] Param: [-0.16038,-0.11909]\n",
            "Eval:  Avg_Loss: 0.52433   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 40   Avg_Loss: 0.49572   Acc: 507/625 (81.120%)\n",
            "Grad: [-0.01163,0.01072] Param: [-0.16122,-0.11828]\n",
            "Eval:  Avg_Loss: 0.53175   Acc: 157/209 (75.120%)\n",
            "Train Epoch: 41   Avg_Loss: 0.50390   Acc: 502/625 (80.320%)\n",
            "Grad: [0.00089,-0.00169] Param: [-0.16099,-0.11812]\n",
            "Eval:  Avg_Loss: 0.52816   Acc: 161/209 (77.033%)\n",
            "Train Epoch: 42   Avg_Loss: 0.49961   Acc: 502/625 (80.320%)\n",
            "Grad: [0.00407,-0.00538] Param: [-0.16032,-0.11865]\n",
            "Eval:  Avg_Loss: 0.52401   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 43   Avg_Loss: 0.49441   Acc: 506/625 (80.960%)\n",
            "Grad: [0.00118,0.00129] Param: [-0.15996,-0.11876]\n",
            "Eval:  Avg_Loss: 0.52124   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 44   Avg_Loss: 0.49163   Acc: 513/625 (82.080%)\n",
            "Grad: [-0.00362,0.00285] Param: [-0.15949,-0.11869]\n",
            "Eval:  Avg_Loss: 0.51962   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 45   Avg_Loss: 0.48824   Acc: 510/625 (81.600%)\n",
            "Grad: [-0.01423,0.01563] Param: [-0.15931,-0.11837]\n",
            "Eval:  Avg_Loss: 0.52346   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 46   Avg_Loss: 0.49073   Acc: 512/625 (81.920%)\n",
            "Grad: [-0.02953,0.03436] Param: [-0.15870,-0.11901]\n",
            "Eval:  Avg_Loss: 0.52270   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 47   Avg_Loss: 0.48809   Acc: 516/625 (82.560%)\n",
            "Grad: [0.00245,-0.00275] Param: [-0.15736,-0.12031]\n",
            "Eval:  Avg_Loss: 0.51821   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 48   Avg_Loss: 0.48601   Acc: 513/625 (82.080%)\n",
            "Grad: [-0.01632,0.01976] Param: [-0.15615,-0.12082]\n",
            "Eval:  Avg_Loss: 0.51789   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 49   Avg_Loss: 0.48511   Acc: 517/625 (82.720%)\n",
            "Grad: [0.01395,-0.01614] Param: [-0.15601,-0.12084]\n",
            "Eval:  Avg_Loss: 0.51563   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 50   Avg_Loss: 0.48676   Acc: 515/625 (82.400%)\n",
            "Grad: [0.00760,-0.00854] Param: [-0.15564,-0.12109]\n",
            "Eval:  Avg_Loss: 0.51574   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 51   Avg_Loss: 0.48791   Acc: 514/625 (82.240%)\n",
            "Grad: [-0.00423,0.00599] Param: [-0.15652,-0.11991]\n",
            "Eval:  Avg_Loss: 0.52345   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 52   Avg_Loss: 0.49142   Acc: 507/625 (81.120%)\n",
            "Grad: [-0.00184,0.00368] Param: [-0.15602,-0.12063]\n",
            "Eval:  Avg_Loss: 0.52464   Acc: 159/209 (76.077%)\n",
            "Train Epoch: 53   Avg_Loss: 0.48541   Acc: 517/625 (82.720%)\n",
            "Grad: [-0.00524,0.00259] Param: [-0.15559,-0.12101]\n",
            "Eval:  Avg_Loss: 0.51174   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 54   Avg_Loss: 0.48478   Acc: 516/625 (82.560%)\n",
            "Grad: [-0.00004,-0.00183] Param: [-0.15399,-0.12194]\n",
            "Eval:  Avg_Loss: 0.51153   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 55   Avg_Loss: 0.48393   Acc: 518/625 (82.880%)\n",
            "Grad: [0.00477,-0.00532] Param: [-0.15393,-0.12136]\n",
            "Eval:  Avg_Loss: 0.51209   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 56   Avg_Loss: 0.48341   Acc: 517/625 (82.720%)\n",
            "Grad: [-0.00342,0.00191] Param: [-0.15349,-0.12148]\n",
            "Eval:  Avg_Loss: 0.52361   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 57   Avg_Loss: 0.49227   Acc: 505/625 (80.800%)\n",
            "Grad: [0.00948,-0.01014] Param: [-0.15320,-0.12162]\n",
            "Eval:  Avg_Loss: 0.52199   Acc: 161/209 (77.033%)\n",
            "Train Epoch: 58   Avg_Loss: 0.48093   Acc: 520/625 (83.200%)\n",
            "Grad: [-0.00052,0.00012] Param: [-0.15162,-0.12291]\n",
            "Eval:  Avg_Loss: 0.52277   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 59   Avg_Loss: 0.48420   Acc: 517/625 (82.720%)\n",
            "Grad: [0.02141,-0.02149] Param: [-0.15086,-0.12326]\n",
            "Eval:  Avg_Loss: 0.51045   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 60   Avg_Loss: 0.47931   Acc: 522/625 (83.520%)\n",
            "Grad: [-0.00085,0.00041] Param: [-0.15145,-0.12267]\n",
            "Eval:  Avg_Loss: 0.51057   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 61   Avg_Loss: 0.48225   Acc: 514/625 (82.240%)\n",
            "Grad: [0.00028,0.00005] Param: [-0.15023,-0.12396]\n",
            "Eval:  Avg_Loss: 0.51360   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 62   Avg_Loss: 0.48656   Acc: 515/625 (82.400%)\n",
            "Grad: [-0.00126,0.00127] Param: [-0.15031,-0.12370]\n",
            "Eval:  Avg_Loss: 0.50925   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 63   Avg_Loss: 0.48013   Acc: 520/625 (83.200%)\n",
            "Grad: [-0.00983,0.01142] Param: [-0.14986,-0.12415]\n",
            "Eval:  Avg_Loss: 0.50857   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 64   Avg_Loss: 0.48929   Acc: 508/625 (81.280%)\n",
            "Grad: [0.00024,-0.00097] Param: [-0.14987,-0.12412]\n",
            "Eval:  Avg_Loss: 0.51376   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 65   Avg_Loss: 0.48351   Acc: 512/625 (81.920%)\n",
            "Grad: [-0.01019,0.01214] Param: [-0.14908,-0.12449]\n",
            "Eval:  Avg_Loss: 0.50836   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 66   Avg_Loss: 0.47573   Acc: 520/625 (83.200%)\n",
            "Grad: [0.01652,-0.01852] Param: [-0.14905,-0.12476]\n",
            "Eval:  Avg_Loss: 0.51082   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 67   Avg_Loss: 0.48803   Acc: 507/625 (81.120%)\n",
            "Grad: [-0.00090,0.00031] Param: [-0.14804,-0.12602]\n",
            "Eval:  Avg_Loss: 0.50657   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 68   Avg_Loss: 0.48168   Acc: 515/625 (82.400%)\n",
            "Grad: [-0.00337,0.00446] Param: [-0.14803,-0.12618]\n",
            "Eval:  Avg_Loss: 0.50734   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 69   Avg_Loss: 0.47703   Acc: 520/625 (83.200%)\n",
            "Grad: [-0.00062,0.00119] Param: [-0.14850,-0.12575]\n",
            "Eval:  Avg_Loss: 0.50658   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 70   Avg_Loss: 0.47550   Acc: 523/625 (83.680%)\n",
            "Grad: [-0.02702,0.02453] Param: [-0.14751,-0.12675]\n",
            "Eval:  Avg_Loss: 0.51449   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 71   Avg_Loss: 0.48838   Acc: 509/625 (81.440%)\n",
            "Grad: [-0.00305,0.00345] Param: [-0.14703,-0.12680]\n",
            "Eval:  Avg_Loss: 0.50613   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 72   Avg_Loss: 0.48154   Acc: 519/625 (83.040%)\n",
            "Grad: [0.00283,0.00347] Param: [-0.14761,-0.12586]\n",
            "Eval:  Avg_Loss: 0.50970   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 73   Avg_Loss: 0.47841   Acc: 518/625 (82.880%)\n",
            "Grad: [-0.00553,0.00530] Param: [-0.14567,-0.12804]\n",
            "Eval:  Avg_Loss: 0.50314   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 74   Avg_Loss: 0.48081   Acc: 522/625 (83.520%)\n",
            "Grad: [0.00558,-0.00583] Param: [-0.14607,-0.12800]\n",
            "Eval:  Avg_Loss: 0.50867   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 75   Avg_Loss: 0.47793   Acc: 520/625 (83.200%)\n",
            "Grad: [-0.01139,0.01172] Param: [-0.14853,-0.12650]\n",
            "Eval:  Avg_Loss: 0.51504   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 76   Avg_Loss: 0.47773   Acc: 519/625 (83.040%)\n",
            "Grad: [0.00778,-0.00729] Param: [-0.14654,-0.12861]\n",
            "Eval:  Avg_Loss: 0.51003   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 77   Avg_Loss: 0.47788   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00771,-0.00941] Param: [-0.14578,-0.12927]\n",
            "Eval:  Avg_Loss: 0.50336   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 78   Avg_Loss: 0.47474   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00311,-0.00360] Param: [-0.14617,-0.12876]\n",
            "Eval:  Avg_Loss: 0.50517   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 79   Avg_Loss: 0.47569   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00827,-0.00797] Param: [-0.14668,-0.12827]\n",
            "Eval:  Avg_Loss: 0.50375   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 80   Avg_Loss: 0.47503   Acc: 524/625 (83.840%)\n",
            "Grad: [-0.00211,0.00156] Param: [-0.14656,-0.12833]\n",
            "Eval:  Avg_Loss: 0.50622   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 81   Avg_Loss: 0.47340   Acc: 526/625 (84.160%)\n",
            "Grad: [0.01259,-0.01320] Param: [-0.14679,-0.12813]\n",
            "Eval:  Avg_Loss: 0.50465   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 82   Avg_Loss: 0.47495   Acc: 522/625 (83.520%)\n",
            "Grad: [-0.00127,0.00128] Param: [-0.14643,-0.12854]\n",
            "Eval:  Avg_Loss: 0.50254   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 83   Avg_Loss: 0.47425   Acc: 524/625 (83.840%)\n",
            "Grad: [0.00782,-0.00704] Param: [-0.14706,-0.12829]\n",
            "Eval:  Avg_Loss: 0.50451   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 84   Avg_Loss: 0.47657   Acc: 522/625 (83.520%)\n",
            "Grad: [-0.00618,0.00617] Param: [-0.14766,-0.12788]\n",
            "Eval:  Avg_Loss: 0.50950   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 85   Avg_Loss: 0.47406   Acc: 523/625 (83.680%)\n",
            "Grad: [0.01125,-0.01143] Param: [-0.14675,-0.12876]\n",
            "Eval:  Avg_Loss: 0.50367   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 86   Avg_Loss: 0.47618   Acc: 527/625 (84.320%)\n",
            "Grad: [0.00777,-0.00829] Param: [-0.14598,-0.12971]\n",
            "Eval:  Avg_Loss: 0.50153   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 87   Avg_Loss: 0.47512   Acc: 519/625 (83.040%)\n",
            "Grad: [0.01438,-0.01437] Param: [-0.14570,-0.13006]\n",
            "Eval:  Avg_Loss: 0.50486   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 88   Avg_Loss: 0.47976   Acc: 522/625 (83.520%)\n",
            "Grad: [-0.00247,0.00177] Param: [-0.14702,-0.12892]\n",
            "Eval:  Avg_Loss: 0.50762   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 89   Avg_Loss: 0.48007   Acc: 517/625 (82.720%)\n",
            "Grad: [0.00051,-0.00109] Param: [-0.14601,-0.12960]\n",
            "Eval:  Avg_Loss: 0.50550   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 90   Avg_Loss: 0.47841   Acc: 521/625 (83.360%)\n",
            "Grad: [-0.00493,0.00639] Param: [-0.14540,-0.12990]\n",
            "Eval:  Avg_Loss: 0.50728   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 91   Avg_Loss: 0.48505   Acc: 513/625 (82.080%)\n",
            "Grad: [-0.01333,0.01267] Param: [-0.14558,-0.12974]\n",
            "Eval:  Avg_Loss: 0.50424   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 92   Avg_Loss: 0.47904   Acc: 514/625 (82.240%)\n",
            "Grad: [0.00342,-0.00361] Param: [-0.14491,-0.13064]\n",
            "Eval:  Avg_Loss: 0.50305   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 93   Avg_Loss: 0.47381   Acc: 523/625 (83.680%)\n",
            "Grad: [-0.00643,0.00667] Param: [-0.14489,-0.13089]\n",
            "Eval:  Avg_Loss: 0.50428   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 94   Avg_Loss: 0.47134   Acc: 529/625 (84.640%)\n",
            "Grad: [0.00512,-0.00512] Param: [-0.14429,-0.13149]\n",
            "Eval:  Avg_Loss: 0.50486   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 95   Avg_Loss: 0.47117   Acc: 526/625 (84.160%)\n",
            "Grad: [0.00559,-0.00643] Param: [-0.14524,-0.13065]\n",
            "Eval:  Avg_Loss: 0.51035   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 96   Avg_Loss: 0.48563   Acc: 514/625 (82.240%)\n",
            "Grad: [0.01274,-0.01757] Param: [-0.14361,-0.13195]\n",
            "Eval:  Avg_Loss: 0.51628   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 97   Avg_Loss: 0.48924   Acc: 507/625 (81.120%)\n",
            "Grad: [0.00594,-0.00655] Param: [-0.14073,-0.13400]\n",
            "Eval:  Avg_Loss: 0.50250   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 98   Avg_Loss: 0.48200   Acc: 517/625 (82.720%)\n",
            "Grad: [0.01052,-0.01121] Param: [-0.14233,-0.13226]\n",
            "Eval:  Avg_Loss: 0.50759   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 99   Avg_Loss: 0.48651   Acc: 511/625 (81.760%)\n",
            "Grad: [0.00785,-0.00772] Param: [-0.14086,-0.13362]\n",
            "Eval:  Avg_Loss: 0.52165   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 100   Avg_Loss: 0.48663   Acc: 517/625 (82.720%)\n",
            "Grad: [-0.00084,0.00021] Param: [-0.14202,-0.13202]\n",
            "Eval:  Avg_Loss: 0.50333   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 101   Avg_Loss: 0.47578   Acc: 522/625 (83.520%)\n",
            "Grad: [-0.01433,0.01503] Param: [-0.14363,-0.13024]\n",
            "Eval:  Avg_Loss: 0.51385   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 102   Avg_Loss: 0.48150   Acc: 516/625 (82.560%)\n",
            "Grad: [-0.00121,0.00187] Param: [-0.14298,-0.13092]\n",
            "Eval:  Avg_Loss: 0.51053   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 103   Avg_Loss: 0.48160   Acc: 514/625 (82.240%)\n",
            "Grad: [0.00606,-0.00624] Param: [-0.14073,-0.13309]\n",
            "Eval:  Avg_Loss: 0.50488   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 104   Avg_Loss: 0.47561   Acc: 521/625 (83.360%)\n",
            "Grad: [-0.00165,0.00436] Param: [-0.14151,-0.13274]\n",
            "Eval:  Avg_Loss: 0.51060   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 105   Avg_Loss: 0.48044   Acc: 514/625 (82.240%)\n",
            "Grad: [-0.01060,0.01056] Param: [-0.14266,-0.13213]\n",
            "Eval:  Avg_Loss: 0.51316   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 106   Avg_Loss: 0.48317   Acc: 517/625 (82.720%)\n",
            "Grad: [0.01530,-0.01536] Param: [-0.14066,-0.13427]\n",
            "Eval:  Avg_Loss: 0.50227   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 107   Avg_Loss: 0.46895   Acc: 528/625 (84.480%)\n",
            "Grad: [-0.00428,0.00589] Param: [-0.14225,-0.13274]\n",
            "Eval:  Avg_Loss: 0.51105   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 108   Avg_Loss: 0.47199   Acc: 522/625 (83.520%)\n",
            "Grad: [-0.00537,0.00601] Param: [-0.14057,-0.13428]\n",
            "Eval:  Avg_Loss: 0.50408   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 109   Avg_Loss: 0.47205   Acc: 528/625 (84.480%)\n",
            "Grad: [0.00868,-0.00839] Param: [-0.14102,-0.13400]\n",
            "Eval:  Avg_Loss: 0.50778   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 110   Avg_Loss: 0.47329   Acc: 524/625 (83.840%)\n",
            "Grad: [0.00784,-0.00820] Param: [-0.14220,-0.13274]\n",
            "Eval:  Avg_Loss: 0.51273   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 111   Avg_Loss: 0.47573   Acc: 516/625 (82.560%)\n",
            "Grad: [-0.00615,0.00364] Param: [-0.14039,-0.13408]\n",
            "Eval:  Avg_Loss: 0.51384   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 112   Avg_Loss: 0.47902   Acc: 513/625 (82.080%)\n",
            "Grad: [-0.01766,0.01971] Param: [-0.14187,-0.13248]\n",
            "Eval:  Avg_Loss: 0.51966   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 113   Avg_Loss: 0.47726   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00857,-0.00792] Param: [-0.13898,-0.13584]\n",
            "Eval:  Avg_Loss: 0.50139   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 114   Avg_Loss: 0.47426   Acc: 523/625 (83.680%)\n",
            "Grad: [0.00146,-0.00181] Param: [-0.13941,-0.13549]\n",
            "Eval:  Avg_Loss: 0.50771   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 115   Avg_Loss: 0.47038   Acc: 528/625 (84.480%)\n",
            "Grad: [-0.00646,0.00727] Param: [-0.14002,-0.13505]\n",
            "Eval:  Avg_Loss: 0.50880   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 116   Avg_Loss: 0.46957   Acc: 526/625 (84.160%)\n",
            "Grad: [-0.02067,0.01622] Param: [-0.14028,-0.13501]\n",
            "Eval:  Avg_Loss: 0.50665   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 117   Avg_Loss: 0.47468   Acc: 521/625 (83.360%)\n",
            "Grad: [-0.01078,0.01168] Param: [-0.14031,-0.13496]\n",
            "Eval:  Avg_Loss: 0.50471   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 118   Avg_Loss: 0.47122   Acc: 524/625 (83.840%)\n",
            "Grad: [0.00736,-0.00593] Param: [-0.14029,-0.13530]\n",
            "Eval:  Avg_Loss: 0.50480   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 119   Avg_Loss: 0.47157   Acc: 524/625 (83.840%)\n",
            "Grad: [-0.00712,0.00866] Param: [-0.14101,-0.13497]\n",
            "Eval:  Avg_Loss: 0.51074   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 120   Avg_Loss: 0.47180   Acc: 526/625 (84.160%)\n",
            "Grad: [-0.00181,0.00204] Param: [-0.14132,-0.13550]\n",
            "Eval:  Avg_Loss: 0.50659   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 121   Avg_Loss: 0.47044   Acc: 529/625 (84.640%)\n",
            "Grad: [-0.00177,0.00174] Param: [-0.14179,-0.13538]\n",
            "Eval:  Avg_Loss: 0.50616   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 122   Avg_Loss: 0.47078   Acc: 527/625 (84.320%)\n",
            "Grad: [-0.01104,0.01239] Param: [-0.14129,-0.13584]\n",
            "Eval:  Avg_Loss: 0.50608   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 123   Avg_Loss: 0.47382   Acc: 519/625 (83.040%)\n",
            "Grad: [0.01931,-0.01853] Param: [-0.14274,-0.13439]\n",
            "Eval:  Avg_Loss: 0.50898   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 124   Avg_Loss: 0.47284   Acc: 523/625 (83.680%)\n",
            "Grad: [0.01195,-0.01134] Param: [-0.14294,-0.13387]\n",
            "Eval:  Avg_Loss: 0.50604   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 125   Avg_Loss: 0.47080   Acc: 528/625 (84.480%)\n",
            "Grad: [-0.00015,0.00028] Param: [-0.14305,-0.13402]\n",
            "Eval:  Avg_Loss: 0.50545   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 126   Avg_Loss: 0.46945   Acc: 528/625 (84.480%)\n",
            "Grad: [-0.00054,0.00090] Param: [-0.14280,-0.13436]\n",
            "Eval:  Avg_Loss: 0.50525   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 127   Avg_Loss: 0.47233   Acc: 524/625 (83.840%)\n",
            "Grad: [-0.00824,0.00782] Param: [-0.14402,-0.13354]\n",
            "Eval:  Avg_Loss: 0.50987   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 128   Avg_Loss: 0.47189   Acc: 524/625 (83.840%)\n",
            "Grad: [0.01187,-0.01193] Param: [-0.14223,-0.13569]\n",
            "Eval:  Avg_Loss: 0.50758   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 129   Avg_Loss: 0.47288   Acc: 522/625 (83.520%)\n",
            "Grad: [-0.02287,0.02497] Param: [-0.14329,-0.13468]\n",
            "Eval:  Avg_Loss: 0.50591   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 130   Avg_Loss: 0.47666   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00082,-0.00342] Param: [-0.14436,-0.13367]\n",
            "Eval:  Avg_Loss: 0.52834   Acc: 157/209 (75.120%)\n",
            "Train Epoch: 131   Avg_Loss: 0.49055   Acc: 510/625 (81.600%)\n",
            "Grad: [0.00242,-0.00349] Param: [-0.14073,-0.13686]\n",
            "Eval:  Avg_Loss: 0.50667   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 132   Avg_Loss: 0.47501   Acc: 519/625 (83.040%)\n",
            "Grad: [-0.00037,0.00030] Param: [-0.14062,-0.13676]\n",
            "Eval:  Avg_Loss: 0.50724   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 133   Avg_Loss: 0.47035   Acc: 528/625 (84.480%)\n",
            "Grad: [-0.01039,0.01106] Param: [-0.14115,-0.13666]\n",
            "Eval:  Avg_Loss: 0.50851   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 134   Avg_Loss: 0.47326   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00780,-0.00642] Param: [-0.14093,-0.13716]\n",
            "Eval:  Avg_Loss: 0.50885   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 135   Avg_Loss: 0.47783   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00651,-0.00385] Param: [-0.14012,-0.13876]\n",
            "Eval:  Avg_Loss: 0.51333   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 136   Avg_Loss: 0.47533   Acc: 519/625 (83.040%)\n",
            "Grad: [-0.00070,0.00190] Param: [-0.14103,-0.13805]\n",
            "Eval:  Avg_Loss: 0.50944   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 137   Avg_Loss: 0.47329   Acc: 523/625 (83.680%)\n",
            "Grad: [-0.00050,-0.00173] Param: [-0.14187,-0.13759]\n",
            "Eval:  Avg_Loss: 0.50998   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 138   Avg_Loss: 0.47223   Acc: 522/625 (83.520%)\n",
            "Grad: [0.00137,-0.00160] Param: [-0.14263,-0.13679]\n",
            "Eval:  Avg_Loss: 0.50836   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 139   Avg_Loss: 0.47567   Acc: 522/625 (83.520%)\n",
            "Grad: [0.00035,-0.00043] Param: [-0.14163,-0.13757]\n",
            "Eval:  Avg_Loss: 0.50504   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 140   Avg_Loss: 0.47590   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00005,-0.00013] Param: [-0.14260,-0.13656]\n",
            "Eval:  Avg_Loss: 0.51326   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 141   Avg_Loss: 0.47695   Acc: 521/625 (83.360%)\n",
            "Grad: [-0.00174,0.00288] Param: [-0.14201,-0.13678]\n",
            "Eval:  Avg_Loss: 0.50835   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 142   Avg_Loss: 0.46822   Acc: 529/625 (84.640%)\n",
            "Grad: [0.00940,-0.00912] Param: [-0.14096,-0.13798]\n",
            "Eval:  Avg_Loss: 0.50558   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 143   Avg_Loss: 0.47050   Acc: 526/625 (84.160%)\n",
            "Grad: [0.00559,-0.00466] Param: [-0.14134,-0.13795]\n",
            "Eval:  Avg_Loss: 0.50749   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 144   Avg_Loss: 0.47131   Acc: 527/625 (84.320%)\n",
            "Grad: [0.01121,-0.01126] Param: [-0.13999,-0.13957]\n",
            "Eval:  Avg_Loss: 0.50786   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 145   Avg_Loss: 0.47193   Acc: 522/625 (83.520%)\n",
            "Grad: [0.01172,-0.01245] Param: [-0.14040,-0.13923]\n",
            "Eval:  Avg_Loss: 0.50987   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 146   Avg_Loss: 0.47148   Acc: 526/625 (84.160%)\n",
            "Grad: [-0.00617,0.00675] Param: [-0.14061,-0.13929]\n",
            "Eval:  Avg_Loss: 0.51214   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 147   Avg_Loss: 0.47148   Acc: 523/625 (83.680%)\n",
            "Grad: [0.00073,-0.00107] Param: [-0.14057,-0.13893]\n",
            "Eval:  Avg_Loss: 0.50796   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 148   Avg_Loss: 0.46699   Acc: 530/625 (84.800%)\n",
            "Grad: [-0.01367,0.01396] Param: [-0.14070,-0.13925]\n",
            "Eval:  Avg_Loss: 0.50765   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 149   Avg_Loss: 0.47047   Acc: 523/625 (83.680%)\n",
            "Grad: [0.00797,-0.01410] Param: [-0.14098,-0.14004]\n",
            "Eval:  Avg_Loss: 0.51241   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 150   Avg_Loss: 0.47421   Acc: 521/625 (83.360%)\n",
            "Grad: [0.01124,-0.00855] Param: [-0.14040,-0.14046]\n",
            "Eval:  Avg_Loss: 0.51038   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 151   Avg_Loss: 0.47021   Acc: 524/625 (83.840%)\n",
            "Grad: [-0.00110,0.00108] Param: [-0.14151,-0.14083]\n",
            "Eval:  Avg_Loss: 0.50961   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 152   Avg_Loss: 0.47292   Acc: 523/625 (83.680%)\n",
            "Grad: [-0.01006,0.01055] Param: [-0.14055,-0.14227]\n",
            "Eval:  Avg_Loss: 0.50766   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 153   Avg_Loss: 0.46900   Acc: 528/625 (84.480%)\n",
            "Grad: [-0.00056,0.00076] Param: [-0.14025,-0.14303]\n",
            "Eval:  Avg_Loss: 0.50120   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 154   Avg_Loss: 0.47012   Acc: 525/625 (84.000%)\n",
            "Grad: [-0.00405,0.00356] Param: [-0.14190,-0.14173]\n",
            "Eval:  Avg_Loss: 0.50904   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 155   Avg_Loss: 0.46733   Acc: 527/625 (84.320%)\n",
            "Grad: [-0.00059,0.00091] Param: [-0.14404,-0.14029]\n",
            "Eval:  Avg_Loss: 0.51031   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 156   Avg_Loss: 0.46888   Acc: 528/625 (84.480%)\n",
            "Grad: [-0.00627,0.00636] Param: [-0.14383,-0.14066]\n",
            "Eval:  Avg_Loss: 0.51255   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 157   Avg_Loss: 0.46994   Acc: 527/625 (84.320%)\n",
            "Grad: [-0.00184,0.00207] Param: [-0.14158,-0.14220]\n",
            "Eval:  Avg_Loss: 0.50060   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 158   Avg_Loss: 0.46667   Acc: 529/625 (84.640%)\n",
            "Grad: [-0.00656,0.00622] Param: [-0.14339,-0.14109]\n",
            "Eval:  Avg_Loss: 0.50869   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 159   Avg_Loss: 0.47277   Acc: 524/625 (83.840%)\n",
            "Grad: [0.00005,-0.00083] Param: [-0.14242,-0.14185]\n",
            "Eval:  Avg_Loss: 0.51033   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 160   Avg_Loss: 0.47425   Acc: 520/625 (83.200%)\n",
            "Grad: [-0.00133,0.00164] Param: [-0.14127,-0.14289]\n",
            "Eval:  Avg_Loss: 0.50860   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 161   Avg_Loss: 0.46946   Acc: 525/625 (84.000%)\n",
            "Grad: [-0.00110,0.00184] Param: [-0.14103,-0.14347]\n",
            "Eval:  Avg_Loss: 0.50919   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 162   Avg_Loss: 0.46582   Acc: 532/625 (85.120%)\n",
            "Grad: [-0.01068,0.01066] Param: [-0.14184,-0.14327]\n",
            "Eval:  Avg_Loss: 0.51018   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 163   Avg_Loss: 0.46886   Acc: 525/625 (84.000%)\n",
            "Grad: [0.00230,-0.00272] Param: [-0.14191,-0.14353]\n",
            "Eval:  Avg_Loss: 0.50849   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 164   Avg_Loss: 0.47196   Acc: 525/625 (84.000%)\n",
            "Grad: [0.02260,-0.02227] Param: [-0.14168,-0.14416]\n",
            "Eval:  Avg_Loss: 0.50549   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 165   Avg_Loss: 0.46850   Acc: 524/625 (83.840%)\n",
            "Grad: [0.00166,-0.00217] Param: [-0.14263,-0.14394]\n",
            "Eval:  Avg_Loss: 0.50979   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 166   Avg_Loss: 0.47270   Acc: 523/625 (83.680%)\n",
            "Grad: [0.00201,-0.00282] Param: [-0.14384,-0.14285]\n",
            "Eval:  Avg_Loss: 0.51329   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 167   Avg_Loss: 0.47257   Acc: 525/625 (84.000%)\n",
            "Grad: [0.00008,-0.00016] Param: [-0.14153,-0.14480]\n",
            "Eval:  Avg_Loss: 0.50463   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 168   Avg_Loss: 0.47445   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00714,-0.00734] Param: [-0.14225,-0.14393]\n",
            "Eval:  Avg_Loss: 0.50814   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 169   Avg_Loss: 0.46846   Acc: 528/625 (84.480%)\n",
            "Grad: [-0.00000,-0.00000] Param: [-0.14369,-0.14275]\n",
            "Eval:  Avg_Loss: 0.51208   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 170   Avg_Loss: 0.46896   Acc: 528/625 (84.480%)\n",
            "Grad: [0.00372,-0.00439] Param: [-0.14168,-0.14508]\n",
            "Eval:  Avg_Loss: 0.51636   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 171   Avg_Loss: 0.47660   Acc: 520/625 (83.200%)\n",
            "Grad: [-0.00195,0.00192] Param: [-0.14192,-0.14459]\n",
            "Eval:  Avg_Loss: 0.50534   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 172   Avg_Loss: 0.47009   Acc: 527/625 (84.320%)\n",
            "Grad: [0.00406,-0.00394] Param: [-0.14342,-0.14312]\n",
            "Eval:  Avg_Loss: 0.51104   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 173   Avg_Loss: 0.46563   Acc: 529/625 (84.640%)\n",
            "Grad: [-0.01300,0.01491] Param: [-0.14489,-0.14167]\n",
            "Eval:  Avg_Loss: 0.51175   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 174   Avg_Loss: 0.46870   Acc: 520/625 (83.200%)\n",
            "Grad: [0.00076,-0.00163] Param: [-0.14217,-0.14477]\n",
            "Eval:  Avg_Loss: 0.50138   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 175   Avg_Loss: 0.46993   Acc: 522/625 (83.520%)\n",
            "Grad: [-0.00088,0.00103] Param: [-0.14444,-0.14295]\n",
            "Eval:  Avg_Loss: 0.51086   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 176   Avg_Loss: 0.46734   Acc: 524/625 (83.840%)\n",
            "Grad: [0.00473,-0.00523] Param: [-0.14381,-0.14369]\n",
            "Eval:  Avg_Loss: 0.50812   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 177   Avg_Loss: 0.46518   Acc: 531/625 (84.960%)\n",
            "Grad: [0.00880,-0.00867] Param: [-0.14422,-0.14330]\n",
            "Eval:  Avg_Loss: 0.51122   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 178   Avg_Loss: 0.46428   Acc: 529/625 (84.640%)\n",
            "Grad: [-0.00112,0.00089] Param: [-0.14486,-0.14272]\n",
            "Eval:  Avg_Loss: 0.51126   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 179   Avg_Loss: 0.47723   Acc: 521/625 (83.360%)\n",
            "Grad: [-0.00784,0.00791] Param: [-0.14501,-0.14238]\n",
            "Eval:  Avg_Loss: 0.51955   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 180   Avg_Loss: 0.47056   Acc: 525/625 (84.000%)\n",
            "Grad: [-0.00622,0.00562] Param: [-0.14414,-0.14301]\n",
            "Eval:  Avg_Loss: 0.49810   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 181   Avg_Loss: 0.46742   Acc: 529/625 (84.640%)\n",
            "Grad: [-0.00807,0.00891] Param: [-0.14554,-0.14177]\n",
            "Eval:  Avg_Loss: 0.50498   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 182   Avg_Loss: 0.47141   Acc: 525/625 (84.000%)\n",
            "Grad: [0.00955,-0.00893] Param: [-0.14380,-0.14339]\n",
            "Eval:  Avg_Loss: 0.49891   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 183   Avg_Loss: 0.46736   Acc: 530/625 (84.800%)\n",
            "Grad: [-0.00000,-0.00132] Param: [-0.14599,-0.14111]\n",
            "Eval:  Avg_Loss: 0.51049   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 184   Avg_Loss: 0.46789   Acc: 527/625 (84.320%)\n",
            "Grad: [0.00695,-0.00763] Param: [-0.14596,-0.14105]\n",
            "Eval:  Avg_Loss: 0.51084   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 185   Avg_Loss: 0.46374   Acc: 532/625 (85.120%)\n",
            "Grad: [-0.00636,0.00560] Param: [-0.14630,-0.14068]\n",
            "Eval:  Avg_Loss: 0.51058   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 186   Avg_Loss: 0.46564   Acc: 527/625 (84.320%)\n",
            "Grad: [0.00152,-0.00041] Param: [-0.14536,-0.14120]\n",
            "Eval:  Avg_Loss: 0.50917   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 187   Avg_Loss: 0.46522   Acc: 529/625 (84.640%)\n",
            "Grad: [0.00701,-0.00694] Param: [-0.14573,-0.14097]\n",
            "Eval:  Avg_Loss: 0.50768   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 188   Avg_Loss: 0.46483   Acc: 529/625 (84.640%)\n",
            "Grad: [-0.00271,0.00347] Param: [-0.14615,-0.14036]\n",
            "Eval:  Avg_Loss: 0.51060   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 189   Avg_Loss: 0.46519   Acc: 528/625 (84.480%)\n",
            "Grad: [0.00757,-0.00669] Param: [-0.14480,-0.14145]\n",
            "Eval:  Avg_Loss: 0.51052   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 190   Avg_Loss: 0.46734   Acc: 528/625 (84.480%)\n",
            "Grad: [0.00537,-0.00594] Param: [-0.14542,-0.14025]\n",
            "Eval:  Avg_Loss: 0.51030   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 191   Avg_Loss: 0.46908   Acc: 527/625 (84.320%)\n",
            "Grad: [0.00792,-0.00667] Param: [-0.14356,-0.14221]\n",
            "Eval:  Avg_Loss: 0.50006   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 192   Avg_Loss: 0.46812   Acc: 523/625 (83.680%)\n",
            "Grad: [-0.00785,0.00585] Param: [-0.14699,-0.13951]\n",
            "Eval:  Avg_Loss: 0.51183   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 193   Avg_Loss: 0.47350   Acc: 520/625 (83.200%)\n",
            "Grad: [-0.00030,0.00032] Param: [-0.14500,-0.14188]\n",
            "Eval:  Avg_Loss: 0.51411   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 194   Avg_Loss: 0.47933   Acc: 520/625 (83.200%)\n",
            "Grad: [-0.01120,0.01082] Param: [-0.14221,-0.14492]\n",
            "Eval:  Avg_Loss: 0.50266   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 195   Avg_Loss: 0.47286   Acc: 521/625 (83.360%)\n",
            "Grad: [0.01393,-0.01496] Param: [-0.14453,-0.14291]\n",
            "Eval:  Avg_Loss: 0.50972   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 196   Avg_Loss: 0.46593   Acc: 526/625 (84.160%)\n",
            "Grad: [-0.00048,-0.00026] Param: [-0.14384,-0.14356]\n",
            "Eval:  Avg_Loss: 0.51057   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 197   Avg_Loss: 0.46999   Acc: 524/625 (83.840%)\n",
            "Grad: [0.00580,-0.00673] Param: [-0.14373,-0.14323]\n",
            "Eval:  Avg_Loss: 0.50903   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 198   Avg_Loss: 0.46942   Acc: 527/625 (84.320%)\n",
            "Grad: [-0.00576,0.00604] Param: [-0.14281,-0.14362]\n",
            "Eval:  Avg_Loss: 0.50939   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 199   Avg_Loss: 0.46565   Acc: 527/625 (84.320%)\n",
            "Grad: [-0.00549,0.00498] Param: [-0.14299,-0.14326]\n",
            "Eval:  Avg_Loss: 0.51041   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 200   Avg_Loss: 0.46437   Acc: 530/625 (84.800%)\n",
            "Grad: [0.00246,-0.00244] Param: [-0.14198,-0.14392]\n",
            "Eval:  Avg_Loss: 0.50803   Acc: 166/209 (79.426%)\n",
            "\n",
            "\n",
            "\n",
            "Optimization ended.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIuAjgVs28Wj",
        "outputId": "5a4136f1-6bc0-47ad-9fbd-5877eb4209a3"
      },
      "source": [
        "size = len(list(neural_net.parameters(recurse=True)))\n",
        "i = 1\n",
        "for param in neural_net.parameters(recurse=True):\n",
        "  print(type(param), param.size())\n",
        "  if i == size:\n",
        "    print(param.data)\n",
        "    print(param.grad)\n",
        "  i = i+1\n",
        "\n",
        "last_param = list(neural_net.parameters(recurse=True))[-1]\n",
        "print('Grad: [{:.5f},{:.5f}] Param: [{:.5f},{:.5f}]'.format(last_param.grad.tolist()[0],\n",
        "                                                                last_param.grad.tolist()[1],\n",
        "                                                                last_param.data.tolist()[0],\n",
        "                                                                last_param.data.tolist()[1]))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.nn.parameter.Parameter'> torch.Size([20, 12])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([20])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([40, 20])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([40])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([20, 40])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([20])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2, 20])\n",
            "<class 'torch.nn.parameter.Parameter'> torch.Size([2])\n",
            "tensor([-0.1420, -0.1439], device='cuda:0')\n",
            "tensor([ 0.0025, -0.0024], device='cuda:0')\n",
            "Grad: [0.00246,-0.00244] Param: [-0.14198,-0.14392]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86OZRLrjavmd"
      },
      "source": [
        "## Interpreting the output of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mklvQruYavme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a8d2c8-61ef-41b9-fe30-5dfc3155add5"
      },
      "source": [
        "# Activate the evaluation mode\n",
        "neural_net = neural_net.eval()\n",
        "\n",
        "# Select the first 10 data points of the validation set\n",
        "data, target = val_dataset[0:10]\n",
        "data = data.to(device)\n",
        "\n",
        "# Executing the neural network\n",
        "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
        "\n",
        "# Transform the output into a probability distribution with a softmax function\n",
        "output_proba = F.softmax(output, dim=1)\n",
        "\n",
        "# Print the probability\n",
        "print(output_proba)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.7310, 0.2690],\n",
            "        [0.7295, 0.2705],\n",
            "        [0.7306, 0.2694],\n",
            "        [0.7308, 0.2692],\n",
            "        [0.2689, 0.7311],\n",
            "        [0.7311, 0.2689],\n",
            "        [0.2689, 0.7311],\n",
            "        [0.7305, 0.2695],\n",
            "        [0.3067, 0.6933],\n",
            "        [0.7310, 0.2690]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvIEqKt0qjeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7293ea-ee8f-44b5-fe6b-0840b02816c3"
      },
      "source": [
        "# For each example, retrieve the class with the highest probability.\n",
        "dummy, prediction = torch.max(output_proba, dim=1)\n",
        "\n",
        "print(\"Model predictions\")\n",
        "print(prediction)\n",
        "print(dummy)\n",
        "print(\"Targets\")\n",
        "print(target)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model predictions\n",
            "tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "tensor([0.7310, 0.7295, 0.7306, 0.7308, 0.7311, 0.7311, 0.7311, 0.7305, 0.6933,\n",
            "        0.7310], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "Targets\n",
            "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn8s38SJio1W",
        "outputId": "4d2862a5-6a65-48eb-d14e-1148c513bb61"
      },
      "source": [
        "F1_score = f1_score(target.cpu().numpy(), prediction.cpu().numpy())\n",
        "print(f'F1 score is {F1_score:.2%}'.format(F1_score))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score is 60.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V11J3Jihavmy"
      },
      "source": [
        "## Visualizing of the learning curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9_9C_tXavmz"
      },
      "source": [
        "Learning curves allow detecting problems that may have occurred during learning, for example, unstable learning, underfitting, or overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNcbpl0tavm0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "680f38f5-e6d1-4ac7-8410-24c5e66701c6"
      },
      "source": [
        "x = list(range(len(train_losses)))\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(x, train_losses, 'r', label=\"Train\")\n",
        "plt.plot(x, val_losses, 'g', label=\"Validation\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Cross-entropy loss')\n",
        "plt.grid()\n",
        "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
        "leg.get_frame().set_alpha(0.99)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-VGQ2pMavm4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "11e44b6e-64d3-4a5d-b60c-481bdf77858c"
      },
      "source": [
        "x = list(range(len(train_accuracies)))\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(x, train_accuracies, 'r', label=\"Train\")\n",
        "plt.plot(x, val_accuracies, 'g', label=\"Validation\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
        "leg.get_frame().set_alpha(0.99)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oktkpkuqjet"
      },
      "source": [
        "### Exercise 6\n",
        "\n",
        "*   What do you observe in the previous graphs?\n",
        "*   At which epoch is it interesting to retrieve the model parameters for inference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwhRt39yzug-"
      },
      "source": [
        "Stop around epoch 75 to not overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK_eUsq3avm8"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UREO5elavm8"
      },
      "source": [
        "We can finally evaluate our model on our test set and compare with the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UZPvzjxlusr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13fb104-cd31-4a2f-ed72-8cd3053c4bcb"
      },
      "source": [
        "valid_loss, valid_acc = evaluate(neural_net, val_loader, device)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval:  Avg_Loss: 0.50803   Acc: 166/209 (79.426%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPWvDM-qavm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77259e3-00ee-4993-cea2-d5e35f7f2d48"
      },
      "source": [
        "test_loss, test_acc = evaluate(neural_net, test_loader, device)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval:  Avg_Loss: 0.50964   Acc: 166/209 (79.426%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvP_-KUwqjez"
      },
      "source": [
        "### Exercise 7\n",
        "\n",
        "* Compare validation and test metrics.\n",
        "* Do you think the chosen features are informative of the fate of the passenger?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov4CKUFh5tun"
      },
      "source": [
        "... # To complete.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfUHiLk5jbC8",
        "outputId": "c3e3ac5c-8b48-41a0-8738-c96a49c1d8a2"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.8.1+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rueFjis6YlLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "outputId": "dcaf6eb9-83ed-45c9-de32-e2ce6e787ec1"
      },
      "source": [
        "import graphviz\n",
        "import torchviz\n",
        "\n",
        "torchviz.make_dot(output,params=dict(neural_net.named_parameters()))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f678ccf3910>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"450pt\" height=\"732pt\"\n viewBox=\"0.00 0.00 450.00 732.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 728)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-728 446,-728 446,4 -4,4\"/>\n<!-- 140082735367264 -->\n<g id=\"node1\" class=\"node\">\n<title>140082735367264</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"251,-31 186,-31 186,0 251,0 251,-31\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10, 2)</text>\n</g>\n<!-- 140082720749968 -->\n<g id=\"node2\" class=\"node\">\n<title>140082720749968</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-86 165,-86 165,-67 272,-67 272,-86\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SigmoidBackward</text>\n</g>\n<!-- 140082720749968&#45;&gt;140082735367264 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140082720749968&#45;&gt;140082735367264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-66.9688C218.5,-60.1289 218.5,-50.5621 218.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-41.3678 218.5,-31.3678 215.0001,-41.3678 222.0001,-41.3678\"/>\n</g>\n<!-- 140082720749776 -->\n<g id=\"node3\" class=\"node\">\n<title>140082720749776</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"266,-141 171,-141 171,-122 266,-122 266,-141\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140082720749776&#45;&gt;140082720749968 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140082720749776&#45;&gt;140082720749968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-121.9197C218.5,-114.9083 218.5,-105.1442 218.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-96.3408 218.5,-86.3408 215.0001,-96.3409 222.0001,-96.3408\"/>\n</g>\n<!-- 140082720750096 -->\n<g id=\"node4\" class=\"node\">\n<title>140082720750096</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"153,-196 52,-196 52,-177 153,-177 153,-196\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750096&#45;&gt;140082720749776 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140082720750096&#45;&gt;140082720749776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M122.7057,-176.9197C140.9737,-168.2581 168.1018,-155.3957 188.8304,-145.5675\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"190.6004,-148.6018 198.1367,-141.155 187.6014,-142.2768 190.6004,-148.6018\"/>\n</g>\n<!-- 140083271789296 -->\n<g id=\"node5\" class=\"node\">\n<title>140083271789296</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"144,-262 61,-262 61,-232 144,-232 144,-262\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">output.bias</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2)</text>\n</g>\n<!-- 140083271789296&#45;&gt;140082720750096 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140083271789296&#45;&gt;140082720750096</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M102.5,-231.7333C102.5,-224.0322 102.5,-214.5977 102.5,-206.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0001,-206.0864 102.5,-196.0864 99.0001,-206.0864 106.0001,-206.0864\"/>\n</g>\n<!-- 140082720750288 -->\n<g id=\"node6\" class=\"node\">\n<title>140082720750288</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"266,-196 171,-196 171,-177 266,-177 266,-196\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140082720750288&#45;&gt;140082720749776 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140082720750288&#45;&gt;140082720749776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-176.9197C218.5,-169.9083 218.5,-160.1442 218.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-151.3408 218.5,-141.3408 215.0001,-151.3409 222.0001,-151.3408\"/>\n</g>\n<!-- 140082720749904 -->\n<g id=\"node7\" class=\"node\">\n<title>140082720749904</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"261,-256.5 166,-256.5 166,-237.5 261,-237.5 261,-256.5\"/>\n<text text-anchor=\"middle\" x=\"213.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140082720749904&#45;&gt;140082720750288 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140082720749904&#45;&gt;140082720750288</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M214.3033,-237.2796C214.9845,-229.0376 215.9838,-216.9457 216.8364,-206.629\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"220.3464,-206.6516 217.682,-196.3972 213.3702,-206.075 220.3464,-206.6516\"/>\n</g>\n<!-- 140082720750544 -->\n<g id=\"node8\" class=\"node\">\n<title>140082720750544</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"127,-322.5 26,-322.5 26,-303.5 127,-303.5 127,-322.5\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750544&#45;&gt;140082720749904 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140082720750544&#45;&gt;140082720749904</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M96.421,-303.403C119.5543,-292.2585 157.9168,-273.7773 184.3899,-261.0238\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.065,-264.1019 193.555,-256.6085 183.0268,-257.7956 186.065,-264.1019\"/>\n</g>\n<!-- 140083271789856 -->\n<g id=\"node9\" class=\"node\">\n<title>140083271789856</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"118,-394 35,-394 35,-364 118,-364 118,-394\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer3.bias</text>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140083271789856&#45;&gt;140082720750544 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140083271789856&#45;&gt;140082720750544</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M76.5,-363.6924C76.5,-354.5067 76.5,-342.7245 76.5,-332.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"80.0001,-332.703 76.5,-322.7031 73.0001,-332.7031 80.0001,-332.703\"/>\n</g>\n<!-- 140082720750672 -->\n<g id=\"node10\" class=\"node\">\n<title>140082720750672</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"240,-322.5 145,-322.5 145,-303.5 240,-303.5 240,-322.5\"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140082720750672&#45;&gt;140082720749904 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140082720750672&#45;&gt;140082720749904</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M195.5986,-303.2615C198.6695,-293.6102 203.4588,-278.558 207.3112,-266.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"210.6918,-267.369 210.3887,-256.7785 204.0213,-265.2465 210.6918,-267.369\"/>\n</g>\n<!-- 140082720750416 -->\n<g id=\"node11\" class=\"node\">\n<title>140082720750416</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"235,-388.5 140,-388.5 140,-369.5 235,-369.5 235,-388.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140082720750416&#45;&gt;140082720750672 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140082720750416&#45;&gt;140082720750672</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M188.2378,-369.2615C188.9615,-359.7077 190.0862,-344.8615 190.9986,-332.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"194.4937,-333.0143 191.7592,-322.7785 187.5137,-332.4855 194.4937,-333.0143\"/>\n</g>\n<!-- 140082720750800 -->\n<g id=\"node12\" class=\"node\">\n<title>140082720750800</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-454.5 0,-454.5 0,-435.5 101,-435.5 101,-454.5\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750800&#45;&gt;140082720750416 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140082720750800&#45;&gt;140082720750416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M70.421,-435.403C93.5543,-424.2585 131.9168,-405.7773 158.3899,-393.0238\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"160.065,-396.1019 167.555,-388.6085 157.0268,-389.7956 160.065,-396.1019\"/>\n</g>\n<!-- 140082735867376 -->\n<g id=\"node13\" class=\"node\">\n<title>140082735867376</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"92,-526 9,-526 9,-496 92,-496 92,-526\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer2.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (40)</text>\n</g>\n<!-- 140082735867376&#45;&gt;140082720750800 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140082735867376&#45;&gt;140082720750800</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-495.6924C50.5,-486.5067 50.5,-474.7245 50.5,-464.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-464.703 50.5,-454.7031 47.0001,-464.7031 54.0001,-464.703\"/>\n</g>\n<!-- 140082720751056 -->\n<g id=\"node14\" class=\"node\">\n<title>140082720751056</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"214,-454.5 119,-454.5 119,-435.5 214,-435.5 214,-454.5\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140082720751056&#45;&gt;140082720750416 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140082720751056&#45;&gt;140082720750416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M169.5986,-435.2615C172.6695,-425.6102 177.4588,-410.558 181.3112,-398.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.6918,-399.369 184.3887,-388.7785 178.0213,-397.2465 184.6918,-399.369\"/>\n</g>\n<!-- 140082720750864 -->\n<g id=\"node15\" class=\"node\">\n<title>140082720750864</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"209,-520.5 114,-520.5 114,-501.5 209,-501.5 209,-520.5\"/>\n<text text-anchor=\"middle\" x=\"161.5\" y=\"-508.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140082720750864&#45;&gt;140082720751056 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140082720750864&#45;&gt;140082720751056</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.2378,-501.2615C162.9615,-491.7077 164.0862,-476.8615 164.9986,-464.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.4937,-465.0143 165.7592,-454.7785 161.5137,-464.4855 168.4937,-465.0143\"/>\n</g>\n<!-- 140082720751184 -->\n<g id=\"node16\" class=\"node\">\n<title>140082720751184</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"112,-586.5 11,-586.5 11,-567.5 112,-567.5 112,-586.5\"/>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-574.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720751184&#45;&gt;140082720750864 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140082720751184&#45;&gt;140082720750864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M76.2553,-567.2615C92.6709,-556.4272 119.3988,-538.7868 138.5509,-526.1464\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.5911,-528.9935 147.0092,-520.5639 136.7352,-523.1512 140.5911,-528.9935\"/>\n</g>\n<!-- 140082735885040 -->\n<g id=\"node17\" class=\"node\">\n<title>140082735885040</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"102,-658 19,-658 19,-628 102,-628 102,-658\"/>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-646\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer1.bias</text>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-635\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140082735885040&#45;&gt;140082720751184 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140082735885040&#45;&gt;140082720751184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M60.7319,-627.6924C60.8711,-618.5067 61.0496,-606.7245 61.1995,-596.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.701,-596.755 61.353,-586.7031 57.7018,-596.6489 64.701,-596.755\"/>\n</g>\n<!-- 140082720751440 -->\n<g id=\"node18\" class=\"node\">\n<title>140082720751440</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"204,-586.5 133,-586.5 133,-567.5 204,-567.5 204,-586.5\"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-574.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140082720751440&#45;&gt;140082720750864 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140082720751440&#45;&gt;140082720750864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M167.4671,-567.2615C166.4538,-557.7077 164.8793,-542.8615 163.6019,-530.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"167.0724,-530.3535 162.5371,-520.7785 160.1114,-531.0919 167.0724,-530.3535\"/>\n</g>\n<!-- 140082720751312 -->\n<g id=\"node19\" class=\"node\">\n<title>140082720751312</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"221,-652.5 120,-652.5 120,-633.5 221,-633.5 221,-652.5\"/>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-640.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720751312&#45;&gt;140082720751440 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140082720751312&#45;&gt;140082720751440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.2049,-633.2615C169.9154,-623.7077 169.4655,-608.8615 169.1006,-596.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"172.5977,-596.6678 168.7963,-586.7785 165.6009,-596.8799 172.5977,-596.6678\"/>\n</g>\n<!-- 140082735884720 -->\n<g id=\"node20\" class=\"node\">\n<title>140082735884720</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"218,-724 123,-724 123,-694 218,-694 218,-724\"/>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-712\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer1.weight</text>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-701\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 12)</text>\n</g>\n<!-- 140082735884720&#45;&gt;140082720751312 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140082735884720&#45;&gt;140082720751312</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.5,-693.6924C170.5,-684.5067 170.5,-672.7245 170.5,-662.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"174.0001,-662.703 170.5,-652.7031 167.0001,-662.7031 174.0001,-662.703\"/>\n</g>\n<!-- 140082720750992 -->\n<g id=\"node21\" class=\"node\">\n<title>140082720750992</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"303,-454.5 232,-454.5 232,-435.5 303,-435.5 303,-454.5\"/>\n<text text-anchor=\"middle\" x=\"267.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140082720750992&#45;&gt;140082720750416 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140082720750992&#45;&gt;140082720750416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M255.6957,-435.2615C242.8604,-424.6723 222.1442,-407.5815 206.9083,-395.0119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0337,-392.228 199.0926,-388.5639 204.579,-397.6276 209.0337,-392.228\"/>\n</g>\n<!-- 140082720751120 -->\n<g id=\"node22\" class=\"node\">\n<title>140082720751120</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"328,-520.5 227,-520.5 227,-501.5 328,-501.5 328,-520.5\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-508.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720751120&#45;&gt;140082720750992 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140082720751120&#45;&gt;140082720750992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M276.0245,-501.2615C274.5769,-491.7077 272.3275,-476.8615 270.5028,-464.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.9402,-464.1413 268.9816,-454.7785 267.0192,-465.19 273.9402,-464.1413\"/>\n</g>\n<!-- 140082735883520 -->\n<g id=\"node23\" class=\"node\">\n<title>140082735883520</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"325,-592 230,-592 230,-562 325,-562 325,-592\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer2.weight</text>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (40, 20)</text>\n</g>\n<!-- 140082735883520&#45;&gt;140082720751120 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140082735883520&#45;&gt;140082720751120</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M277.5,-561.6924C277.5,-552.5067 277.5,-540.7245 277.5,-530.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"281.0001,-530.703 277.5,-520.7031 274.0001,-530.7031 281.0001,-530.703\"/>\n</g>\n<!-- 140082720750352 -->\n<g id=\"node24\" class=\"node\">\n<title>140082720750352</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"329,-322.5 258,-322.5 258,-303.5 329,-303.5 329,-322.5\"/>\n<text text-anchor=\"middle\" x=\"293.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140082720750352&#45;&gt;140082720749904 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140082720750352&#45;&gt;140082720749904</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M281.6957,-303.2615C268.8604,-292.6723 248.1442,-275.5815 232.9083,-263.0119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.0337,-260.228 225.0926,-256.5639 230.579,-265.6276 235.0337,-260.228\"/>\n</g>\n<!-- 140082720750736 -->\n<g id=\"node25\" class=\"node\">\n<title>140082720750736</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"397,-388.5 296,-388.5 296,-369.5 397,-369.5 397,-388.5\"/>\n<text text-anchor=\"middle\" x=\"346.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750736&#45;&gt;140082720750352 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140082720750736&#45;&gt;140082720750352</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M338.6797,-369.2615C330.538,-359.1228 317.61,-343.0238 307.6652,-330.6397\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.3428,-328.3842 301.3524,-322.7785 304.8848,-332.7671 310.3428,-328.3842\"/>\n</g>\n<!-- 140082735674304 -->\n<g id=\"node26\" class=\"node\">\n<title>140082735674304</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"416,-460 321,-460 321,-430 416,-430 416,-460\"/>\n<text text-anchor=\"middle\" x=\"368.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer3.weight</text>\n<text text-anchor=\"middle\" x=\"368.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 40)</text>\n</g>\n<!-- 140082735674304&#45;&gt;140082720750736 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140082735674304&#45;&gt;140082720750736</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M363.3975,-429.6924C360.2697,-420.3092 356.2388,-408.2165 352.8985,-398.1956\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"356.2171,-397.0831 349.7344,-388.7031 349.5763,-399.2967 356.2171,-397.0831\"/>\n</g>\n<!-- 140082720750224 -->\n<g id=\"node27\" class=\"node\">\n<title>140082720750224</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"381,-196 310,-196 310,-177 381,-177 381,-196\"/>\n<text text-anchor=\"middle\" x=\"345.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140082720750224&#45;&gt;140082720749776 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140082720750224&#45;&gt;140082720749776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M323.3783,-176.9197C303.1064,-168.1406 272.8692,-155.0457 250.0628,-145.1689\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"251.3617,-141.9174 240.7943,-141.155 248.5798,-148.3409 251.3617,-141.9174\"/>\n</g>\n<!-- 140082720750480 -->\n<g id=\"node28\" class=\"node\">\n<title>140082720750480</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"423,-256.5 322,-256.5 322,-237.5 423,-237.5 423,-256.5\"/>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750480&#45;&gt;140082720750224 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140082720750480&#45;&gt;140082720750224</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M368.162,-237.2796C364.3663,-228.7746 358.7412,-216.17 354.0449,-205.6469\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"357.1886,-204.1027 349.9169,-196.3972 350.7962,-206.9555 357.1886,-204.1027\"/>\n</g>\n<!-- 140083271789456 -->\n<g id=\"node29\" class=\"node\">\n<title>140083271789456</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"442,-328 347,-328 347,-298 442,-298 442,-328\"/>\n<text text-anchor=\"middle\" x=\"394.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">output.weight</text>\n<text text-anchor=\"middle\" x=\"394.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 20)</text>\n</g>\n<!-- 140083271789456&#45;&gt;140082720750480 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140083271789456&#45;&gt;140082720750480</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M389.3975,-297.6924C386.2697,-288.3092 382.2388,-276.2165 378.8985,-266.1956\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"382.2171,-265.0831 375.7344,-256.7031 375.5763,-267.2967 382.2171,-265.0831\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oo1h4rQAlxGR",
        "outputId": "966a4588-918f-4d40-d661-9cf52a6bf4e9"
      },
      "source": [
        "torchviz.make_dot(output_proba,params=dict(neural_net.named_parameters()))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f678ccf7bd0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"450pt\" height=\"787pt\"\n viewBox=\"0.00 0.00 450.00 787.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 783)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-783 446,-783 446,4 -4,4\"/>\n<!-- 140084741888432 -->\n<g id=\"node1\" class=\"node\">\n<title>140084741888432</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"251,-31 186,-31 186,0 251,0 251,-31\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10, 2)</text>\n</g>\n<!-- 140082720750608 -->\n<g id=\"node2\" class=\"node\">\n<title>140082720750608</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-86 165,-86 165,-67 272,-67 272,-86\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SoftmaxBackward</text>\n</g>\n<!-- 140082720750608&#45;&gt;140084741888432 -->\n<g id=\"edge29\" class=\"edge\">\n<title>140082720750608&#45;&gt;140084741888432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-66.9688C218.5,-60.1289 218.5,-50.5621 218.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-41.3678 218.5,-31.3678 215.0001,-41.3678 222.0001,-41.3678\"/>\n</g>\n<!-- 140082720749968 -->\n<g id=\"node3\" class=\"node\">\n<title>140082720749968</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-141 165,-141 165,-122 272,-122 272,-141\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SigmoidBackward</text>\n</g>\n<!-- 140082720749968&#45;&gt;140082720750608 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140082720749968&#45;&gt;140082720750608</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-121.9197C218.5,-114.9083 218.5,-105.1442 218.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-96.3408 218.5,-86.3408 215.0001,-96.3409 222.0001,-96.3408\"/>\n</g>\n<!-- 140082720749776 -->\n<g id=\"node4\" class=\"node\">\n<title>140082720749776</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"266,-196 171,-196 171,-177 266,-177 266,-196\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140082720749776&#45;&gt;140082720749968 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140082720749776&#45;&gt;140082720749968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-176.9197C218.5,-169.9083 218.5,-160.1442 218.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-151.3408 218.5,-141.3408 215.0001,-151.3409 222.0001,-151.3408\"/>\n</g>\n<!-- 140082720750096 -->\n<g id=\"node5\" class=\"node\">\n<title>140082720750096</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"153,-251 52,-251 52,-232 153,-232 153,-251\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750096&#45;&gt;140082720749776 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140082720750096&#45;&gt;140082720749776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M122.7057,-231.9197C140.9737,-223.2581 168.1018,-210.3957 188.8304,-200.5675\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"190.6004,-203.6018 198.1367,-196.155 187.6014,-197.2768 190.6004,-203.6018\"/>\n</g>\n<!-- 140083271789296 -->\n<g id=\"node6\" class=\"node\">\n<title>140083271789296</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"144,-317 61,-317 61,-287 144,-287 144,-317\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">output.bias</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2)</text>\n</g>\n<!-- 140083271789296&#45;&gt;140082720750096 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140083271789296&#45;&gt;140082720750096</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M102.5,-286.7333C102.5,-279.0322 102.5,-269.5977 102.5,-261.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0001,-261.0864 102.5,-251.0864 99.0001,-261.0864 106.0001,-261.0864\"/>\n</g>\n<!-- 140082720750288 -->\n<g id=\"node7\" class=\"node\">\n<title>140082720750288</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"266,-251 171,-251 171,-232 266,-232 266,-251\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140082720750288&#45;&gt;140082720749776 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140082720750288&#45;&gt;140082720749776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-231.9197C218.5,-224.9083 218.5,-215.1442 218.5,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-206.3408 218.5,-196.3408 215.0001,-206.3409 222.0001,-206.3408\"/>\n</g>\n<!-- 140082720749904 -->\n<g id=\"node8\" class=\"node\">\n<title>140082720749904</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"261,-311.5 166,-311.5 166,-292.5 261,-292.5 261,-311.5\"/>\n<text text-anchor=\"middle\" x=\"213.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140082720749904&#45;&gt;140082720750288 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140082720749904&#45;&gt;140082720750288</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M214.3033,-292.2796C214.9845,-284.0376 215.9838,-271.9457 216.8364,-261.629\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"220.3464,-261.6516 217.682,-251.3972 213.3702,-261.075 220.3464,-261.6516\"/>\n</g>\n<!-- 140082720750544 -->\n<g id=\"node9\" class=\"node\">\n<title>140082720750544</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"127,-377.5 26,-377.5 26,-358.5 127,-358.5 127,-377.5\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750544&#45;&gt;140082720749904 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140082720750544&#45;&gt;140082720749904</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M96.421,-358.403C119.5543,-347.2585 157.9168,-328.7773 184.3899,-316.0238\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.065,-319.1019 193.555,-311.6085 183.0268,-312.7956 186.065,-319.1019\"/>\n</g>\n<!-- 140083271789856 -->\n<g id=\"node10\" class=\"node\">\n<title>140083271789856</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"118,-449 35,-449 35,-419 118,-419 118,-449\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer3.bias</text>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140083271789856&#45;&gt;140082720750544 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140083271789856&#45;&gt;140082720750544</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M76.5,-418.6924C76.5,-409.5067 76.5,-397.7245 76.5,-387.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"80.0001,-387.703 76.5,-377.7031 73.0001,-387.7031 80.0001,-387.703\"/>\n</g>\n<!-- 140082720750672 -->\n<g id=\"node11\" class=\"node\">\n<title>140082720750672</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"240,-377.5 145,-377.5 145,-358.5 240,-358.5 240,-377.5\"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140082720750672&#45;&gt;140082720749904 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140082720750672&#45;&gt;140082720749904</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M195.5986,-358.2615C198.6695,-348.6102 203.4588,-333.558 207.3112,-321.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"210.6918,-322.369 210.3887,-311.7785 204.0213,-320.2465 210.6918,-322.369\"/>\n</g>\n<!-- 140082720750416 -->\n<g id=\"node12\" class=\"node\">\n<title>140082720750416</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"235,-443.5 140,-443.5 140,-424.5 235,-424.5 235,-443.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-431.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140082720750416&#45;&gt;140082720750672 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140082720750416&#45;&gt;140082720750672</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M188.2378,-424.2615C188.9615,-414.7077 190.0862,-399.8615 190.9986,-387.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"194.4937,-388.0143 191.7592,-377.7785 187.5137,-387.4855 194.4937,-388.0143\"/>\n</g>\n<!-- 140082720750800 -->\n<g id=\"node13\" class=\"node\">\n<title>140082720750800</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-509.5 0,-509.5 0,-490.5 101,-490.5 101,-509.5\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-497.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750800&#45;&gt;140082720750416 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140082720750800&#45;&gt;140082720750416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M70.421,-490.403C93.5543,-479.2585 131.9168,-460.7773 158.3899,-448.0238\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"160.065,-451.1019 167.555,-443.6085 157.0268,-444.7956 160.065,-451.1019\"/>\n</g>\n<!-- 140082735867376 -->\n<g id=\"node14\" class=\"node\">\n<title>140082735867376</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"92,-581 9,-581 9,-551 92,-551 92,-581\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer2.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-558\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (40)</text>\n</g>\n<!-- 140082735867376&#45;&gt;140082720750800 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140082735867376&#45;&gt;140082720750800</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-550.6924C50.5,-541.5067 50.5,-529.7245 50.5,-519.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-519.703 50.5,-509.7031 47.0001,-519.7031 54.0001,-519.703\"/>\n</g>\n<!-- 140082720751056 -->\n<g id=\"node15\" class=\"node\">\n<title>140082720751056</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"214,-509.5 119,-509.5 119,-490.5 214,-490.5 214,-509.5\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-497.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140082720751056&#45;&gt;140082720750416 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140082720751056&#45;&gt;140082720750416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M169.5986,-490.2615C172.6695,-480.6102 177.4588,-465.558 181.3112,-453.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.6918,-454.369 184.3887,-443.7785 178.0213,-452.2465 184.6918,-454.369\"/>\n</g>\n<!-- 140082720750864 -->\n<g id=\"node16\" class=\"node\">\n<title>140082720750864</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"209,-575.5 114,-575.5 114,-556.5 209,-556.5 209,-575.5\"/>\n<text text-anchor=\"middle\" x=\"161.5\" y=\"-563.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140082720750864&#45;&gt;140082720751056 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140082720750864&#45;&gt;140082720751056</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.2378,-556.2615C162.9615,-546.7077 164.0862,-531.8615 164.9986,-519.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.4937,-520.0143 165.7592,-509.7785 161.5137,-519.4855 168.4937,-520.0143\"/>\n</g>\n<!-- 140082720751184 -->\n<g id=\"node17\" class=\"node\">\n<title>140082720751184</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"112,-641.5 11,-641.5 11,-622.5 112,-622.5 112,-641.5\"/>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-629.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720751184&#45;&gt;140082720750864 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140082720751184&#45;&gt;140082720750864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M76.2553,-622.2615C92.6709,-611.4272 119.3988,-593.7868 138.5509,-581.1464\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.5911,-583.9935 147.0092,-575.5639 136.7352,-578.1512 140.5911,-583.9935\"/>\n</g>\n<!-- 140082735885040 -->\n<g id=\"node18\" class=\"node\">\n<title>140082735885040</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"102,-713 19,-713 19,-683 102,-683 102,-713\"/>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-701\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer1.bias</text>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-690\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140082735885040&#45;&gt;140082720751184 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140082735885040&#45;&gt;140082720751184</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M60.7319,-682.6924C60.8711,-673.5067 61.0496,-661.7245 61.1995,-651.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.701,-651.755 61.353,-641.7031 57.7018,-651.6489 64.701,-651.755\"/>\n</g>\n<!-- 140082720751440 -->\n<g id=\"node19\" class=\"node\">\n<title>140082720751440</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"204,-641.5 133,-641.5 133,-622.5 204,-622.5 204,-641.5\"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-629.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140082720751440&#45;&gt;140082720750864 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140082720751440&#45;&gt;140082720750864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M167.4671,-622.2615C166.4538,-612.7077 164.8793,-597.8615 163.6019,-585.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"167.0724,-585.3535 162.5371,-575.7785 160.1114,-586.0919 167.0724,-585.3535\"/>\n</g>\n<!-- 140082720751312 -->\n<g id=\"node20\" class=\"node\">\n<title>140082720751312</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"221,-707.5 120,-707.5 120,-688.5 221,-688.5 221,-707.5\"/>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-695.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720751312&#45;&gt;140082720751440 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140082720751312&#45;&gt;140082720751440</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.2049,-688.2615C169.9154,-678.7077 169.4655,-663.8615 169.1006,-651.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"172.5977,-651.6678 168.7963,-641.7785 165.6009,-651.8799 172.5977,-651.6678\"/>\n</g>\n<!-- 140082735884720 -->\n<g id=\"node21\" class=\"node\">\n<title>140082735884720</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"218,-779 123,-779 123,-749 218,-749 218,-779\"/>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-767\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer1.weight</text>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-756\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 12)</text>\n</g>\n<!-- 140082735884720&#45;&gt;140082720751312 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140082735884720&#45;&gt;140082720751312</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.5,-748.6924C170.5,-739.5067 170.5,-727.7245 170.5,-717.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"174.0001,-717.703 170.5,-707.7031 167.0001,-717.7031 174.0001,-717.703\"/>\n</g>\n<!-- 140082720750992 -->\n<g id=\"node22\" class=\"node\">\n<title>140082720750992</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"303,-509.5 232,-509.5 232,-490.5 303,-490.5 303,-509.5\"/>\n<text text-anchor=\"middle\" x=\"267.5\" y=\"-497.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140082720750992&#45;&gt;140082720750416 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140082720750992&#45;&gt;140082720750416</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M255.6957,-490.2615C242.8604,-479.6723 222.1442,-462.5815 206.9083,-450.0119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0337,-447.228 199.0926,-443.5639 204.579,-452.6276 209.0337,-447.228\"/>\n</g>\n<!-- 140082720751120 -->\n<g id=\"node23\" class=\"node\">\n<title>140082720751120</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"328,-575.5 227,-575.5 227,-556.5 328,-556.5 328,-575.5\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-563.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720751120&#45;&gt;140082720750992 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140082720751120&#45;&gt;140082720750992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M276.0245,-556.2615C274.5769,-546.7077 272.3275,-531.8615 270.5028,-519.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.9402,-519.1413 268.9816,-509.7785 267.0192,-520.19 273.9402,-519.1413\"/>\n</g>\n<!-- 140082735883520 -->\n<g id=\"node24\" class=\"node\">\n<title>140082735883520</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"325,-647 230,-647 230,-617 325,-617 325,-647\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-635\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer2.weight</text>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-624\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (40, 20)</text>\n</g>\n<!-- 140082735883520&#45;&gt;140082720751120 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140082735883520&#45;&gt;140082720751120</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M277.5,-616.6924C277.5,-607.5067 277.5,-595.7245 277.5,-585.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"281.0001,-585.703 277.5,-575.7031 274.0001,-585.7031 281.0001,-585.703\"/>\n</g>\n<!-- 140082720750352 -->\n<g id=\"node25\" class=\"node\">\n<title>140082720750352</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"329,-377.5 258,-377.5 258,-358.5 329,-358.5 329,-377.5\"/>\n<text text-anchor=\"middle\" x=\"293.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140082720750352&#45;&gt;140082720749904 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140082720750352&#45;&gt;140082720749904</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M281.6957,-358.2615C268.8604,-347.6723 248.1442,-330.5815 232.9083,-318.0119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.0337,-315.228 225.0926,-311.5639 230.579,-320.6276 235.0337,-315.228\"/>\n</g>\n<!-- 140082720750736 -->\n<g id=\"node26\" class=\"node\">\n<title>140082720750736</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"397,-443.5 296,-443.5 296,-424.5 397,-424.5 397,-443.5\"/>\n<text text-anchor=\"middle\" x=\"346.5\" y=\"-431.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750736&#45;&gt;140082720750352 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140082720750736&#45;&gt;140082720750352</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M338.6797,-424.2615C330.538,-414.1228 317.61,-398.0238 307.6652,-385.6397\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.3428,-383.3842 301.3524,-377.7785 304.8848,-387.7671 310.3428,-383.3842\"/>\n</g>\n<!-- 140082735674304 -->\n<g id=\"node27\" class=\"node\">\n<title>140082735674304</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"416,-515 321,-515 321,-485 416,-485 416,-515\"/>\n<text text-anchor=\"middle\" x=\"368.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer3.weight</text>\n<text text-anchor=\"middle\" x=\"368.5\" y=\"-492\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 40)</text>\n</g>\n<!-- 140082735674304&#45;&gt;140082720750736 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140082735674304&#45;&gt;140082720750736</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M363.3975,-484.6924C360.2697,-475.3092 356.2388,-463.2165 352.8985,-453.1956\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"356.2171,-452.0831 349.7344,-443.7031 349.5763,-454.2967 356.2171,-452.0831\"/>\n</g>\n<!-- 140082720750224 -->\n<g id=\"node28\" class=\"node\">\n<title>140082720750224</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"381,-251 310,-251 310,-232 381,-232 381,-251\"/>\n<text text-anchor=\"middle\" x=\"345.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140082720750224&#45;&gt;140082720749776 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140082720750224&#45;&gt;140082720749776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M323.3783,-231.9197C303.1064,-223.1406 272.8692,-210.0457 250.0628,-200.1689\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"251.3617,-196.9174 240.7943,-196.155 248.5798,-203.3409 251.3617,-196.9174\"/>\n</g>\n<!-- 140082720750480 -->\n<g id=\"node29\" class=\"node\">\n<title>140082720750480</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"423,-311.5 322,-311.5 322,-292.5 423,-292.5 423,-311.5\"/>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140082720750480&#45;&gt;140082720750224 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140082720750480&#45;&gt;140082720750224</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M368.162,-292.2796C364.3663,-283.7746 358.7412,-271.17 354.0449,-260.6469\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"357.1886,-259.1027 349.9169,-251.3972 350.7962,-261.9555 357.1886,-259.1027\"/>\n</g>\n<!-- 140083271789456 -->\n<g id=\"node30\" class=\"node\">\n<title>140083271789456</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"442,-383 347,-383 347,-353 442,-353 442,-383\"/>\n<text text-anchor=\"middle\" x=\"394.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">output.weight</text>\n<text text-anchor=\"middle\" x=\"394.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 20)</text>\n</g>\n<!-- 140083271789456&#45;&gt;140082720750480 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140083271789456&#45;&gt;140082720750480</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M389.3975,-352.6924C386.2697,-343.3092 382.2388,-331.2165 378.8985,-321.1956\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"382.2171,-320.0831 375.7344,-311.7031 375.5763,-322.2967 382.2171,-320.0831\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}