{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week2_tutorial_denis_lemarchand.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delemarchand2020/IVADO_MILA_DL/blob/main/week2_tutorial_denis_lemarchand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipSoOVlavkb"
      },
      "source": [
        "# IVADO/MILA DEEP LEARNING SCHOOL\n",
        "# Spring 2021\n",
        "# Tutorial : Categorical data with multilayer perceptron (MLP)\n",
        "\n",
        "## Authors: \n",
        "\n",
        "Arsène Fansi Tchango <arsene.fansi.tchango@mila.quebec>\n",
        "\n",
        "Gaétan Marceau Caron <gaetan.marceau.caron@mila.quebec>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLHwvggEZERd"
      },
      "source": [
        "# Preface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKNGtQkkohiM"
      },
      "source": [
        "This tutorial introduces the practical aspects of Deep Learning through the realization of a simple end-to-end project. We will use the deep learning library <a href=\"https://pytorch.org/\"> `PyTorch`</a>, which is well-known for its ease of use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu9DZNmYpePz"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWCNdeTIpkCa"
      },
      "source": [
        "Before we begin, we must install all the required libraries for this part of the tutorial. To do so, we will use the `pip` utility. Execute the cell below by selecting it and pressing `shift`+`Enter`. (This operation may take a few minutes.)\n",
        "\n",
        "We need to be using the latest version of `pillow` for this tutorial. If you are prompted with:\n",
        "\n",
        "> WARNING: The following packages were previously imported in this runtime:\n",
        "  [PIL]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "\n",
        "Then click on restart runtime and rerun the cells afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g_0k-_Eppi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74597c8c-1233-44ea-e244-e90ed6bce978"
      },
      "source": [
        "!pip3 install torch torchvision matplotlib\n",
        "!pip3 install --upgrade pillow==8.1.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (8.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already up-to-date: pillow==8.1.0 in /usr/local/lib/python3.7/dist-packages (8.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKufekB4FnqN"
      },
      "source": [
        "To ensure that all required libraries are available, let's try to load all libraries and modules we will need during this tutorial by executing this cell: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loLQlRg3sV-r"
      },
      "source": [
        "import importlib\n",
        "required_libraries = ['torch', 'torchvision', 'PIL', 'matplotlib', \n",
        "                      'numpy', 'pandas']\n",
        "for lib in required_libraries:\n",
        "    if importlib.util.find_spec(lib) is None:\n",
        "        print(\"%s unavailable\" % lib)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeLUcnk5scNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ba8251-4b74-4a40-e395-8f54b7652689"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "print(\"Torch version: \", torch.__version__)\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch version:  1.8.1+cu101\n",
            "GPU Available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc9wc4qq7qob"
      },
      "source": [
        "Fix the seed for the different libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05jTJ12r7msf"
      },
      "source": [
        "seed = 4321\n",
        "np.random.seed(seed) # Set the random seed of numpy for the data split.\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKzgFV9Favkt"
      },
      "source": [
        "# PyTorch in a nutshell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrus_-F0avkt"
      },
      "source": [
        "*PyTorch* is a Python library that supports a vibrant ecosystem of tools and libraries for machine learning (ML) in vision, NLP, and more. It provides two high-level features:\n",
        "<ul>\n",
        "<li> operations on <a href=\"https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\">tensors</a> (such as NumPy) with GPU support,</li>\n",
        "<li> operations for creating and optimizing computational graphs with an automatic differentiation system called <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\">Autograd</a>.</li>\n",
        "</ul>\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/torch.html\">PyTorch docs</a> contain the API documentation and <a href=\"https://pytorch.org/tutorials/\">many tutorials</a>.\n",
        "Also, PyTorch offers several data processing utilities. One of these utilities is the class <a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.Dataset`</a> which offers an easy to use interface to handle a data set. For more information, please refer to the following urls: \n",
        "<ul>\n",
        "<li>PyTorch data sets: <a href=\"http://pytorch.org/docs/master/data.html\"> PyTorch - datasets</a>.</li>\n",
        "<li>A tutorial for loading data: <a href=\"http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\"> PyTorch - data loading tutorial</a>.</li>\n",
        "</ul>\n",
        "\n",
        "<a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> is a package that provides the same functions as CPU tensors but for  CUDA tensors, which are used for GPU computing. <a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> returns a boolean indicating if CUDA is currently available. Finally, we recommend using a `device` variable that identifies the device that will perform computations. We can assign a tensor to a device with the method `.to(device)`. By default, the tensors are CPU tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm122vNmq92L"
      },
      "source": [
        "# Ingredients for a proof of concept (POC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqvhR0ebavmE"
      },
      "source": [
        "To realize a ML POC, you need:\n",
        "<ul>\n",
        "<li>a task description as well as data to support it,</li>\n",
        "<li>evaluation metrics to assess the performance of models,</li>\n",
        "<li>a model description,</li>\n",
        "<li>a loss function to minimize,</li>\n",
        "<li>an optimizer that adjusts the parameters of the model.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8_pfpu2f6AO"
      },
      "source": [
        "# How to prepare the dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5piZxYUhSzq"
      },
      "source": [
        "In this tutorial, we study the tragedy of the Titanic through a data-driven approach. Our task is to determine whether or not a passenger survived the Titanic sinking based on passenger data only. The results will show how the passengers' fate was pre-determined by the features selected in this study, without considering other factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4GuYNDFavlU"
      },
      "source": [
        "## Titanic dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiOJx2ytavlU"
      },
      "source": [
        "First, we download the Titanic dataset from the following address:\n",
        "<br/>\n",
        "https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true.<br/>\n",
        "This dataset provides information on the fate of 1309 passengers of the first and only journey of the liner RMS Titanic, summarized by economic status (class), gender, age, family information, and survival. We use this dataset because we can train models very quickly for the purpose of this tutorial. The Kaggle platform also uses this dataset as an introduction to classical machine learning. <br/>\n",
        "\n",
        "Let's take a look at the features and some examples from this dataset. To do so, we use the library <a href=\"https://pandas.pydata.org/\">Pandas</a> to load the dataset into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX_RSiffavlW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "85c50d68-0187-46bc-9a03-93a5843adbf4"
      },
      "source": [
        "titanic_df = pd.read_csv(\n",
        "    'https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true', \n",
        "    sep='\\t', \n",
        "    index_col=None, \n",
        "    na_values=['NA']\n",
        ")\n",
        "\n",
        "# a snapshot of the first 5 data points\n",
        "titanic_df.head()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pclass</th>\n",
              "      <th>survived</th>\n",
              "      <th>name</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>ticket</th>\n",
              "      <th>fare</th>\n",
              "      <th>cabin</th>\n",
              "      <th>embarked</th>\n",
              "      <th>boat</th>\n",
              "      <th>body</th>\n",
              "      <th>home.dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allen, Miss. Elisabeth Walton</td>\n",
              "      <td>female</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24160</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>B5</td>\n",
              "      <td>S</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>St Louis, MO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allison, Master. Hudson Trevor</td>\n",
              "      <td>male</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Miss. Helen Loraine</td>\n",
              "      <td>female</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
              "      <td>male</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>135.0</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
              "      <td>female</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pclass  survived  ...   body                        home.dest\n",
              "0       1         1  ...    NaN                     St Louis, MO\n",
              "1       1         1  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "2       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "3       1         0  ...  135.0  Montreal, PQ / Chesterville, ON\n",
              "4       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj88WmCmavlf"
      },
      "source": [
        "Here's the list of the features with their description:\n",
        "\n",
        "<ol>\n",
        "\n",
        "  <li> <b>pclass</b>: Passenger class (1 = first; 2 = second; 3 = third) </li>\n",
        "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
        "  <li> <b>name</b>: Name </li>\n",
        "  <li> <b>sex</b>: Sex </li>\n",
        "  <li> <b>age</b>: Age </li>\n",
        "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
        "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
        "  <li> <b>ticket</b>: Ticket number </li>\n",
        "  <li> <b>fare</b>: Passenger fare </li>\n",
        "  <li> <b>cabin</b>: Cabin number </li>\n",
        "  <li> <b>embarked</b>: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) </li>\n",
        "  <li> <b>boat</b>: Lifeboat (if the passenger survived, otherwise the value is Not a Number (NaN) </li>\n",
        "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found, otherwise the value is NaN) </li>\n",
        "  <li> <b>home.dest</b>: the passenger's destination </li>\n",
        " </ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2ed5fozqjce"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__vcZhPnavlg"
      },
      "source": [
        "### Feature selection\n",
        "Some features are not relevant to the task, for example:\n",
        "<ol>\n",
        "  <li> <b>name</b>: Name </li>\n",
        "  <li> <b>ticket</b>: Ticket number </li>\n",
        "  <li> <b>cabin</b>: Cabin number </li>\n",
        "  <li> <b>home.dest</b>: Passenger's destination </li>\n",
        " </ol>\n",
        " \n",
        "Other features are directly related to the passenger survival and are not interesting for our study because they give away the label to be predicted:\n",
        "<ol>\n",
        "  <li> <b>boat</b>: Lifeboat (if the passenger survived) </li>\n",
        "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found) </li>\n",
        " </ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoD8x0dXofmj"
      },
      "source": [
        "titanic_df.drop(['name', 'ticket', 'cabin', 'home.dest', 'boat', 'body'], axis=1, inplace=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PW_IWwqwdHq"
      },
      "source": [
        "### Handling missing values\n",
        "\n",
        "Handling missing values in datasets is difficult since examples with missing values can be informative for subgroups. Suppose that features associated with third-class tickets have many missing values compared to first-class tickets. Removing all examples with missing values will introduce a bias in the dataset and could completely change the correlations between features and the probability of survival. Another approach is to impute missing values by replacing them with simples statistics such as the mean or the median of the feature. Other imputing techniques use the other features to predict the missing values, which is very close to a ML task. See Scikit-learn reference for more details ([Imputation of missing values](https://www.google.com/url?q=https://scikit-learn.org/stable/modules/impute.html&sa=D&source=editors&ust=1617044175668000&usg=AFQjCNHxRm4BcLBrZEPzW7i3C_PJ7oAuPQ)). Thus, handling missing values should be done carefully and often depend on the dataset and domain knowledge. In the following, we discard all examples with missing values, but since it is a substantial number of examples, we are probably introducing a bias in the dataset. To avoid such bias, one can re-annotate the data manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8YKDg69wjb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7af92b7-c6da-414a-d7dc-29e3b40c712f"
      },
      "source": [
        "n_examples = len(titanic_df)\n",
        "titanic_df = titanic_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
        "print(f'We removed {n_examples-len(titanic_df)} examples over {n_examples} containing missing values.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We removed 266 examples over 1309 containing missing values.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MckYm0M_xhR"
      },
      "source": [
        "### Feature encoding\n",
        " \n",
        "Some features are **categorical variables**, which means that they can take a finite number of values.\n",
        " <ol>\n",
        "  <li> <b>pclass</b>: Passenger Class </li>\n",
        "  <li> <b>sex</b>: Sex </li>\n",
        "  <li> <b>embarked</b>: Port of embarkation </li>\n",
        " </ol>\n",
        "\n",
        "To process categorical variables, we need to encode them in a way that does not imply an arbitrary order such as using natural numbers (e.g., 1, 2, 3). <a href=\"https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics\">One-hot encoding</a> is a way to achieve it. To use this encoding, we can simply call the function `get_dummies` in Pandas. The meaning of the encoded variables is as follows:\n",
        "\n",
        "<ol>\n",
        "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
        "  <li> <b>pclass_1</b>: (1 if passenger in first class; 0 if not) </li>\n",
        "  <li> <b>pclass_2</b>: (1 if passenger in second class; 0 if not) </li>\n",
        "  <li> <b>pclass_3</b>: (1 if passenger in third class; 0 if not) </li>\n",
        "  <li> <b>sex_female</b>: (1 if passenger is female; 0 if not) </li>\n",
        "  <li> <b>sex_male</b>: (1 if passenger is male; 0 if not) </li>\n",
        "  <li> <b>age</b>: Age </li>\n",
        "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
        "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
        "  <li> <b>fare</b>: Passenger fare </li>\n",
        "  <li> <b>embarked_C</b>: (1 if Port of embarkation = Cherbourg (C); 0 otherwise) </li> \n",
        "  <li> <b>embarked_Q</b>: (1 if Port of embarkation = Queenstown (Q); 0 otherwise) </li> \n",
        "  <li> <b>embarked_S</b>: (1 if Port of embarkation = Southampton (S); 0 otherwise)</li> \n",
        " </ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7bFJc5X_qjx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "37f263df-a933-40e8-ef5f-98bd8c93ae26"
      },
      "source": [
        "titanic_preprocess_df = pd.get_dummies(data=titanic_df, columns=['pclass', 'sex', 'embarked'])\n",
        "titanic_preprocess_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   survived      age  sibsp  ...  embarked_C  embarked_Q  embarked_S\n",
              "0         1  29.0000      0  ...           0           0           1\n",
              "1         1   0.9167      1  ...           0           0           1\n",
              "2         0   2.0000      1  ...           0           0           1\n",
              "3         0  30.0000      1  ...           0           0           1\n",
              "4         0  25.0000      1  ...           0           0           1\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeBNt5EW6tm9"
      },
      "source": [
        "Now, we can check if all examples have a one-hot encoding for their categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RjZZEokvJ3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3ecc1a44-ed0d-45ca-8bfb-b6cb5ea1d9f5"
      },
      "source": [
        "titanic_preprocess_df.loc[titanic_preprocess_df[['sex_male','sex_female']].sum(axis=1) != 1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [survived, age, sibsp, parch, fare, pclass_1, pclass_2, pclass_3, sex_female, sex_male, embarked_C, embarked_Q, embarked_S]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3yueVPfvEhI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f2a1ac82-2b8b-4fd3-eaed-f1f337d956e8"
      },
      "source": [
        "titanic_preprocess_df.loc[titanic_preprocess_df[['pclass_1','pclass_2', 'pclass_3']].sum(axis=1) != 1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [survived, age, sibsp, parch, fare, pclass_1, pclass_2, pclass_3, sex_female, sex_male, embarked_C, embarked_Q, embarked_S]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4No15nhu8Hi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "aa343e81-74d9-435e-85d4-cef67d9ced8c"
      },
      "source": [
        "titanic_preprocess_df.loc[titanic_preprocess_df[['embarked_C','embarked_Q', 'embarked_S']].sum(axis=1) != 1]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [survived, age, sibsp, parch, fare, pclass_1, pclass_2, pclass_3, sex_female, sex_male, embarked_C, embarked_Q, embarked_S]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4a-35Xa66Tj"
      },
      "source": [
        "Since there are only two examples with no port of embarkation, we decide to discard them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wODe7zbglC1I",
        "outputId": "6aced995-24e6-48ff-a855-accf7820c4e5"
      },
      "source": [
        "titanic_df.loc[148,:]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pclass            1\n",
              "survived          0\n",
              "sex          female\n",
              "age              50\n",
              "sibsp             0\n",
              "parch             0\n",
              "fare        28.7125\n",
              "embarked          C\n",
              "Name: 148, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkBNPNPDk9ZN"
      },
      "source": [
        "# Drop rows where there is no port of embarkation associated\n",
        "titanic_preprocess_df = titanic_preprocess_df.drop(index=148).reset_index(drop=True)\n",
        "titanic_preprocess_df = titanic_preprocess_df.drop(index=248).reset_index(drop=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2p2GAKHm-92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bdb5c06-903d-49b4-d6f0-26d3621d4426"
      },
      "source": [
        "print(f'There are {len(titanic_preprocess_df)} remaining examples in the dataset.')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1041 remaining examples in the dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJcs6PUTavlm"
      },
      "source": [
        "## Train / validation / test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjbgvffmavlo"
      },
      "source": [
        "At this point, we need to divide the dataset into three subsets:\n",
        "\n",
        "<ol>\n",
        "<li> <b> Train</b> (60% of the dataset): used to train the classification model. </li>   \n",
        "<li> <b> Validation</b> (20% of the dataset): used to evaluate hyper-parameters on held-out data. </li>   \n",
        "<li> <b> Test</b> (20% of the dataset): used to evaluate the generalization performance of the chosen model on held-out data. </li>\n",
        "</ol>\n",
        "\n",
        "We use the <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.split.html\">numpy.split function</a> to separate our dataset into subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs07L8F5488f"
      },
      "source": [
        "### Exercise 1\n",
        "Complete the missing code to create the validation and the test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBmL8VBOavlo"
      },
      "source": [
        "train, validate, test = np.split(\n",
        "    titanic_preprocess_df.sample(frac=1, random_state=seed), # cela revient a un shuffle du set de donnees\n",
        "    [int(.6*len(titanic_preprocess_df)), int(.8*len(titanic_preprocess_df))])\n",
        "\n",
        "# Remove the label column from X and create a label vectors.\n",
        "X_train = train.drop(['survived'], axis=1).to_numpy()\n",
        "y_train = train['survived'].to_numpy()\n",
        "\n",
        "X_val = validate.drop(['survived'], axis=1).to_numpy() \n",
        "y_val = validate['survived'].to_numpy()\n",
        "\n",
        "X_test = test.drop(['survived'], axis=1).to_numpy() \n",
        "y_test = test['survived'].to_numpy()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv74TbIWavlr"
      },
      "source": [
        "## Datasets in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_LJtG-Xavlt"
      },
      "source": [
        "We will use the subclass <b><a href=\"https://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset\"> `torch.utils.data.TensorDataset`</a> </b> to manipulate together the features and targets of a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZUfbAtG5S92"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Complete the missing code to load the validation and the test sets in TensorDatasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JtT4tV7avlt"
      },
      "source": [
        "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obEPHnlTavkc"
      },
      "source": [
        "# How to define the learning algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhN5GL6Gavks"
      },
      "source": [
        "To train a deep learning model, we need to define:\n",
        "* the network architecture by choosing the non-linear function and the number of hidden units per layer, \n",
        "* the loss function and optimizer.\n",
        "\n",
        "In this tutorial, we consider the multilayer perceptron (MLP). A MLP is a simple computational graph composed of \"hidden layers,\" which are defined by two modules: a **linear transformation** followed by a **non-linearity**. The result of a hidden layer is a vector called a **distributed representation** where each component is associated with a hidden unit.\n",
        "\n",
        "To solve our task, we will use a MLP with the following architecture:\n",
        "* the input dimension of the model is 12,\n",
        "* the output dimension of the model is 2,\n",
        "* the first dimension of the output is the probability of death and the second dimension is the probability of survival,\n",
        "* the number of hidden layers is 3, \n",
        "* the dimensions of the hidden layers are 20, 40, 20 respectively, \n",
        "* the activation function is a ReLu for all hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701t0e-ravkr"
      },
      "source": [
        "## How to define a model in PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4F5cyijavkv"
      },
      "source": [
        "The <a href=\"https://pytorch.org/docs/stable/nn.html\">PyTorch NN package</a> contains many useful classes for creating computation graphs.\n",
        "* The class <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module\">torch.nn.Module</a>: \n",
        "any new module must inherit from this class or its descendants (subclasses).\n",
        "* The `forward` method:  any class defining a module must implement the `forward(...)` method, which defines the transformation of inputs to outputs.\n",
        "* The class <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a>: this class implements a linear transformation. By default, it takes two parameters: \n",
        "    * `in_features`: the size of the data at the input of the module. \n",
        "    * `out_features`: the size of the data at the output of the module.\n",
        "\n",
        "* The module <a href=\"https://pytorch.org/docs/master/nn.functional.html#torch-nn-functional\">`torch.nn.functional`</a>: \n",
        "it defines a set of functions that can be applied directly to any tensor. As examples, we have:\n",
        "    * non-linear functions: `sigmoid(...)`, `tanh(...)`, `relu(...)`, ...\n",
        "    * loss functions: `mse_loss(...)`, `nll(...)`, `cross_entropy(...)`, ... \n",
        "    * regularization functions: `droupout(...)`, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tscha6S-KIBB"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Complete the following methods to define a neural network:\n",
        "* The `__init__` method that defines the layers.\n",
        "* The `forward(input)` method that returns the `output`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR5eBfIbavk0"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(NeuralNet, self).__init__()\n",
        "      self.layer1 = nn.Linear(12, 20)\n",
        "      self.layer2 = nn.Linear(20, 40)\n",
        "      self.layer3 = nn.Linear(40, 20)\n",
        "      self.output = nn.Linear(20, 2)\n",
        "\n",
        "    def forward(self, x):      \n",
        "      x = F.relu(self.layer1(x))\n",
        "      x = F.relu(self.layer2(x))\n",
        "      x = F.relu(self.layer3(x))\n",
        "      out = torch.sigmoid(self.output(x))\n",
        "      return out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLnHRZ5avk2"
      },
      "source": [
        "## Making predictions with a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEXgJMDDavk3"
      },
      "source": [
        "Now, we are ready to test our neural network on randomly selected data.\n",
        "\n",
        "In PyTorch, a model has two different modes:\n",
        "    <ul>\n",
        "    <li> <b>train</b>: used during training, </li>\n",
        "    <li> <b>eval</b>: used during inference for model evaluation. </li>\n",
        "    </ul>\n",
        "\n",
        "The distinction is important since some modules behave differently according to this mode.\n",
        "We will use the <b>eval</b> mode in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqtE_hr650pz"
      },
      "source": [
        "### Exercise 4\n",
        "Complete the missing code so that the model outputs a probability vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzcABMezavk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fea2637-42dc-41fb-dd42-0010d6576301"
      },
      "source": [
        "# Model definition\n",
        "neural_net = NeuralNet()\n",
        "neural_net = neural_net.to(device)\n",
        "\n",
        "# Evaluation mode activation\n",
        "neural_net = neural_net.eval()\n",
        "\n",
        "# Select the first 5 data points\n",
        "data, target = val_dataset[0:5]\n",
        "data = data.to(device)\n",
        "target = target.to(device)\n",
        "\n",
        "# Forward propagation of the data through the model\n",
        "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
        "\n",
        "# Convert the logits into probabilities with softmax function\n",
        "output_proba = F.softmax(output,dim=1)\n",
        "\n",
        "# Printing the probability\n",
        "print(output_proba)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4817, 0.5183],\n",
            "        [0.4717, 0.5283],\n",
            "        [0.4722, 0.5278],\n",
            "        [0.4788, 0.5212],\n",
            "        [0.4920, 0.5080]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVep0BElavlS"
      },
      "source": [
        "The rows define the output of the network, in terms of probabilities over two classes: <b>deceased</b> (first column) or <b>survived</b> (second column), for each of the five input data points. Let us take the label with maximum probability as the predicted label and compare it to the correct label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jV4No36qjdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebc0d01-5c3d-48f0-aaf3-2782f705554f"
      },
      "source": [
        "# Printing predictions (class with the highest probability)\n",
        "_, prediction = torch.max(output_proba, dim=1)\n",
        "\n",
        "print('Model prediction')\n",
        "print(prediction)\n",
        "\n",
        "# Printing the real labels\n",
        "print(\"Actual data\")\n",
        "print(target)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model prediction\n",
            "tensor([1, 1, 1, 1, 1], device='cuda:0')\n",
            "Actual data\n",
            "tensor([0, 0, 0, 1, 1], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEIIjqOuqjdc"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "1.   What would be a good way to measure the model performances?\n",
        "2.   How does our model perform?\n",
        "3.   Considering that the model is not trained on the dataset, do you see any problem with your selected measure?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Fa66z1qqH9em"
      },
      "source": [
        "#@title Métriques\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Compute the accuracy score.\n",
        "  \n",
        "  Args:\n",
        "     y_true: ground truth labels.\n",
        "     y_pred: predicted labels by a classifier.\n",
        "     \n",
        "  Return:\n",
        "     Accuracy score.\n",
        "     \n",
        "  \"\"\"\n",
        "  return metrics.accuracy_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Compute the F1 score.\n",
        "  \n",
        "  Args:\n",
        "     y_true: ground truth labels.\n",
        "     y_pred: predicted labels by a classifier.\n",
        "     \n",
        "  Return:\n",
        "     F1 score.\n",
        "     \n",
        "  \"\"\"\n",
        "  return metrics.f1_score(y_true, y_pred, average='macro')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reJFVgMHABD7",
        "outputId": "9294ec33-aa6b-40a0-b2a7-a8333af8ac05"
      },
      "source": [
        "# calcul F1 score\n",
        "F1_score = f1_score(target.cpu().numpy(), prediction.cpu().numpy())\n",
        "print(f'F1 score is {F1_score:.2%}'.format(F1_score))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score is 28.57%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uySA2TCavmD"
      },
      "source": [
        "## Define the loss function and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkoobCLMavmE"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkX7uSXQavmF"
      },
      "source": [
        "We define the loss function according to the task we want to achieve.\n",
        "\n",
        "PyTorch offers <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">many ready-to-use loss functions</a>.\n",
        "\n",
        "For classification problems, the usual loss function is <b>cross-entropy</b>, and this is the one we will use in this tutorial. In PyTorch, it is defined by the function <a href=\"https://pytorch.org/docs/master/nn.functional.html#cross-entropy\">`torch.nn.functional.cross_entropy`</a>.  Cross entropy allows comparing a $p$ distribution with a reference distribution $t$. It attains its minimum when $t=p$. Its formula for calculating it between the prediction and the target is: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction, $t$ the target, $i$ the example and $j$ the classe of the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHnfYeS5avmF"
      },
      "source": [
        "def loss_function(prediction, target):\n",
        "    loss = F.cross_entropy(prediction, target)\n",
        "    return loss"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsx_cv9Wqjdj"
      },
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hcZaIKtavmH"
      },
      "source": [
        "In Pytorch, thanks to the automatic differentiation mechanism <a href=\"http://pytorch.org/docs/master/notes/autograd.html\">Autograd</a>, it is possible to automatically calculate the gradient of the loss function and backpropagate it through the computational graph.\n",
        "\n",
        "To do this, we only have to call the method `backward()` on the variable returned by the loss function, e.g., with\n",
        "\n",
        "```python\n",
        "loss = loss_function(....)\n",
        "loss.backward()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YNo_ymYavmH"
      },
      "source": [
        "### Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4AlX9TwavmH"
      },
      "source": [
        "PyTorch provides a <a href=\"http://pytorch.org/docs/master/optim.html#algorithms\">set of optimization methods (`torch.optim`)</a> commonly used by the deep learning community. These methods include the following: \n",
        "* <b>SGD</b> (Stochastic Gradient Descent) <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a>\n",
        "* <b>Adam</b> (Adaptive Moment Estimation): a variant of the gradient descent method in which the learning rate is adjusted for each parameter by estimating the first and second moments of the gradients. This optimizer has demonstrated excellent performance compared to SGD on many benchmarks.\n",
        "\n",
        "To be able to use an optimizer in PyTorch, we must instantiate it by passing the following elements:\n",
        "* <b>The parameters of the model</b>: these are obtained using the method `parameters()` on the instantiated model.\n",
        "* <b>The learning rate (lr)</b>: this is the learning rate to be used to update parameters during the optimization process. \n",
        "* There may be other parameters specific to the chosen optimizer.\n",
        "\n",
        "PyTorch offers a simplified interface to interact with any optimizer:\n",
        "* `zero_grad()`: Allows to reinitialize the gradients to zero at the beginning of an optimization step.\n",
        "* `step()`: Allows to perform an optimization step after a gradient backpropagation step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ-lKExqavmI"
      },
      "source": [
        "We will use Adam with a lr of 0.001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDMOziJTavmI"
      },
      "source": [
        "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnFOAfdGqjdr"
      },
      "source": [
        "# How to train and evaluate a model?\n",
        "First, we need some definitions:\n",
        "<ol>\n",
        "<li>\n",
        "<b>Epoch</b>: a complete pass over the entire training dataset.\n",
        "</li>\n",
        "<li>\n",
        "<b>Iteration</b>: an update of the model parameters. Many iterations can occur before the end of an epoch.\n",
        "</li>\n",
        "<li>\n",
        "<b>Mini-batch</b>: A subset of training data used to estimate the average of gradients. In other words, at each iteration, a mini-batch is used. \n",
        "</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLXjNiDTavmK"
      },
      "source": [
        "## Creating the mini-batches\n",
        "PyTorch offers a utility called <b><a href=\"http://pytorch.org/docs/master/data.html\"> `torch.utils.data.DataLoader`</a></b> to load any dataset and automatically split it into mini-batches. During training, the data presented to the network should appear in a different order from one epoch to another. We will prepare the `DataLoader` for our three datasets (training, validation, and test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGoQZSdqavmM"
      },
      "source": [
        "train_batch_size = 32  # number of data in a training batch.\n",
        "eval_batch_size = 32   # number of data in an batch.\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=False)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size, shuffle=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4ecBllSdsQ_",
        "outputId": "31ae680a-8abe-4d11-9b25-04c2f96c5472"
      },
      "source": [
        "train_loader_iter = iter(train_loader)\n",
        "\n",
        "for i in range(len(train_loader)):\n",
        "  t = next(train_loader_iter)\n",
        "  print(len(t[0])) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia3ai-GvavmP"
      },
      "source": [
        "## Simple training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9wNZrTnavmQ"
      },
      "source": [
        "Here we define our training procedure for an epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyK9xCsZavmR"
      },
      "source": [
        "def train(epoch, model, train_loader, optimizer, device):\n",
        "    \n",
        "    # activate the training mode\n",
        "    model.train()\n",
        "    \n",
        "    torch.set_grad_enabled(True)\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    # iteration over the mini-batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        # transfer the data on the chosen device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # reinitialize the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward propagation on the data\n",
        "        prediction = model(data)\n",
        "        \n",
        "        # compute the loss function w.r.t. the targets\n",
        "        loss = loss_function(prediction, target)\n",
        "        \n",
        "        # execute the backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # execute an optimization step\n",
        "        optimizer.step()\n",
        "        \n",
        "        # accumulate the loss\n",
        "        total_loss += loss.item()*len(data)\n",
        "        \n",
        "        # compute the number of correct predictions\n",
        "        _, pred_classes = torch.max(prediction, dim=1)        \n",
        "        correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()\n",
        "         \n",
        "        \n",
        "    # compute the average loss per epoch\n",
        "    mean_loss = total_loss/len(train_loader.dataset)\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "        \n",
        "    print('Train Epoch: {}   Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
        "        epoch, mean_loss, correct, len(train_loader.dataset),\n",
        "        100. * acc))   \n",
        "    \n",
        "    # return the average loss and the accuracy\n",
        "    return mean_loss, acc"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGexbWaHavmU"
      },
      "source": [
        "## Evaluation procedure\n",
        "Here we define our model evaluation procedure.\n",
        "<br/>\n",
        "In addition to switching the model to **eval** mode, it is essential to disable the gradient calculation. \n",
        "<br/>\n",
        "To do this, PyTorch offers a set of context managers to <a href=\"https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation\">locally disable/enable gradient calculation </a>:\n",
        "1. `torch.no_grad()`: disable gradient calculation.\n",
        "2. `torch.enable_grad()`: enable gradient calculation.\n",
        "3. `torch.set_grad_enabled(bool)`: enable/disable gradient calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gQj9W5LavmU"
      },
      "source": [
        "def evaluate(model, eval_loader, device):\n",
        "    \n",
        "    # activate the evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # iterate over the batches\n",
        "        for batch_idx, (data, target) in enumerate(eval_loader):\n",
        "\n",
        "            # transfer the data on the chosen device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # forward propagation on the data\n",
        "            prediction = model(data)\n",
        "\n",
        "            # compute the loss function w.r.t. the targets\n",
        "            loss = loss_function(prediction, target)           \n",
        "\n",
        "\n",
        "            # accumulate the loss\n",
        "            total_loss += loss.item()*len(data)\n",
        "\n",
        "            # compute the number of correct predictions en sortie)\n",
        "            _, pred_classes = torch.max(prediction, dim=1) \n",
        "            correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()         \n",
        "          \n",
        "    \n",
        "    # compute the average loss per epoch\n",
        "    mean_loss = total_loss/len(eval_loader.dataset)\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc = correct / len(eval_loader.dataset)\n",
        "        \n",
        "    print('Eval:  Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
        "        mean_loss, correct, len(eval_loader.dataset),\n",
        "        100. * acc)) \n",
        "    \n",
        "    # return the average loss and the accuracy\n",
        "    return mean_loss, acc"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQLklQXAavmW"
      },
      "source": [
        "## Checkpointing\n",
        "\n",
        "During training, we recommend saving the model parameters periodically. By doing so, we avoid having to retrain the model from scratch if the experiment goes wrong such as losing communication with GPU, out-of-memory error, numerical errors, incorrect learning rates making the experiment unstable, etc. This practice is commonly referred to as <b>checkpointing</b>.\n",
        "\n",
        "PyTorch offers <a href=\"http://pytorch.org/docs/master/notes/serialization.html\">a simple mechanism</a> to perform this operation.\n",
        "\n",
        "We implement two methods here:\n",
        "<ul>\n",
        "<li> the first one for <b> saving </b> a model,</li>\n",
        "<li> the second for <b> loading </b> a model checkpoint. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMmNpma2avmX"
      },
      "source": [
        "def save_model(epoch, model, path='./'):\n",
        "    \n",
        "    # creating the file name indexed by the epoch value\n",
        "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
        "    \n",
        "    # saving the model parameters\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZptgqQRavmZ"
      },
      "source": [
        "def load_model(epoch, model, path='./'):\n",
        "    \n",
        "    # creating the file name indexed by the epoch value\n",
        "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
        "    \n",
        "    # loading the parameters of the saved model\n",
        "    model.load_state_dict(torch.load(filename))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve8sOocWavma"
      },
      "source": [
        "It is also possible to save the optimizer's status in PyTorch, which is very important when we want to resume training the model from a given backup. For more information, please consult <a href='https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/3'>the following URL</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lcAP8-1avma"
      },
      "source": [
        "## Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keMpyePsavmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfd56b9-033e-4165-e560-b37cc8df7b6d"
      },
      "source": [
        "# maximum number of epoch\n",
        "numEpochs = 200\n",
        "\n",
        "# Saving frequency\n",
        "checkpoint_freq = 10\n",
        "\n",
        "# Directory for data backup\n",
        "path = './'\n",
        "\n",
        "# Accumulators of average losses obtained per epoch\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Performance accumulators per epoch\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Model definition\n",
        "neural_net = NeuralNet()\n",
        "\n",
        "# Load the model on the chosen device\n",
        "neural_net = neural_net.to(device)\n",
        "\n",
        "# Optimizer definition\n",
        "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) \n",
        "# optimizer = optim.SGD(neural_net.parameters(), lr=0.001) \n",
        "\n",
        "\n",
        "# Learning loop\n",
        "for epoch in range(1, numEpochs + 1):\n",
        "    \n",
        "    # train the model with the train dataset\n",
        "    train_loss, train_acc = train(epoch, neural_net, train_loader, optimizer, device)   \n",
        "    \n",
        "    # evaluate the model with the validation dataset\n",
        "    val_loss, val_acc = evaluate(neural_net, val_loader, device)       \n",
        "    \n",
        "    # Save the losses obtained\n",
        "    train_losses.append(train_loss)    \n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    # Save the performances\n",
        "    train_accuracies.append(train_acc)    \n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    # Checkpoint\n",
        "    if epoch % checkpoint_freq ==0:\n",
        "        save_model(epoch, neural_net, path)\n",
        "\n",
        "# Save the model at the end of the training\n",
        "save_model(numEpochs, neural_net, path)\n",
        "    \n",
        "print(\"\\n\\n\\nOptimization ended.\\n\")    \n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1   Avg_Loss: 0.66089   Acc: 406/624 (65.064%)\n",
            "Eval:  Avg_Loss: 0.63660   Acc: 142/208 (68.269%)\n",
            "Train Epoch: 2   Avg_Loss: 0.63687   Acc: 400/624 (64.103%)\n",
            "Eval:  Avg_Loss: 0.61909   Acc: 143/208 (68.750%)\n",
            "Train Epoch: 3   Avg_Loss: 0.62791   Acc: 408/624 (65.385%)\n",
            "Eval:  Avg_Loss: 0.61107   Acc: 145/208 (69.712%)\n",
            "Train Epoch: 4   Avg_Loss: 0.62288   Acc: 419/624 (67.147%)\n",
            "Eval:  Avg_Loss: 0.60916   Acc: 146/208 (70.192%)\n",
            "Train Epoch: 5   Avg_Loss: 0.62226   Acc: 414/624 (66.346%)\n",
            "Eval:  Avg_Loss: 0.60287   Acc: 144/208 (69.231%)\n",
            "Train Epoch: 6   Avg_Loss: 0.61693   Acc: 422/624 (67.628%)\n",
            "Eval:  Avg_Loss: 0.60475   Acc: 146/208 (70.192%)\n",
            "Train Epoch: 7   Avg_Loss: 0.61379   Acc: 423/624 (67.788%)\n",
            "Eval:  Avg_Loss: 0.60021   Acc: 145/208 (69.712%)\n",
            "Train Epoch: 8   Avg_Loss: 0.60977   Acc: 421/624 (67.468%)\n",
            "Eval:  Avg_Loss: 0.59719   Acc: 145/208 (69.712%)\n",
            "Train Epoch: 9   Avg_Loss: 0.60627   Acc: 434/624 (69.551%)\n",
            "Eval:  Avg_Loss: 0.59406   Acc: 152/208 (73.077%)\n",
            "Train Epoch: 10   Avg_Loss: 0.60340   Acc: 431/624 (69.071%)\n",
            "Eval:  Avg_Loss: 0.58966   Acc: 150/208 (72.115%)\n",
            "Train Epoch: 11   Avg_Loss: 0.59766   Acc: 437/624 (70.032%)\n",
            "Eval:  Avg_Loss: 0.58005   Acc: 152/208 (73.077%)\n",
            "Train Epoch: 12   Avg_Loss: 0.59213   Acc: 441/624 (70.673%)\n",
            "Eval:  Avg_Loss: 0.57787   Acc: 153/208 (73.558%)\n",
            "Train Epoch: 13   Avg_Loss: 0.58095   Acc: 448/624 (71.795%)\n",
            "Eval:  Avg_Loss: 0.58046   Acc: 159/208 (76.442%)\n",
            "Train Epoch: 14   Avg_Loss: 0.57364   Acc: 456/624 (73.077%)\n",
            "Eval:  Avg_Loss: 0.54778   Acc: 163/208 (78.365%)\n",
            "Train Epoch: 15   Avg_Loss: 0.56471   Acc: 451/624 (72.276%)\n",
            "Eval:  Avg_Loss: 0.54911   Acc: 164/208 (78.846%)\n",
            "Train Epoch: 16   Avg_Loss: 0.55709   Acc: 461/624 (73.878%)\n",
            "Eval:  Avg_Loss: 0.52783   Acc: 161/208 (77.404%)\n",
            "Train Epoch: 17   Avg_Loss: 0.54667   Acc: 461/624 (73.878%)\n",
            "Eval:  Avg_Loss: 0.51822   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 18   Avg_Loss: 0.53565   Acc: 480/624 (76.923%)\n",
            "Eval:  Avg_Loss: 0.51375   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 19   Avg_Loss: 0.52645   Acc: 493/624 (79.006%)\n",
            "Eval:  Avg_Loss: 0.50308   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 20   Avg_Loss: 0.51673   Acc: 490/624 (78.526%)\n",
            "Eval:  Avg_Loss: 0.50084   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 21   Avg_Loss: 0.51512   Acc: 494/624 (79.167%)\n",
            "Eval:  Avg_Loss: 0.50181   Acc: 165/208 (79.327%)\n",
            "Train Epoch: 22   Avg_Loss: 0.52032   Acc: 483/624 (77.404%)\n",
            "Eval:  Avg_Loss: 0.49778   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 23   Avg_Loss: 0.51721   Acc: 491/624 (78.686%)\n",
            "Eval:  Avg_Loss: 0.49680   Acc: 166/208 (79.808%)\n",
            "Train Epoch: 24   Avg_Loss: 0.51732   Acc: 482/624 (77.244%)\n",
            "Eval:  Avg_Loss: 0.48530   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 25   Avg_Loss: 0.51432   Acc: 487/624 (78.045%)\n",
            "Eval:  Avg_Loss: 0.48410   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 26   Avg_Loss: 0.50023   Acc: 501/624 (80.288%)\n",
            "Eval:  Avg_Loss: 0.48435   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 27   Avg_Loss: 0.50825   Acc: 495/624 (79.327%)\n",
            "Eval:  Avg_Loss: 0.49043   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 28   Avg_Loss: 0.50017   Acc: 503/624 (80.609%)\n",
            "Eval:  Avg_Loss: 0.48167   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 29   Avg_Loss: 0.49892   Acc: 508/624 (81.410%)\n",
            "Eval:  Avg_Loss: 0.48604   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 30   Avg_Loss: 0.50720   Acc: 493/624 (79.006%)\n",
            "Eval:  Avg_Loss: 0.47866   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 31   Avg_Loss: 0.49850   Acc: 503/624 (80.609%)\n",
            "Eval:  Avg_Loss: 0.48137   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 32   Avg_Loss: 0.50069   Acc: 507/624 (81.250%)\n",
            "Eval:  Avg_Loss: 0.47908   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 33   Avg_Loss: 0.50047   Acc: 501/624 (80.288%)\n",
            "Eval:  Avg_Loss: 0.48078   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 34   Avg_Loss: 0.50149   Acc: 503/624 (80.609%)\n",
            "Eval:  Avg_Loss: 0.49196   Acc: 167/208 (80.288%)\n",
            "Train Epoch: 35   Avg_Loss: 0.50177   Acc: 503/624 (80.609%)\n",
            "Eval:  Avg_Loss: 0.49207   Acc: 167/208 (80.288%)\n",
            "Train Epoch: 36   Avg_Loss: 0.50021   Acc: 501/624 (80.288%)\n",
            "Eval:  Avg_Loss: 0.47907   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 37   Avg_Loss: 0.49268   Acc: 509/624 (81.571%)\n",
            "Eval:  Avg_Loss: 0.48527   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 38   Avg_Loss: 0.50671   Acc: 502/624 (80.449%)\n",
            "Eval:  Avg_Loss: 0.50544   Acc: 162/208 (77.885%)\n",
            "Train Epoch: 39   Avg_Loss: 0.50228   Acc: 502/624 (80.449%)\n",
            "Eval:  Avg_Loss: 0.47933   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 40   Avg_Loss: 0.49047   Acc: 507/624 (81.250%)\n",
            "Eval:  Avg_Loss: 0.48710   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 41   Avg_Loss: 0.49946   Acc: 501/624 (80.288%)\n",
            "Eval:  Avg_Loss: 0.47744   Acc: 174/208 (83.654%)\n",
            "Train Epoch: 42   Avg_Loss: 0.48751   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48076   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 43   Avg_Loss: 0.49000   Acc: 513/624 (82.212%)\n",
            "Eval:  Avg_Loss: 0.47822   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 44   Avg_Loss: 0.49105   Acc: 505/624 (80.929%)\n",
            "Eval:  Avg_Loss: 0.47824   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 45   Avg_Loss: 0.48829   Acc: 512/624 (82.051%)\n",
            "Eval:  Avg_Loss: 0.47879   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 46   Avg_Loss: 0.48685   Acc: 513/624 (82.212%)\n",
            "Eval:  Avg_Loss: 0.47912   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 47   Avg_Loss: 0.49160   Acc: 509/624 (81.571%)\n",
            "Eval:  Avg_Loss: 0.47687   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 48   Avg_Loss: 0.49451   Acc: 502/624 (80.449%)\n",
            "Eval:  Avg_Loss: 0.48942   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 49   Avg_Loss: 0.48824   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.47788   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 50   Avg_Loss: 0.49150   Acc: 512/624 (82.051%)\n",
            "Eval:  Avg_Loss: 0.47648   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 51   Avg_Loss: 0.48481   Acc: 507/624 (81.250%)\n",
            "Eval:  Avg_Loss: 0.47889   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 52   Avg_Loss: 0.48280   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.47990   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 53   Avg_Loss: 0.48555   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.47920   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 54   Avg_Loss: 0.48811   Acc: 509/624 (81.571%)\n",
            "Eval:  Avg_Loss: 0.47780   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 55   Avg_Loss: 0.48613   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.47919   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 56   Avg_Loss: 0.47986   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.48450   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 57   Avg_Loss: 0.48269   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48517   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 58   Avg_Loss: 0.48504   Acc: 510/624 (81.731%)\n",
            "Eval:  Avg_Loss: 0.48077   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 59   Avg_Loss: 0.48618   Acc: 511/624 (81.891%)\n",
            "Eval:  Avg_Loss: 0.49582   Acc: 167/208 (80.288%)\n",
            "Train Epoch: 60   Avg_Loss: 0.49134   Acc: 512/624 (82.051%)\n",
            "Eval:  Avg_Loss: 0.47946   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 61   Avg_Loss: 0.48792   Acc: 506/624 (81.090%)\n",
            "Eval:  Avg_Loss: 0.47887   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 62   Avg_Loss: 0.48005   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48150   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 63   Avg_Loss: 0.48592   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48205   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 64   Avg_Loss: 0.48613   Acc: 510/624 (81.731%)\n",
            "Eval:  Avg_Loss: 0.47914   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 65   Avg_Loss: 0.48866   Acc: 508/624 (81.410%)\n",
            "Eval:  Avg_Loss: 0.48197   Acc: 175/208 (84.135%)\n",
            "Train Epoch: 66   Avg_Loss: 0.48698   Acc: 507/624 (81.250%)\n",
            "Eval:  Avg_Loss: 0.48050   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 67   Avg_Loss: 0.48318   Acc: 511/624 (81.891%)\n",
            "Eval:  Avg_Loss: 0.48196   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 68   Avg_Loss: 0.48204   Acc: 512/624 (82.051%)\n",
            "Eval:  Avg_Loss: 0.48564   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 69   Avg_Loss: 0.48117   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.48174   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 70   Avg_Loss: 0.48024   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.48557   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 71   Avg_Loss: 0.48147   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48119   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 72   Avg_Loss: 0.48174   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48196   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 73   Avg_Loss: 0.47835   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.48174   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 74   Avg_Loss: 0.47730   Acc: 524/624 (83.974%)\n",
            "Eval:  Avg_Loss: 0.48257   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 75   Avg_Loss: 0.47728   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48182   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 76   Avg_Loss: 0.47721   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.49763   Acc: 167/208 (80.288%)\n",
            "Train Epoch: 77   Avg_Loss: 0.47784   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.48245   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 78   Avg_Loss: 0.49133   Acc: 506/624 (81.090%)\n",
            "Eval:  Avg_Loss: 0.49736   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 79   Avg_Loss: 0.49758   Acc: 506/624 (81.090%)\n",
            "Eval:  Avg_Loss: 0.48817   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 80   Avg_Loss: 0.47415   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48311   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 81   Avg_Loss: 0.47889   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48354   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 82   Avg_Loss: 0.47558   Acc: 520/624 (83.333%)\n",
            "Eval:  Avg_Loss: 0.48241   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 83   Avg_Loss: 0.48188   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48093   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 84   Avg_Loss: 0.48024   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48352   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 85   Avg_Loss: 0.48109   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48034   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 86   Avg_Loss: 0.47670   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48327   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 87   Avg_Loss: 0.47735   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48443   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 88   Avg_Loss: 0.48210   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.49058   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 89   Avg_Loss: 0.48180   Acc: 512/624 (82.051%)\n",
            "Eval:  Avg_Loss: 0.48138   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 90   Avg_Loss: 0.48838   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.52106   Acc: 164/208 (78.846%)\n",
            "Train Epoch: 91   Avg_Loss: 0.49061   Acc: 501/624 (80.288%)\n",
            "Eval:  Avg_Loss: 0.48289   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 92   Avg_Loss: 0.48008   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.48513   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 93   Avg_Loss: 0.47531   Acc: 524/624 (83.974%)\n",
            "Eval:  Avg_Loss: 0.48378   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 94   Avg_Loss: 0.47726   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48504   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 95   Avg_Loss: 0.47916   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48283   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 96   Avg_Loss: 0.48095   Acc: 508/624 (81.410%)\n",
            "Eval:  Avg_Loss: 0.48616   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 97   Avg_Loss: 0.49329   Acc: 507/624 (81.250%)\n",
            "Eval:  Avg_Loss: 0.51872   Acc: 166/208 (79.808%)\n",
            "Train Epoch: 98   Avg_Loss: 0.49314   Acc: 506/624 (81.090%)\n",
            "Eval:  Avg_Loss: 0.48095   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 99   Avg_Loss: 0.48314   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48364   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 100   Avg_Loss: 0.47876   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48320   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 101   Avg_Loss: 0.47706   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.48560   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 102   Avg_Loss: 0.48172   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48716   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 103   Avg_Loss: 0.48104   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.49183   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 104   Avg_Loss: 0.48171   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.48168   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 105   Avg_Loss: 0.47658   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48948   Acc: 166/208 (79.808%)\n",
            "Train Epoch: 106   Avg_Loss: 0.47992   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.49228   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 107   Avg_Loss: 0.47767   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48353   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 108   Avg_Loss: 0.47871   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48396   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 109   Avg_Loss: 0.49662   Acc: 504/624 (80.769%)\n",
            "Eval:  Avg_Loss: 0.48514   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 110   Avg_Loss: 0.48660   Acc: 512/624 (82.051%)\n",
            "Eval:  Avg_Loss: 0.47974   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 111   Avg_Loss: 0.47478   Acc: 524/624 (83.974%)\n",
            "Eval:  Avg_Loss: 0.48353   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 112   Avg_Loss: 0.47835   Acc: 511/624 (81.891%)\n",
            "Eval:  Avg_Loss: 0.48110   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 113   Avg_Loss: 0.47610   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48252   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 114   Avg_Loss: 0.47737   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48622   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 115   Avg_Loss: 0.47533   Acc: 526/624 (84.295%)\n",
            "Eval:  Avg_Loss: 0.48204   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 116   Avg_Loss: 0.47625   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.48320   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 117   Avg_Loss: 0.47881   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.48250   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 118   Avg_Loss: 0.47775   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48189   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 119   Avg_Loss: 0.47402   Acc: 520/624 (83.333%)\n",
            "Eval:  Avg_Loss: 0.48475   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 120   Avg_Loss: 0.48616   Acc: 510/624 (81.731%)\n",
            "Eval:  Avg_Loss: 0.48321   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 121   Avg_Loss: 0.48213   Acc: 513/624 (82.212%)\n",
            "Eval:  Avg_Loss: 0.48171   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 122   Avg_Loss: 0.47823   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48286   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 123   Avg_Loss: 0.47665   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48304   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 124   Avg_Loss: 0.47642   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48363   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 125   Avg_Loss: 0.47907   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48419   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 126   Avg_Loss: 0.48525   Acc: 512/624 (82.051%)\n",
            "Eval:  Avg_Loss: 0.48089   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 127   Avg_Loss: 0.48022   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48284   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 128   Avg_Loss: 0.47955   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48400   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 129   Avg_Loss: 0.47659   Acc: 520/624 (83.333%)\n",
            "Eval:  Avg_Loss: 0.48427   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 130   Avg_Loss: 0.48142   Acc: 513/624 (82.212%)\n",
            "Eval:  Avg_Loss: 0.48333   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 131   Avg_Loss: 0.47624   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48408   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 132   Avg_Loss: 0.48429   Acc: 512/624 (82.051%)\n",
            "Eval:  Avg_Loss: 0.48267   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 133   Avg_Loss: 0.47928   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.49019   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 134   Avg_Loss: 0.48979   Acc: 508/624 (81.410%)\n",
            "Eval:  Avg_Loss: 0.48242   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 135   Avg_Loss: 0.47637   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48326   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 136   Avg_Loss: 0.48783   Acc: 513/624 (82.212%)\n",
            "Eval:  Avg_Loss: 0.48689   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 137   Avg_Loss: 0.48162   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48510   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 138   Avg_Loss: 0.48058   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.49001   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 139   Avg_Loss: 0.47795   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48344   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 140   Avg_Loss: 0.47721   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48318   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 141   Avg_Loss: 0.47357   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48550   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 142   Avg_Loss: 0.48257   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.49129   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 143   Avg_Loss: 0.48806   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.48239   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 144   Avg_Loss: 0.47895   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48408   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 145   Avg_Loss: 0.47471   Acc: 520/624 (83.333%)\n",
            "Eval:  Avg_Loss: 0.48232   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 146   Avg_Loss: 0.47490   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48622   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 147   Avg_Loss: 0.47834   Acc: 511/624 (81.891%)\n",
            "Eval:  Avg_Loss: 0.48488   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 148   Avg_Loss: 0.47217   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48480   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 149   Avg_Loss: 0.48329   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48272   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 150   Avg_Loss: 0.47643   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48423   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 151   Avg_Loss: 0.47336   Acc: 525/624 (84.135%)\n",
            "Eval:  Avg_Loss: 0.48707   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 152   Avg_Loss: 0.47416   Acc: 524/624 (83.974%)\n",
            "Eval:  Avg_Loss: 0.48572   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 153   Avg_Loss: 0.47772   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48417   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 154   Avg_Loss: 0.47289   Acc: 523/624 (83.814%)\n",
            "Eval:  Avg_Loss: 0.48522   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 155   Avg_Loss: 0.47375   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.49058   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 156   Avg_Loss: 0.48897   Acc: 507/624 (81.250%)\n",
            "Eval:  Avg_Loss: 0.48274   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 157   Avg_Loss: 0.47505   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48475   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 158   Avg_Loss: 0.47215   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48532   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 159   Avg_Loss: 0.47141   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48518   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 160   Avg_Loss: 0.47705   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48423   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 161   Avg_Loss: 0.47799   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48451   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 162   Avg_Loss: 0.47337   Acc: 517/624 (82.853%)\n",
            "Eval:  Avg_Loss: 0.48454   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 163   Avg_Loss: 0.46983   Acc: 528/624 (84.615%)\n",
            "Eval:  Avg_Loss: 0.48576   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 164   Avg_Loss: 0.47146   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.48608   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 165   Avg_Loss: 0.50455   Acc: 501/624 (80.288%)\n",
            "Eval:  Avg_Loss: 0.49101   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 166   Avg_Loss: 0.48339   Acc: 513/624 (82.212%)\n",
            "Eval:  Avg_Loss: 0.48205   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 167   Avg_Loss: 0.48783   Acc: 511/624 (81.891%)\n",
            "Eval:  Avg_Loss: 0.48657   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 168   Avg_Loss: 0.48659   Acc: 509/624 (81.571%)\n",
            "Eval:  Avg_Loss: 0.48472   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 169   Avg_Loss: 0.47250   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48437   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 170   Avg_Loss: 0.48098   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48371   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 171   Avg_Loss: 0.47537   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48381   Acc: 172/208 (82.692%)\n",
            "Train Epoch: 172   Avg_Loss: 0.47879   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48490   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 173   Avg_Loss: 0.47412   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48562   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 174   Avg_Loss: 0.47211   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48528   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 175   Avg_Loss: 0.47664   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.48591   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 176   Avg_Loss: 0.47241   Acc: 523/624 (83.814%)\n",
            "Eval:  Avg_Loss: 0.48683   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 177   Avg_Loss: 0.48026   Acc: 514/624 (82.372%)\n",
            "Eval:  Avg_Loss: 0.48323   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 178   Avg_Loss: 0.47999   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48548   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 179   Avg_Loss: 0.47496   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.48608   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 180   Avg_Loss: 0.47169   Acc: 523/624 (83.814%)\n",
            "Eval:  Avg_Loss: 0.48648   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 181   Avg_Loss: 0.47332   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48369   Acc: 173/208 (83.173%)\n",
            "Train Epoch: 182   Avg_Loss: 0.47758   Acc: 518/624 (83.013%)\n",
            "Eval:  Avg_Loss: 0.48330   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 183   Avg_Loss: 0.47451   Acc: 520/624 (83.333%)\n",
            "Eval:  Avg_Loss: 0.48485   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 184   Avg_Loss: 0.47542   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48571   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 185   Avg_Loss: 0.47464   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48437   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 186   Avg_Loss: 0.47410   Acc: 520/624 (83.333%)\n",
            "Eval:  Avg_Loss: 0.48537   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 187   Avg_Loss: 0.46806   Acc: 527/624 (84.455%)\n",
            "Eval:  Avg_Loss: 0.48553   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 188   Avg_Loss: 0.47181   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48505   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 189   Avg_Loss: 0.47263   Acc: 523/624 (83.814%)\n",
            "Eval:  Avg_Loss: 0.48329   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 190   Avg_Loss: 0.47523   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48475   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 191   Avg_Loss: 0.47102   Acc: 521/624 (83.494%)\n",
            "Eval:  Avg_Loss: 0.48363   Acc: 168/208 (80.769%)\n",
            "Train Epoch: 192   Avg_Loss: 0.47270   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48441   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 193   Avg_Loss: 0.47975   Acc: 515/624 (82.532%)\n",
            "Eval:  Avg_Loss: 0.48087   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 194   Avg_Loss: 0.47659   Acc: 516/624 (82.692%)\n",
            "Eval:  Avg_Loss: 0.48319   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 195   Avg_Loss: 0.47774   Acc: 524/624 (83.974%)\n",
            "Eval:  Avg_Loss: 0.48396   Acc: 171/208 (82.212%)\n",
            "Train Epoch: 196   Avg_Loss: 0.47524   Acc: 520/624 (83.333%)\n",
            "Eval:  Avg_Loss: 0.48326   Acc: 169/208 (81.250%)\n",
            "Train Epoch: 197   Avg_Loss: 0.47156   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48495   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 198   Avg_Loss: 0.47520   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48643   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 199   Avg_Loss: 0.47402   Acc: 519/624 (83.173%)\n",
            "Eval:  Avg_Loss: 0.48483   Acc: 170/208 (81.731%)\n",
            "Train Epoch: 200   Avg_Loss: 0.47098   Acc: 522/624 (83.654%)\n",
            "Eval:  Avg_Loss: 0.48675   Acc: 169/208 (81.250%)\n",
            "\n",
            "\n",
            "\n",
            "Optimization ended.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86OZRLrjavmd"
      },
      "source": [
        "## Interpreting the output of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mklvQruYavme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace54091-8b42-4312-c716-ba882bd29340"
      },
      "source": [
        "# Activate the evaluation mode\n",
        "neural_net = neural_net.eval()\n",
        "\n",
        "# Select the first 10 data points of the validation set\n",
        "data, target = val_dataset[0:10]\n",
        "data = data.to(device)\n",
        "\n",
        "# Executing the neural network\n",
        "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
        "\n",
        "# Transform the output into a probability distribution with a softmax function\n",
        "output_proba = F.softmax(output, dim=1)\n",
        "\n",
        "# Print the probability\n",
        "print(output_proba)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.7310, 0.2690],\n",
            "        [0.2689, 0.7311],\n",
            "        [0.7311, 0.2689],\n",
            "        [0.7061, 0.2939],\n",
            "        [0.7301, 0.2699],\n",
            "        [0.7309, 0.2691],\n",
            "        [0.2693, 0.7307],\n",
            "        [0.2790, 0.7210],\n",
            "        [0.7311, 0.2689],\n",
            "        [0.7311, 0.2689]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvIEqKt0qjeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40db2192-9311-4504-c801-be44e45ecb7d"
      },
      "source": [
        "# For each example, retrieve the class with the highest probability.\n",
        "dummy, prediction = torch.max(output_proba, dim=1)\n",
        "\n",
        "print(\"Model predictions\")\n",
        "print(prediction)\n",
        "print(dummy)\n",
        "print(\"Targets\")\n",
        "print(target)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model predictions\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "tensor([0.7310, 0.7311, 0.7311, 0.7061, 0.7301, 0.7309, 0.7307, 0.7210, 0.7311,\n",
            "        0.7311], device='cuda:0', grad_fn=<MaxBackward0>)\n",
            "Targets\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn8s38SJio1W",
        "outputId": "3611064f-42d4-4b9d-df1a-3014732849a5"
      },
      "source": [
        "F1_score = f1_score(target.cpu().numpy(), prediction.cpu().numpy())\n",
        "print(f'F1 score is {F1_score:.2%}'.format(F1_score))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score is 67.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V11J3Jihavmy"
      },
      "source": [
        "## Visualizing of the learning curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9_9C_tXavmz"
      },
      "source": [
        "Learning curves allow detecting problems that may have occurred during learning, for example, unstable learning, underfitting, or overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNcbpl0tavm0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "decf6ce8-7442-4b2c-8261-cc961b0f4af0"
      },
      "source": [
        "x = list(range(len(train_losses)))\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(x, train_losses, 'r', label=\"Train\")\n",
        "plt.plot(x, val_losses, 'g', label=\"Validation\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Cross-entropy loss')\n",
        "plt.grid()\n",
        "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
        "leg.get_frame().set_alpha(0.99)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-VGQ2pMavm4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "d92191ee-5575-4527-bef8-0b237720ce5c"
      },
      "source": [
        "x = list(range(len(train_accuracies)))\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(x, train_accuracies, 'r', label=\"Train\")\n",
        "plt.plot(x, val_accuracies, 'g', label=\"Validation\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
        "leg.get_frame().set_alpha(0.99)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oktkpkuqjet"
      },
      "source": [
        "### Exercise 6\n",
        "\n",
        "*   What do you observe in the previous graphs?\n",
        "*   At which epoch is it interesting to retrieve the model parameters for inference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwhRt39yzug-"
      },
      "source": [
        "... # To complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK_eUsq3avm8"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UREO5elavm8"
      },
      "source": [
        "We can finally evaluate our model on our test set and compare with the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UZPvzjxlusr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf35702-7e92-4f27-d983-eb7c41adb08d"
      },
      "source": [
        "valid_loss, valid_acc = evaluate(neural_net, val_loader, device)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval:  Avg_Loss: 0.48675   Acc: 169/208 (81.250%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPWvDM-qavm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e8a244-0d83-4dfd-e0af-60ed26877b2b"
      },
      "source": [
        "test_loss, test_acc = evaluate(neural_net, test_loader, device)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval:  Avg_Loss: 0.49298   Acc: 170/209 (81.340%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvP_-KUwqjez"
      },
      "source": [
        "### Exercise 7\n",
        "\n",
        "* Compare validation and test metrics.\n",
        "* Do you think the chosen features are informative of the fate of the passenger?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov4CKUFh5tun"
      },
      "source": [
        "... # To complete.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfUHiLk5jbC8",
        "outputId": "cc7b2184-4619-40ae-8145-73bb4611e5fa"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "  Downloading https://files.pythonhosted.org/packages/79/e7/643808913211d6c1fc96a3a4333bf4c9276858fab00bcafaf98ea58a97be/torchviz-0.0.2.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.8.1+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (1.19.5)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-cp37-none-any.whl size=4152 sha256=82910278b43d5504b77244dd896a232add522773e2b1b71a2330df23cda0bf92\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/26/58/026ffd533dbe8b3972eb423da9c7949beca68d1c98ed9e8624\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rueFjis6YlLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "outputId": "5bed9670-8666-4adf-ab61-f4616367f22f"
      },
      "source": [
        "import graphviz\n",
        "import torchviz\n",
        "\n",
        "torchviz.make_dot(output,params=dict(neural_net.named_parameters()))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fa51e174090>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"450pt\" height=\"732pt\"\n viewBox=\"0.00 0.00 450.00 732.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 728)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-728 446,-728 446,4 -4,4\"/>\n<!-- 140347167371824 -->\n<g id=\"node1\" class=\"node\">\n<title>140347167371824</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"251,-31 186,-31 186,0 251,0 251,-31\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10, 2)</text>\n</g>\n<!-- 140347151634128 -->\n<g id=\"node2\" class=\"node\">\n<title>140347151634128</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-86 165,-86 165,-67 272,-67 272,-86\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SigmoidBackward</text>\n</g>\n<!-- 140347151634128&#45;&gt;140347167371824 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140347151634128&#45;&gt;140347167371824</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-66.9688C218.5,-60.1289 218.5,-50.5621 218.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-41.3678 218.5,-31.3678 215.0001,-41.3678 222.0001,-41.3678\"/>\n</g>\n<!-- 140347152037648 -->\n<g id=\"node3\" class=\"node\">\n<title>140347152037648</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"266,-141 171,-141 171,-122 266,-122 266,-141\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140347152037648&#45;&gt;140347151634128 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140347152037648&#45;&gt;140347151634128</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-121.9197C218.5,-114.9083 218.5,-105.1442 218.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-96.3408 218.5,-86.3408 215.0001,-96.3409 222.0001,-96.3408\"/>\n</g>\n<!-- 140347151173776 -->\n<g id=\"node4\" class=\"node\">\n<title>140347151173776</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"153,-196 52,-196 52,-177 153,-177 153,-196\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151173776&#45;&gt;140347152037648 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140347151173776&#45;&gt;140347152037648</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M122.7057,-176.9197C140.9737,-168.2581 168.1018,-155.3957 188.8304,-145.5675\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"190.6004,-148.6018 198.1367,-141.155 187.6014,-142.2768 190.6004,-148.6018\"/>\n</g>\n<!-- 140347167455024 -->\n<g id=\"node5\" class=\"node\">\n<title>140347167455024</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"144,-262 61,-262 61,-232 144,-232 144,-262\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">output.bias</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2)</text>\n</g>\n<!-- 140347167455024&#45;&gt;140347151173776 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140347167455024&#45;&gt;140347151173776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M102.5,-231.7333C102.5,-224.0322 102.5,-214.5977 102.5,-206.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0001,-206.0864 102.5,-196.0864 99.0001,-206.0864 106.0001,-206.0864\"/>\n</g>\n<!-- 140347151173264 -->\n<g id=\"node6\" class=\"node\">\n<title>140347151173264</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"266,-196 171,-196 171,-177 266,-177 266,-196\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140347151173264&#45;&gt;140347152037648 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140347151173264&#45;&gt;140347152037648</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-176.9197C218.5,-169.9083 218.5,-160.1442 218.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-151.3408 218.5,-141.3408 215.0001,-151.3409 222.0001,-151.3408\"/>\n</g>\n<!-- 140347151175248 -->\n<g id=\"node7\" class=\"node\">\n<title>140347151175248</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"261,-256.5 166,-256.5 166,-237.5 261,-237.5 261,-256.5\"/>\n<text text-anchor=\"middle\" x=\"213.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140347151175248&#45;&gt;140347151173264 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140347151175248&#45;&gt;140347151173264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M214.3033,-237.2796C214.9845,-229.0376 215.9838,-216.9457 216.8364,-206.629\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"220.3464,-206.6516 217.682,-196.3972 213.3702,-206.075 220.3464,-206.6516\"/>\n</g>\n<!-- 140347151173072 -->\n<g id=\"node8\" class=\"node\">\n<title>140347151173072</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"127,-322.5 26,-322.5 26,-303.5 127,-303.5 127,-322.5\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151173072&#45;&gt;140347151175248 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140347151173072&#45;&gt;140347151175248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M96.421,-303.403C119.5543,-292.2585 157.9168,-273.7773 184.3899,-261.0238\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.065,-264.1019 193.555,-256.6085 183.0268,-257.7956 186.065,-264.1019\"/>\n</g>\n<!-- 140347167462896 -->\n<g id=\"node9\" class=\"node\">\n<title>140347167462896</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"118,-394 35,-394 35,-364 118,-364 118,-394\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer3.bias</text>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140347167462896&#45;&gt;140347151173072 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140347167462896&#45;&gt;140347151173072</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M76.5,-363.6924C76.5,-354.5067 76.5,-342.7245 76.5,-332.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"80.0001,-332.703 76.5,-322.7031 73.0001,-332.7031 80.0001,-332.703\"/>\n</g>\n<!-- 140347151173200 -->\n<g id=\"node10\" class=\"node\">\n<title>140347151173200</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"240,-322.5 145,-322.5 145,-303.5 240,-303.5 240,-322.5\"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140347151173200&#45;&gt;140347151175248 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140347151173200&#45;&gt;140347151175248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M195.5986,-303.2615C198.6695,-293.6102 203.4588,-278.558 207.3112,-266.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"210.6918,-267.369 210.3887,-256.7785 204.0213,-265.2465 210.6918,-267.369\"/>\n</g>\n<!-- 140347151175568 -->\n<g id=\"node11\" class=\"node\">\n<title>140347151175568</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"235,-388.5 140,-388.5 140,-369.5 235,-369.5 235,-388.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140347151175568&#45;&gt;140347151173200 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140347151175568&#45;&gt;140347151173200</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M188.2378,-369.2615C188.9615,-359.7077 190.0862,-344.8615 190.9986,-332.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"194.4937,-333.0143 191.7592,-322.7785 187.5137,-332.4855 194.4937,-333.0143\"/>\n</g>\n<!-- 140347151174544 -->\n<g id=\"node12\" class=\"node\">\n<title>140347151174544</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-454.5 0,-454.5 0,-435.5 101,-435.5 101,-454.5\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151174544&#45;&gt;140347151175568 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140347151174544&#45;&gt;140347151175568</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M70.421,-435.403C93.5543,-424.2585 131.9168,-405.7773 158.3899,-393.0238\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"160.065,-396.1019 167.555,-388.6085 157.0268,-389.7956 160.065,-396.1019\"/>\n</g>\n<!-- 140347167463056 -->\n<g id=\"node13\" class=\"node\">\n<title>140347167463056</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"92,-526 9,-526 9,-496 92,-496 92,-526\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer2.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (40)</text>\n</g>\n<!-- 140347167463056&#45;&gt;140347151174544 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140347167463056&#45;&gt;140347151174544</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-495.6924C50.5,-486.5067 50.5,-474.7245 50.5,-464.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-464.703 50.5,-454.7031 47.0001,-464.7031 54.0001,-464.703\"/>\n</g>\n<!-- 140347151172240 -->\n<g id=\"node14\" class=\"node\">\n<title>140347151172240</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"214,-454.5 119,-454.5 119,-435.5 214,-435.5 214,-454.5\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140347151172240&#45;&gt;140347151175568 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140347151172240&#45;&gt;140347151175568</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M169.5986,-435.2615C172.6695,-425.6102 177.4588,-410.558 181.3112,-398.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.6918,-399.369 184.3887,-388.7785 178.0213,-397.2465 184.6918,-399.369\"/>\n</g>\n<!-- 140347151173136 -->\n<g id=\"node15\" class=\"node\">\n<title>140347151173136</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"209,-520.5 114,-520.5 114,-501.5 209,-501.5 209,-520.5\"/>\n<text text-anchor=\"middle\" x=\"161.5\" y=\"-508.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140347151173136&#45;&gt;140347151172240 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140347151173136&#45;&gt;140347151172240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.2378,-501.2615C162.9615,-491.7077 164.0862,-476.8615 164.9986,-464.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.4937,-465.0143 165.7592,-454.7785 161.5137,-464.4855 168.4937,-465.0143\"/>\n</g>\n<!-- 140347151172304 -->\n<g id=\"node16\" class=\"node\">\n<title>140347151172304</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"112,-586.5 11,-586.5 11,-567.5 112,-567.5 112,-586.5\"/>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-574.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151172304&#45;&gt;140347151173136 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140347151172304&#45;&gt;140347151173136</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M76.2553,-567.2615C92.6709,-556.4272 119.3988,-538.7868 138.5509,-526.1464\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.5911,-528.9935 147.0092,-520.5639 136.7352,-523.1512 140.5911,-528.9935\"/>\n</g>\n<!-- 140347167462656 -->\n<g id=\"node17\" class=\"node\">\n<title>140347167462656</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"102,-658 19,-658 19,-628 102,-628 102,-658\"/>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-646\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer1.bias</text>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-635\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140347167462656&#45;&gt;140347151172304 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140347167462656&#45;&gt;140347151172304</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M60.7319,-627.6924C60.8711,-618.5067 61.0496,-606.7245 61.1995,-596.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.701,-596.755 61.353,-586.7031 57.7018,-596.6489 64.701,-596.755\"/>\n</g>\n<!-- 140347151174032 -->\n<g id=\"node18\" class=\"node\">\n<title>140347151174032</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"204,-586.5 133,-586.5 133,-567.5 204,-567.5 204,-586.5\"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-574.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140347151174032&#45;&gt;140347151173136 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140347151174032&#45;&gt;140347151173136</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M167.4671,-567.2615C166.4538,-557.7077 164.8793,-542.8615 163.6019,-530.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"167.0724,-530.3535 162.5371,-520.7785 160.1114,-531.0919 167.0724,-530.3535\"/>\n</g>\n<!-- 140347151172112 -->\n<g id=\"node19\" class=\"node\">\n<title>140347151172112</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"221,-652.5 120,-652.5 120,-633.5 221,-633.5 221,-652.5\"/>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-640.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151172112&#45;&gt;140347151174032 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140347151172112&#45;&gt;140347151174032</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.2049,-633.2615C169.9154,-623.7077 169.4655,-608.8615 169.1006,-596.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"172.5977,-596.6678 168.7963,-586.7785 165.6009,-596.8799 172.5977,-596.6678\"/>\n</g>\n<!-- 140347167462576 -->\n<g id=\"node20\" class=\"node\">\n<title>140347167462576</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"218,-724 123,-724 123,-694 218,-694 218,-724\"/>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-712\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer1.weight</text>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-701\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 12)</text>\n</g>\n<!-- 140347167462576&#45;&gt;140347151172112 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140347167462576&#45;&gt;140347151172112</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.5,-693.6924C170.5,-684.5067 170.5,-672.7245 170.5,-662.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"174.0001,-662.703 170.5,-652.7031 167.0001,-662.7031 174.0001,-662.703\"/>\n</g>\n<!-- 140347151173456 -->\n<g id=\"node21\" class=\"node\">\n<title>140347151173456</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"303,-454.5 232,-454.5 232,-435.5 303,-435.5 303,-454.5\"/>\n<text text-anchor=\"middle\" x=\"267.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140347151173456&#45;&gt;140347151175568 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140347151173456&#45;&gt;140347151175568</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M255.6957,-435.2615C242.8604,-424.6723 222.1442,-407.5815 206.9083,-395.0119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0337,-392.228 199.0926,-388.5639 204.579,-397.6276 209.0337,-392.228\"/>\n</g>\n<!-- 140347151171984 -->\n<g id=\"node22\" class=\"node\">\n<title>140347151171984</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"328,-520.5 227,-520.5 227,-501.5 328,-501.5 328,-520.5\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-508.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151171984&#45;&gt;140347151173456 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140347151171984&#45;&gt;140347151173456</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M276.0245,-501.2615C274.5769,-491.7077 272.3275,-476.8615 270.5028,-464.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.9402,-464.1413 268.9816,-454.7785 267.0192,-465.19 273.9402,-464.1413\"/>\n</g>\n<!-- 140347167462976 -->\n<g id=\"node23\" class=\"node\">\n<title>140347167462976</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"325,-592 230,-592 230,-562 325,-562 325,-592\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer2.weight</text>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (40, 20)</text>\n</g>\n<!-- 140347167462976&#45;&gt;140347151171984 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140347167462976&#45;&gt;140347151171984</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M277.5,-561.6924C277.5,-552.5067 277.5,-540.7245 277.5,-530.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"281.0001,-530.703 277.5,-520.7031 274.0001,-530.7031 281.0001,-530.703\"/>\n</g>\n<!-- 140347151173008 -->\n<g id=\"node24\" class=\"node\">\n<title>140347151173008</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"329,-322.5 258,-322.5 258,-303.5 329,-303.5 329,-322.5\"/>\n<text text-anchor=\"middle\" x=\"293.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140347151173008&#45;&gt;140347151175248 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140347151173008&#45;&gt;140347151175248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M281.6957,-303.2615C268.8604,-292.6723 248.1442,-275.5815 232.9083,-263.0119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.0337,-260.228 225.0926,-256.5639 230.579,-265.6276 235.0337,-260.228\"/>\n</g>\n<!-- 140347151174864 -->\n<g id=\"node25\" class=\"node\">\n<title>140347151174864</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"397,-388.5 296,-388.5 296,-369.5 397,-369.5 397,-388.5\"/>\n<text text-anchor=\"middle\" x=\"346.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151174864&#45;&gt;140347151173008 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140347151174864&#45;&gt;140347151173008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M338.6797,-369.2615C330.538,-359.1228 317.61,-343.0238 307.6652,-330.6397\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.3428,-328.3842 301.3524,-322.7785 304.8848,-332.7671 310.3428,-328.3842\"/>\n</g>\n<!-- 140347167462816 -->\n<g id=\"node26\" class=\"node\">\n<title>140347167462816</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"416,-460 321,-460 321,-430 416,-430 416,-460\"/>\n<text text-anchor=\"middle\" x=\"368.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer3.weight</text>\n<text text-anchor=\"middle\" x=\"368.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 40)</text>\n</g>\n<!-- 140347167462816&#45;&gt;140347151174864 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140347167462816&#45;&gt;140347151174864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M363.3975,-429.6924C360.2697,-420.3092 356.2388,-408.2165 352.8985,-398.1956\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"356.2171,-397.0831 349.7344,-388.7031 349.5763,-399.2967 356.2171,-397.0831\"/>\n</g>\n<!-- 140347151172944 -->\n<g id=\"node27\" class=\"node\">\n<title>140347151172944</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"381,-196 310,-196 310,-177 381,-177 381,-196\"/>\n<text text-anchor=\"middle\" x=\"345.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140347151172944&#45;&gt;140347152037648 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140347151172944&#45;&gt;140347152037648</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M323.3783,-176.9197C303.1064,-168.1406 272.8692,-155.0457 250.0628,-145.1689\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"251.3617,-141.9174 240.7943,-141.155 248.5798,-148.3409 251.3617,-141.9174\"/>\n</g>\n<!-- 140347151173840 -->\n<g id=\"node28\" class=\"node\">\n<title>140347151173840</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"423,-256.5 322,-256.5 322,-237.5 423,-237.5 423,-256.5\"/>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151173840&#45;&gt;140347151172944 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140347151173840&#45;&gt;140347151172944</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M368.162,-237.2796C364.3663,-228.7746 358.7412,-216.17 354.0449,-205.6469\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"357.1886,-204.1027 349.9169,-196.3972 350.7962,-206.9555 357.1886,-204.1027\"/>\n</g>\n<!-- 140347167453264 -->\n<g id=\"node29\" class=\"node\">\n<title>140347167453264</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"442,-328 347,-328 347,-298 442,-298 442,-328\"/>\n<text text-anchor=\"middle\" x=\"394.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">output.weight</text>\n<text text-anchor=\"middle\" x=\"394.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 20)</text>\n</g>\n<!-- 140347167453264&#45;&gt;140347151173840 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140347167453264&#45;&gt;140347151173840</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M389.3975,-297.6924C386.2697,-288.3092 382.2388,-276.2165 378.8985,-266.1956\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"382.2171,-265.0831 375.7344,-256.7031 375.5763,-267.2967 382.2171,-265.0831\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oo1h4rQAlxGR",
        "outputId": "4900103c-4d1d-4248-cfeb-50532c5c500f"
      },
      "source": [
        "torchviz.make_dot(output_proba,params=dict(neural_net.named_parameters()))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fa51e17e190>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"450pt\" height=\"787pt\"\n viewBox=\"0.00 0.00 450.00 787.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 783)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-783 446,-783 446,4 -4,4\"/>\n<!-- 140347712318272 -->\n<g id=\"node1\" class=\"node\">\n<title>140347712318272</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"251,-31 186,-31 186,0 251,0 251,-31\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10, 2)</text>\n</g>\n<!-- 140347152597136 -->\n<g id=\"node2\" class=\"node\">\n<title>140347152597136</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-86 165,-86 165,-67 272,-67 272,-86\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SoftmaxBackward</text>\n</g>\n<!-- 140347152597136&#45;&gt;140347712318272 -->\n<g id=\"edge29\" class=\"edge\">\n<title>140347152597136&#45;&gt;140347712318272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-66.9688C218.5,-60.1289 218.5,-50.5621 218.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-41.3678 218.5,-31.3678 215.0001,-41.3678 222.0001,-41.3678\"/>\n</g>\n<!-- 140347151213136 -->\n<g id=\"node3\" class=\"node\">\n<title>140347151213136</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"272,-141 165,-141 165,-122 272,-122 272,-141\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SigmoidBackward</text>\n</g>\n<!-- 140347151213136&#45;&gt;140347152597136 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140347151213136&#45;&gt;140347152597136</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-121.9197C218.5,-114.9083 218.5,-105.1442 218.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-96.3408 218.5,-86.3408 215.0001,-96.3409 222.0001,-96.3408\"/>\n</g>\n<!-- 140347151215376 -->\n<g id=\"node4\" class=\"node\">\n<title>140347151215376</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"266,-196 171,-196 171,-177 266,-177 266,-196\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140347151215376&#45;&gt;140347151213136 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140347151215376&#45;&gt;140347151213136</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-176.9197C218.5,-169.9083 218.5,-160.1442 218.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-151.3408 218.5,-141.3408 215.0001,-151.3409 222.0001,-151.3408\"/>\n</g>\n<!-- 140347151215824 -->\n<g id=\"node5\" class=\"node\">\n<title>140347151215824</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"153,-251 52,-251 52,-232 153,-232 153,-251\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151215824&#45;&gt;140347151215376 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140347151215824&#45;&gt;140347151215376</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M122.7057,-231.9197C140.9737,-223.2581 168.1018,-210.3957 188.8304,-200.5675\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"190.6004,-203.6018 198.1367,-196.155 187.6014,-197.2768 190.6004,-203.6018\"/>\n</g>\n<!-- 140347167455024 -->\n<g id=\"node6\" class=\"node\">\n<title>140347167455024</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"144,-317 61,-317 61,-287 144,-287 144,-317\"/>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">output.bias</text>\n<text text-anchor=\"middle\" x=\"102.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2)</text>\n</g>\n<!-- 140347167455024&#45;&gt;140347151215824 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140347167455024&#45;&gt;140347151215824</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M102.5,-286.7333C102.5,-279.0322 102.5,-269.5977 102.5,-261.3414\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.0001,-261.0864 102.5,-251.0864 99.0001,-261.0864 106.0001,-261.0864\"/>\n</g>\n<!-- 140347151215568 -->\n<g id=\"node7\" class=\"node\">\n<title>140347151215568</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"266,-251 171,-251 171,-232 266,-232 266,-251\"/>\n<text text-anchor=\"middle\" x=\"218.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140347151215568&#45;&gt;140347151215376 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140347151215568&#45;&gt;140347151215376</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.5,-231.9197C218.5,-224.9083 218.5,-215.1442 218.5,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"222.0001,-206.3408 218.5,-196.3408 215.0001,-206.3409 222.0001,-206.3408\"/>\n</g>\n<!-- 140347151213200 -->\n<g id=\"node8\" class=\"node\">\n<title>140347151213200</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"261,-311.5 166,-311.5 166,-292.5 261,-292.5 261,-311.5\"/>\n<text text-anchor=\"middle\" x=\"213.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140347151213200&#45;&gt;140347151215568 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140347151213200&#45;&gt;140347151215568</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M214.3033,-292.2796C214.9845,-284.0376 215.9838,-271.9457 216.8364,-261.629\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"220.3464,-261.6516 217.682,-251.3972 213.3702,-261.075 220.3464,-261.6516\"/>\n</g>\n<!-- 140347151214480 -->\n<g id=\"node9\" class=\"node\">\n<title>140347151214480</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"127,-377.5 26,-377.5 26,-358.5 127,-358.5 127,-377.5\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151214480&#45;&gt;140347151213200 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140347151214480&#45;&gt;140347151213200</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M96.421,-358.403C119.5543,-347.2585 157.9168,-328.7773 184.3899,-316.0238\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.065,-319.1019 193.555,-311.6085 183.0268,-312.7956 186.065,-319.1019\"/>\n</g>\n<!-- 140347167462896 -->\n<g id=\"node10\" class=\"node\">\n<title>140347167462896</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"118,-449 35,-449 35,-419 118,-419 118,-449\"/>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer3.bias</text>\n<text text-anchor=\"middle\" x=\"76.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140347167462896&#45;&gt;140347151214480 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140347167462896&#45;&gt;140347151214480</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M76.5,-418.6924C76.5,-409.5067 76.5,-397.7245 76.5,-387.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"80.0001,-387.703 76.5,-377.7031 73.0001,-387.7031 80.0001,-387.703\"/>\n</g>\n<!-- 140347151213648 -->\n<g id=\"node11\" class=\"node\">\n<title>140347151213648</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"240,-377.5 145,-377.5 145,-358.5 240,-358.5 240,-377.5\"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140347151213648&#45;&gt;140347151213200 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140347151213648&#45;&gt;140347151213200</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M195.5986,-358.2615C198.6695,-348.6102 203.4588,-333.558 207.3112,-321.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"210.6918,-322.369 210.3887,-311.7785 204.0213,-320.2465 210.6918,-322.369\"/>\n</g>\n<!-- 140347151214288 -->\n<g id=\"node12\" class=\"node\">\n<title>140347151214288</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"235,-443.5 140,-443.5 140,-424.5 235,-424.5 235,-443.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-431.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140347151214288&#45;&gt;140347151213648 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140347151214288&#45;&gt;140347151213648</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M188.2378,-424.2615C188.9615,-414.7077 190.0862,-399.8615 190.9986,-387.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"194.4937,-388.0143 191.7592,-377.7785 187.5137,-387.4855 194.4937,-388.0143\"/>\n</g>\n<!-- 140347151214160 -->\n<g id=\"node13\" class=\"node\">\n<title>140347151214160</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-509.5 0,-509.5 0,-490.5 101,-490.5 101,-509.5\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-497.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151214160&#45;&gt;140347151214288 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140347151214160&#45;&gt;140347151214288</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M70.421,-490.403C93.5543,-479.2585 131.9168,-460.7773 158.3899,-448.0238\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"160.065,-451.1019 167.555,-443.6085 157.0268,-444.7956 160.065,-451.1019\"/>\n</g>\n<!-- 140347167463056 -->\n<g id=\"node14\" class=\"node\">\n<title>140347167463056</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"92,-581 9,-581 9,-551 92,-551 92,-581\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer2.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-558\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (40)</text>\n</g>\n<!-- 140347167463056&#45;&gt;140347151214160 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140347167463056&#45;&gt;140347151214160</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-550.6924C50.5,-541.5067 50.5,-529.7245 50.5,-519.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-519.703 50.5,-509.7031 47.0001,-519.7031 54.0001,-519.703\"/>\n</g>\n<!-- 140347151214032 -->\n<g id=\"node15\" class=\"node\">\n<title>140347151214032</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"214,-509.5 119,-509.5 119,-490.5 214,-490.5 214,-509.5\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-497.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140347151214032&#45;&gt;140347151214288 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140347151214032&#45;&gt;140347151214288</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M169.5986,-490.2615C172.6695,-480.6102 177.4588,-465.558 181.3112,-453.4506\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.6918,-454.369 184.3887,-443.7785 178.0213,-452.2465 184.6918,-454.369\"/>\n</g>\n<!-- 140347152576016 -->\n<g id=\"node16\" class=\"node\">\n<title>140347152576016</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"209,-575.5 114,-575.5 114,-556.5 209,-556.5 209,-575.5\"/>\n<text text-anchor=\"middle\" x=\"161.5\" y=\"-563.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward</text>\n</g>\n<!-- 140347152576016&#45;&gt;140347151214032 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140347152576016&#45;&gt;140347151214032</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.2378,-556.2615C162.9615,-546.7077 164.0862,-531.8615 164.9986,-519.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.4937,-520.0143 165.7592,-509.7785 161.5137,-519.4855 168.4937,-520.0143\"/>\n</g>\n<!-- 140347151213904 -->\n<g id=\"node17\" class=\"node\">\n<title>140347151213904</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"112,-641.5 11,-641.5 11,-622.5 112,-622.5 112,-641.5\"/>\n<text text-anchor=\"middle\" x=\"61.5\" y=\"-629.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151213904&#45;&gt;140347152576016 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140347151213904&#45;&gt;140347152576016</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M76.2553,-622.2615C92.6709,-611.4272 119.3988,-593.7868 138.5509,-581.1464\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.5911,-583.9935 147.0092,-575.5639 136.7352,-578.1512 140.5911,-583.9935\"/>\n</g>\n<!-- 140347167462656 -->\n<g id=\"node18\" class=\"node\">\n<title>140347167462656</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"102,-713 19,-713 19,-683 102,-683 102,-713\"/>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-701\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer1.bias</text>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-690\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 140347167462656&#45;&gt;140347151213904 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140347167462656&#45;&gt;140347151213904</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M60.7319,-682.6924C60.8711,-673.5067 61.0496,-661.7245 61.1995,-651.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.701,-651.755 61.353,-641.7031 57.7018,-651.6489 64.701,-651.755\"/>\n</g>\n<!-- 140347158563664 -->\n<g id=\"node19\" class=\"node\">\n<title>140347158563664</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"204,-641.5 133,-641.5 133,-622.5 204,-622.5 204,-641.5\"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-629.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140347158563664&#45;&gt;140347152576016 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140347158563664&#45;&gt;140347152576016</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M167.4671,-622.2615C166.4538,-612.7077 164.8793,-597.8615 163.6019,-585.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"167.0724,-585.3535 162.5371,-575.7785 160.1114,-586.0919 167.0724,-585.3535\"/>\n</g>\n<!-- 140347172273232 -->\n<g id=\"node20\" class=\"node\">\n<title>140347172273232</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"221,-707.5 120,-707.5 120,-688.5 221,-688.5 221,-707.5\"/>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-695.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347172273232&#45;&gt;140347158563664 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140347172273232&#45;&gt;140347158563664</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.2049,-688.2615C169.9154,-678.7077 169.4655,-663.8615 169.1006,-651.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"172.5977,-651.6678 168.7963,-641.7785 165.6009,-651.8799 172.5977,-651.6678\"/>\n</g>\n<!-- 140347167462576 -->\n<g id=\"node21\" class=\"node\">\n<title>140347167462576</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"218,-779 123,-779 123,-749 218,-749 218,-779\"/>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-767\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer1.weight</text>\n<text text-anchor=\"middle\" x=\"170.5\" y=\"-756\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 12)</text>\n</g>\n<!-- 140347167462576&#45;&gt;140347172273232 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140347167462576&#45;&gt;140347172273232</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M170.5,-748.6924C170.5,-739.5067 170.5,-727.7245 170.5,-717.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"174.0001,-717.703 170.5,-707.7031 167.0001,-717.7031 174.0001,-717.703\"/>\n</g>\n<!-- 140347151213968 -->\n<g id=\"node22\" class=\"node\">\n<title>140347151213968</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"303,-509.5 232,-509.5 232,-490.5 303,-490.5 303,-509.5\"/>\n<text text-anchor=\"middle\" x=\"267.5\" y=\"-497.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140347151213968&#45;&gt;140347151214288 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140347151213968&#45;&gt;140347151214288</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M255.6957,-490.2615C242.8604,-479.6723 222.1442,-462.5815 206.9083,-450.0119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.0337,-447.228 199.0926,-443.5639 204.579,-452.6276 209.0337,-447.228\"/>\n</g>\n<!-- 140347151215312 -->\n<g id=\"node23\" class=\"node\">\n<title>140347151215312</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"328,-575.5 227,-575.5 227,-556.5 328,-556.5 328,-575.5\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-563.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347151215312&#45;&gt;140347151213968 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140347151215312&#45;&gt;140347151213968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M276.0245,-556.2615C274.5769,-546.7077 272.3275,-531.8615 270.5028,-519.8183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.9402,-519.1413 268.9816,-509.7785 267.0192,-520.19 273.9402,-519.1413\"/>\n</g>\n<!-- 140347167462976 -->\n<g id=\"node24\" class=\"node\">\n<title>140347167462976</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"325,-647 230,-647 230,-617 325,-617 325,-647\"/>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-635\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer2.weight</text>\n<text text-anchor=\"middle\" x=\"277.5\" y=\"-624\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (40, 20)</text>\n</g>\n<!-- 140347167462976&#45;&gt;140347151215312 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140347167462976&#45;&gt;140347151215312</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M277.5,-616.6924C277.5,-607.5067 277.5,-595.7245 277.5,-585.8312\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"281.0001,-585.703 277.5,-575.7031 274.0001,-585.7031 281.0001,-585.703\"/>\n</g>\n<!-- 140347151213520 -->\n<g id=\"node25\" class=\"node\">\n<title>140347151213520</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"329,-377.5 258,-377.5 258,-358.5 329,-358.5 329,-377.5\"/>\n<text text-anchor=\"middle\" x=\"293.5\" y=\"-365.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140347151213520&#45;&gt;140347151213200 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140347151213520&#45;&gt;140347151213200</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M281.6957,-358.2615C268.8604,-347.6723 248.1442,-330.5815 232.9083,-318.0119\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.0337,-315.228 225.0926,-311.5639 230.579,-320.6276 235.0337,-315.228\"/>\n</g>\n<!-- 140349768029072 -->\n<g id=\"node26\" class=\"node\">\n<title>140349768029072</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"397,-443.5 296,-443.5 296,-424.5 397,-424.5 397,-443.5\"/>\n<text text-anchor=\"middle\" x=\"346.5\" y=\"-431.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140349768029072&#45;&gt;140347151213520 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140349768029072&#45;&gt;140347151213520</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M338.6797,-424.2615C330.538,-414.1228 317.61,-398.0238 307.6652,-385.6397\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"310.3428,-383.3842 301.3524,-377.7785 304.8848,-387.7671 310.3428,-383.3842\"/>\n</g>\n<!-- 140347167462816 -->\n<g id=\"node27\" class=\"node\">\n<title>140347167462816</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"416,-515 321,-515 321,-485 416,-485 416,-515\"/>\n<text text-anchor=\"middle\" x=\"368.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">layer3.weight</text>\n<text text-anchor=\"middle\" x=\"368.5\" y=\"-492\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (20, 40)</text>\n</g>\n<!-- 140347167462816&#45;&gt;140349768029072 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140347167462816&#45;&gt;140349768029072</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M363.3975,-484.6924C360.2697,-475.3092 356.2388,-463.2165 352.8985,-453.1956\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"356.2171,-452.0831 349.7344,-443.7031 349.5763,-454.2967 356.2171,-452.0831\"/>\n</g>\n<!-- 140347151213008 -->\n<g id=\"node28\" class=\"node\">\n<title>140347151213008</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"381,-251 310,-251 310,-232 381,-232 381,-251\"/>\n<text text-anchor=\"middle\" x=\"345.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward</text>\n</g>\n<!-- 140347151213008&#45;&gt;140347151215376 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140347151213008&#45;&gt;140347151215376</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M323.3783,-231.9197C303.1064,-223.1406 272.8692,-210.0457 250.0628,-200.1689\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"251.3617,-196.9174 240.7943,-196.155 248.5798,-203.3409 251.3617,-196.9174\"/>\n</g>\n<!-- 140347152010704 -->\n<g id=\"node29\" class=\"node\">\n<title>140347152010704</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"423,-311.5 322,-311.5 322,-292.5 423,-292.5 423,-311.5\"/>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140347152010704&#45;&gt;140347151213008 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140347152010704&#45;&gt;140347151213008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M368.162,-292.2796C364.3663,-283.7746 358.7412,-271.17 354.0449,-260.6469\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"357.1886,-259.1027 349.9169,-251.3972 350.7962,-261.9555 357.1886,-259.1027\"/>\n</g>\n<!-- 140347167453264 -->\n<g id=\"node30\" class=\"node\">\n<title>140347167453264</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"442,-383 347,-383 347,-353 442,-353 442,-383\"/>\n<text text-anchor=\"middle\" x=\"394.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">output.weight</text>\n<text text-anchor=\"middle\" x=\"394.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 20)</text>\n</g>\n<!-- 140347167453264&#45;&gt;140347152010704 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140347167453264&#45;&gt;140347152010704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M389.3975,-352.6924C386.2697,-343.3092 382.2388,-331.2165 378.8985,-321.1956\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"382.2171,-320.0831 375.7344,-311.7031 375.5763,-322.2967 382.2171,-320.0831\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}